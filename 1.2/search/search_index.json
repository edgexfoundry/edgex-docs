{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction EdgeX Foundry is a vendor-neutral open source software platform at the edge of the network, that interacts with the physical, every day working world of devices, sensors, actuators, and other IoT objects. The intent is to build a common framework for Industrial IoT edge computing. The EdgeX platform enables and encourages the rapidly growing community of IoT solution providers to work together in an ecosystem of interoperable components to reduce uncertainty, accelerate time to market, and facilitate scale. By bringing this much-needed interoperability, EdgeX makes it easier to monitor physical world items, send instructions to them, collect data from them, move the data across the fog up to the cloud where it may be stored, aggregated, analyzed, and turned into information, actuated, and acted upon. So EdgeX enables data to travel northwards towards the Cloud and also laterally to other gateways, or back to devices, sensors, and actuators. The initiative is aligned around a common goal: the simplification and standardization of the foundation for tiered edge computing architectures in the industrial IoT market while still enabling the ecosystem to provide significant value-added differentiation. If you don't need further description and want to immediately use EdgeX Foundry use this link: Getting Started Guide Definitions: \"South Side\" and \"North Side\" South Side: All IoT objects, within the physical realm, and the edge of the network that communicates directly with those devices, sensors, actuators, and other IoT objects, and collects the data from them, is known collectively as the \"South Side.\" North Side: The Cloud (or Enterprise system) where data is collected, stored, aggregated, analyzed, and turned into information, and the part of the network that communicates with the Cloud, is referred to as the \"north side\" of the network. EdgeX enables data to be sent \"north,\" \"south,\" or laterally as needed and as directed. EdgeX Foundry Architectural Tenets EdgeX Foundry was conceived with the following tenets guiding the overall architecture: EdgeX Foundry must be platform agnostic with regard to Hardware Operating system (Linux, Windows, etc.) Distribution - it must allow for the distribution of functionality through microservices at the edge, on a gateway, in the fog, on cloud, etc. Protocol and sensor agnostic EdgeX Foundry must be extremely flexible Any part of the platform may be upgraded, replaced or augmented by other micro services or software components Allow services to scale up and down based on device capability and use case EdgeX Foundry should provide \"reference implementation\" services but encourages best of breed solutions EdgeX Foundry must provide for store and forward capability (to support disconnected/remote edge systems) EdgeX Foundry must support and facilitate \"intelligence\" moving closer to the edge in order to address Actuation latency concerns Bandwidth and storage concerns Operating remotely concerns EdgeX Foundry must support brown and green device/sensor field deployments EdgeX Foundry must be secure and easily managed EdgeX Foundry Service Layers EdgeX Foundry is a collection of open source microservices. These microservices are organized into 4 service layers, and 2 underlying augmenting system services. The Service Layers traverse from the edge of the physical realm from the Device Services Layer, to the edge of the information realm of the Export Services Layer, with the Core Services Layer at the center. The 4 Service Layers of EdgeX Foundry are as follows: Core Services Layer Supporting Services Layer Export Services Layer Device Services Layer The 2 underlying System Services of EdgeX Foundry are as follows: Security System Management Core Services Layer The Core Services (CS) Layer separates the north side and south side layers at the edge. Core services include the following components: Core data: a persistence repository and associated management service for data collected from the south side objects. Command: a service that facilitates and controls actuation requests from the north side to the south side. Metadata: a repository and associated management service of metadata about the objects that are connected to EdgeX Foundry. Provides the capability to provision new devices and pair them with their owning device services. Registry and Configuration: provides other EdgeX Foundry microservices with information about associated services within EdgeX Foundry and microservices configuration properties (i.e. - a repository of initialization values). Supporting Services Layer The Supporting Services (SS) Layer encompass a wide range of microservices that provide the edge analytics and intelligence, and provide service to EdgeX Foundry itself. Normal software application duties such as logging, scheduling, and data clean up (scrubbing) are performed by microservices in the SS Layer. The rules engines, and alerting and notification microservices are within the SS Layer because they operate on top of the Core Services Layer. The local analytics capability (implemented today as a simple rules engine) is also located in this layer. Device Services Layer The Device Services Layer interacts with Device Services. Device Services (DS) are the edge connectors interacting with the Devices or IoT objects (the \"things\") that include, but are not limited to: alarm systems, heating and air conditioning systems in homes and office buildings, lights, machines in any industry, irrigation systems, drones, currently automated transit such as some rail systems, currently automated factories, and appliances in your home. In the future, this may include driverless cars and trucks, traffic signals, fully automated fast food facilities, fully automated self-serve grocery stores, devices taking medical readings from patients etc. Device services may service one or a number of devices (sensor, actuator, and so forth) at one time. A \"device\" that a DS manages, could be something other than a simple single physical device and could be another gateway (and all of that gateway's devices); a device manager; or a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The DS layer's microservices communicate with the devices, sensors, actuators, and other IoT objects through protocols native to each IoT object. The DS Layer converts the data produced and communicated by the IoT object into a common EdgeX Foundry data structure, and sends that converted data into the Core Services layer, and to other microservices in other layers of EdgeX Foundry. EdgeX Foundry provides device service software developer kits (SDK) for generating the shell of a device service. There are SDKs in Go and C to support the creation of device services in the most convenient language for your use case. SDKs make the creation of new device services easier and provides connector code to the Core Services Layer (as well as other services). Examples of what work a Device Service does A BACnet DS converts the BACNet device-supplied temperature and humidity readings into a common EdgeX Foundry object data structure. A DS receives and translates commands from other EdgeX Foundry services or enterprise systems, and communicates those requests to the devices for actuation in a programming language that the device understands. A DS may receive a request to turn off a Modbus PLC-controlled motor. The DS would translate the generic EdgeX Foundry \"shutoff\" request into a Modbus serial command that the PLC-controlled motor understands for actuation. System Services Layer Security Infrastructure Security elements both inside and outside of EdgeX Foundry protect the data and command of devices, sensors, and other IoT objects managed by EdgeX Foundry. System Management System Management facilities provide the central point of contact for external management systems to * start/stop/restart EdgeX services * get metrics on the EdgeX services (such as memory usage) so that the EdgeX services can be monitored In future releases, the EdgeX system management capability will expand to include being able to set service configuration, provide a status/health check of all of the services, and providing other performance and operational information to management platforms.","title":"Introduction"},{"location":"#introduction","text":"EdgeX Foundry is a vendor-neutral open source software platform at the edge of the network, that interacts with the physical, every day working world of devices, sensors, actuators, and other IoT objects. The intent is to build a common framework for Industrial IoT edge computing. The EdgeX platform enables and encourages the rapidly growing community of IoT solution providers to work together in an ecosystem of interoperable components to reduce uncertainty, accelerate time to market, and facilitate scale. By bringing this much-needed interoperability, EdgeX makes it easier to monitor physical world items, send instructions to them, collect data from them, move the data across the fog up to the cloud where it may be stored, aggregated, analyzed, and turned into information, actuated, and acted upon. So EdgeX enables data to travel northwards towards the Cloud and also laterally to other gateways, or back to devices, sensors, and actuators. The initiative is aligned around a common goal: the simplification and standardization of the foundation for tiered edge computing architectures in the industrial IoT market while still enabling the ecosystem to provide significant value-added differentiation. If you don't need further description and want to immediately use EdgeX Foundry use this link: Getting Started Guide","title":"Introduction"},{"location":"#definitions-south-side-and-north-side","text":"South Side: All IoT objects, within the physical realm, and the edge of the network that communicates directly with those devices, sensors, actuators, and other IoT objects, and collects the data from them, is known collectively as the \"South Side.\" North Side: The Cloud (or Enterprise system) where data is collected, stored, aggregated, analyzed, and turned into information, and the part of the network that communicates with the Cloud, is referred to as the \"north side\" of the network. EdgeX enables data to be sent \"north,\" \"south,\" or laterally as needed and as directed.","title":"Definitions: \"South Side\" and \"North Side\""},{"location":"#edgex-foundry-architectural-tenets","text":"EdgeX Foundry was conceived with the following tenets guiding the overall architecture: EdgeX Foundry must be platform agnostic with regard to Hardware Operating system (Linux, Windows, etc.) Distribution - it must allow for the distribution of functionality through microservices at the edge, on a gateway, in the fog, on cloud, etc. Protocol and sensor agnostic EdgeX Foundry must be extremely flexible Any part of the platform may be upgraded, replaced or augmented by other micro services or software components Allow services to scale up and down based on device capability and use case EdgeX Foundry should provide \"reference implementation\" services but encourages best of breed solutions EdgeX Foundry must provide for store and forward capability (to support disconnected/remote edge systems) EdgeX Foundry must support and facilitate \"intelligence\" moving closer to the edge in order to address Actuation latency concerns Bandwidth and storage concerns Operating remotely concerns EdgeX Foundry must support brown and green device/sensor field deployments EdgeX Foundry must be secure and easily managed","title":"EdgeX Foundry Architectural Tenets"},{"location":"#edgex-foundry-service-layers","text":"EdgeX Foundry is a collection of open source microservices. These microservices are organized into 4 service layers, and 2 underlying augmenting system services. The Service Layers traverse from the edge of the physical realm from the Device Services Layer, to the edge of the information realm of the Export Services Layer, with the Core Services Layer at the center. The 4 Service Layers of EdgeX Foundry are as follows: Core Services Layer Supporting Services Layer Export Services Layer Device Services Layer The 2 underlying System Services of EdgeX Foundry are as follows: Security System Management","title":"EdgeX Foundry Service Layers"},{"location":"#core-services-layer","text":"The Core Services (CS) Layer separates the north side and south side layers at the edge. Core services include the following components: Core data: a persistence repository and associated management service for data collected from the south side objects. Command: a service that facilitates and controls actuation requests from the north side to the south side. Metadata: a repository and associated management service of metadata about the objects that are connected to EdgeX Foundry. Provides the capability to provision new devices and pair them with their owning device services. Registry and Configuration: provides other EdgeX Foundry microservices with information about associated services within EdgeX Foundry and microservices configuration properties (i.e. - a repository of initialization values).","title":"Core Services Layer"},{"location":"#supporting-services-layer","text":"The Supporting Services (SS) Layer encompass a wide range of microservices that provide the edge analytics and intelligence, and provide service to EdgeX Foundry itself. Normal software application duties such as logging, scheduling, and data clean up (scrubbing) are performed by microservices in the SS Layer. The rules engines, and alerting and notification microservices are within the SS Layer because they operate on top of the Core Services Layer. The local analytics capability (implemented today as a simple rules engine) is also located in this layer.","title":"Supporting Services Layer"},{"location":"#device-services-layer","text":"The Device Services Layer interacts with Device Services. Device Services (DS) are the edge connectors interacting with the Devices or IoT objects (the \"things\") that include, but are not limited to: alarm systems, heating and air conditioning systems in homes and office buildings, lights, machines in any industry, irrigation systems, drones, currently automated transit such as some rail systems, currently automated factories, and appliances in your home. In the future, this may include driverless cars and trucks, traffic signals, fully automated fast food facilities, fully automated self-serve grocery stores, devices taking medical readings from patients etc. Device services may service one or a number of devices (sensor, actuator, and so forth) at one time. A \"device\" that a DS manages, could be something other than a simple single physical device and could be another gateway (and all of that gateway's devices); a device manager; or a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The DS layer's microservices communicate with the devices, sensors, actuators, and other IoT objects through protocols native to each IoT object. The DS Layer converts the data produced and communicated by the IoT object into a common EdgeX Foundry data structure, and sends that converted data into the Core Services layer, and to other microservices in other layers of EdgeX Foundry. EdgeX Foundry provides device service software developer kits (SDK) for generating the shell of a device service. There are SDKs in Go and C to support the creation of device services in the most convenient language for your use case. SDKs make the creation of new device services easier and provides connector code to the Core Services Layer (as well as other services). Examples of what work a Device Service does A BACnet DS converts the BACNet device-supplied temperature and humidity readings into a common EdgeX Foundry object data structure. A DS receives and translates commands from other EdgeX Foundry services or enterprise systems, and communicates those requests to the devices for actuation in a programming language that the device understands. A DS may receive a request to turn off a Modbus PLC-controlled motor. The DS would translate the generic EdgeX Foundry \"shutoff\" request into a Modbus serial command that the PLC-controlled motor understands for actuation.","title":"Device Services Layer"},{"location":"#system-services-layer","text":"Security Infrastructure Security elements both inside and outside of EdgeX Foundry protect the data and command of devices, sensors, and other IoT objects managed by EdgeX Foundry. System Management System Management facilities provide the central point of contact for external management systems to * start/stop/restart EdgeX services * get metrics on the EdgeX services (such as memory usage) so that the EdgeX services can be monitored In future releases, the EdgeX system management capability will expand to include being able to set service configuration, provide a status/health check of all of the services, and providing other performance and operational information to management platforms.","title":"System Services Layer"},{"location":"api/Ch-APIDeviceServicesVirtual/","text":"APIs - Device Services - Virtual Device Service Architecture Reference For a description of the architecture, see Virtual Device SDK Reference To use the Software Development Kit (SDK), see Getting Started Introduction The Virtual Device Service simulates different kinds of devices to generate Events and Readings to Core Data Microservice and Users can send commands and get responses through the Command and Control Microservice. The ability to do these activities is very useful when executing functional or performance tests without any real devices. This version of Virtual Device Service is implemented based on the Device Service SDK. https://github.com/edgexfoundry/device-virtual/blob/master/raml/device-virtual.raml","title":"APIs - Device Services - Virtual Device Service"},{"location":"api/Ch-APIDeviceServicesVirtual/#apis-device-services-virtual-device-service","text":"","title":"APIs - Device Services - Virtual Device Service"},{"location":"api/Ch-APIDeviceServicesVirtual/#architecture-reference","text":"For a description of the architecture, see Virtual Device","title":"Architecture Reference"},{"location":"api/Ch-APIDeviceServicesVirtual/#sdk-reference","text":"To use the Software Development Kit (SDK), see Getting Started","title":"SDK Reference"},{"location":"api/Ch-APIDeviceServicesVirtual/#introduction","text":"The Virtual Device Service simulates different kinds of devices to generate Events and Readings to Core Data Microservice and Users can send commands and get responses through the Command and Control Microservice. The ability to do these activities is very useful when executing functional or performance tests without any real devices. This version of Virtual Device Service is implemented based on the Device Service SDK. https://github.com/edgexfoundry/device-virtual/blob/master/raml/device-virtual.raml","title":"Introduction"},{"location":"api/Ch-APISystemManagement/","text":"APIs - System Management - Agent Architecture Reference Coming Soon Introduction The EdgeX System Management Agent (SMA) exposes the EdgeX management service API to 3rd party systems. In other words, the Agent serves as a proxy for system management service API calls into each micro service. In the future, the SMA may also offer the management API in other remote management/control system protocols like LWM2M, OMA DM, etc. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/system-agent.raml System Management V1 API Swagger Documentation","title":"APIs - System Management - Agent"},{"location":"api/Ch-APISystemManagement/#apis-system-management-agent","text":"","title":"APIs - System Management - Agent"},{"location":"api/Ch-APISystemManagement/#architecture-reference","text":"Coming Soon","title":"Architecture Reference"},{"location":"api/Ch-APISystemManagement/#introduction","text":"The EdgeX System Management Agent (SMA) exposes the EdgeX management service API to 3rd party systems. In other words, the Agent serves as a proxy for system management service API calls into each micro service. In the future, the SMA may also offer the management API in other remote management/control system protocols like LWM2M, OMA DM, etc. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/system-agent.raml System Management V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreCommand/","text":"Core Command Architecture Reference For a description of the architecture, see Core-Command Introduction EdgeX Foundry's Command microservice is a conduit for other services to trigger action on devices and sensors through their managing Device Services. The service provides an API to get the list of commands that can be issued for all devices or a single device. Commands are divided into two groups for each device: GET commands are issued to a device or sensor to get a current value for a particular attribute on the device, such as the current temperature provided by a thermostat sensor, or the on/off status of a light. PUT commands are issued to a device or sensor to change the current state or status of a device or one of its attributes, such as setting the speed in RPMs of a motor, or setting the brightness of a dimmer light. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-command.raml Core Command V1 API Swagger Documentation","title":"Core Command"},{"location":"api/core/Ch-APICoreCommand/#core-command","text":"","title":"Core Command"},{"location":"api/core/Ch-APICoreCommand/#architecture-reference","text":"For a description of the architecture, see Core-Command","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreCommand/#introduction","text":"EdgeX Foundry's Command microservice is a conduit for other services to trigger action on devices and sensors through their managing Device Services. The service provides an API to get the list of commands that can be issued for all devices or a single device. Commands are divided into two groups for each device: GET commands are issued to a device or sensor to get a current value for a particular attribute on the device, such as the current temperature provided by a thermostat sensor, or the on/off status of a light. PUT commands are issued to a device or sensor to change the current state or status of a device or one of its attributes, such as setting the speed in RPMs of a motor, or setting the brightness of a dimmer light. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-command.raml Core Command V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreData/","text":"Core Data Architecture Reference For a description of the architecture, see Core-Data Introduction EdgeX Foundry Core Data Service includes the device and sensor collected data database and APIs to expose the database to other services as well as north-bound integration. The database is secure. Direct access to the database is restricted to the Core Data service APIs. Core Data also provides the REST API to create and register a new device. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-data.raml Core Data V1 API Swagger Documentation","title":"Core Data"},{"location":"api/core/Ch-APICoreData/#core-data","text":"","title":"Core Data"},{"location":"api/core/Ch-APICoreData/#architecture-reference","text":"For a description of the architecture, see Core-Data","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreData/#introduction","text":"EdgeX Foundry Core Data Service includes the device and sensor collected data database and APIs to expose the database to other services as well as north-bound integration. The database is secure. Direct access to the database is restricted to the Core Data service APIs. Core Data also provides the REST API to create and register a new device. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-data.raml Core Data V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreMetadata/","text":"Core Metadata Architecture Reference For a description of the architecture, see Core-Metadata Introduction The Metadata microservice includes the device/sensor metadata database and APIs to expose the database to other services. In particular, the device provisioning service deposits and manages device metadata through this service. This service may also hold and manage other configuration metadata used by other services on the gateway such as clean up schedules, hardware configuration (Wi-Fi connection info, MQTT queues, and so forth). Non-device metadata may need to be held in a different database and/or managed by another service--depending upon implementation. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-metadata.raml Core Metadata V1 API Swagger Documentation","title":"Core Metadata"},{"location":"api/core/Ch-APICoreMetadata/#core-metadata","text":"","title":"Core Metadata"},{"location":"api/core/Ch-APICoreMetadata/#architecture-reference","text":"For a description of the architecture, see Core-Metadata","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreMetadata/#introduction","text":"The Metadata microservice includes the device/sensor metadata database and APIs to expose the database to other services. In particular, the device provisioning service deposits and manages device metadata through this service. This service may also hold and manage other configuration metadata used by other services on the gateway such as clean up schedules, hardware configuration (Wi-Fi connection info, MQTT queues, and so forth). Non-device metadata may need to be held in a different database and/or managed by another service--depending upon implementation. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-metadata.raml Core Metadata V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreServiceConfiguration/","text":"Configuration and Registry Architecture Reference For a description of the architecture, see Configuration Introduction The RESTful APIs are provided by Consul directly, and several communities supply Consul client libraries for different programming languages, including Go (official), Python, Java, PHP, Scala, Erlang/OTP, Ruby, Node.js, and C#. For the client libraries of different languages, please refer to the list of this page: https://www.consul.io/downloads_tools.html Configuration Management For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/kv.html https://www.consul.io/docs/agent/http/kv.html Service Registry For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/services.html https://www.consul.io/docs/agent/http/catalog.html https://www.consul.io/docs/agent/http/agent.html https://www.consul.io/docs/agent/checks.html https://www.consul.io/docs/agent/http/health.html Service Registration While each microservice is starting up, it should connect to Consul to register its endpoint information, including microservice ID, address, port number, and health checking method. After that, other microservices can locate its URL from Consul, and Consul has the ability to monitor its health status. The RESTful API of registration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_register Service Deregistration Before microservices shut down, they have to deregister themselves from Consul. The RESTful API of deregistration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_deregister Service Discovery Service Discovery feature allows client micro services to query the endpoint information of a particular microservice by its microservice IDor list all available services registered in Consul. The RESTful API of querying service by microservice IDis described on the following Consul page: https://www.consul.io/docs/agent/http/catalog.html#catalog_service The RESTful API of listing all available services is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_services Health Checking Health checking is a critical feature that prevents using services that are unhealthy. Consul provides a variety of methods to check the health of services, including Script + Interval, HTTP + Interval, TCP + Interval, Time to Live (TTL), and Docker + Interval. The detailed introduction and examples of each checking methods are described on the following Consul page: https://www.consul.io/docs/agent/checks.html The health checks should be established during service registration. Please see the paragraph on this page of Service Registration section.","title":"Configuration and Registry"},{"location":"api/core/Ch-APICoreServiceConfiguration/#configuration-and-registry","text":"","title":"Configuration and Registry"},{"location":"api/core/Ch-APICoreServiceConfiguration/#architecture-reference","text":"For a description of the architecture, see Configuration","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreServiceConfiguration/#introduction","text":"The RESTful APIs are provided by Consul directly, and several communities supply Consul client libraries for different programming languages, including Go (official), Python, Java, PHP, Scala, Erlang/OTP, Ruby, Node.js, and C#. For the client libraries of different languages, please refer to the list of this page: https://www.consul.io/downloads_tools.html","title":"Introduction"},{"location":"api/core/Ch-APICoreServiceConfiguration/#configuration-management","text":"For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/kv.html https://www.consul.io/docs/agent/http/kv.html","title":"Configuration Management"},{"location":"api/core/Ch-APICoreServiceConfiguration/#service-registry","text":"For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/services.html https://www.consul.io/docs/agent/http/catalog.html https://www.consul.io/docs/agent/http/agent.html https://www.consul.io/docs/agent/checks.html https://www.consul.io/docs/agent/http/health.html Service Registration While each microservice is starting up, it should connect to Consul to register its endpoint information, including microservice ID, address, port number, and health checking method. After that, other microservices can locate its URL from Consul, and Consul has the ability to monitor its health status. The RESTful API of registration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_register Service Deregistration Before microservices shut down, they have to deregister themselves from Consul. The RESTful API of deregistration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_deregister Service Discovery Service Discovery feature allows client micro services to query the endpoint information of a particular microservice by its microservice IDor list all available services registered in Consul. The RESTful API of querying service by microservice IDis described on the following Consul page: https://www.consul.io/docs/agent/http/catalog.html#catalog_service The RESTful API of listing all available services is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_services Health Checking Health checking is a critical feature that prevents using services that are unhealthy. Consul provides a variety of methods to check the health of services, including Script + Interval, HTTP + Interval, TCP + Interval, Time to Live (TTL), and Docker + Interval. The detailed introduction and examples of each checking methods are described on the following Consul page: https://www.consul.io/docs/agent/checks.html The health checks should be established during service registration. Please see the paragraph on this page of Service Registration section.","title":"Service Registry"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/","text":"Alerts & Notifications Architecture Reference For a description of the architecture, see Alerts and Notifications Introduction When a person or a system needs to be informed of something discovered on the node by another microservice on the node, EdgeX Foundry's Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services might need to broadcast include sensor data detected outside of certain parameters, usually detected by a Rules Engine service, or a system or service malfunction, usually detected by system management services. https://github.com/edgexfoundry/support-notifications/blob/master/raml/support-notifications.raml Alerts and Notifications V1 API Swagger Documentation","title":"Alerts & Notifications"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/#alerts-notifications","text":"","title":"Alerts &amp; Notifications"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/#architecture-reference","text":"For a description of the architecture, see Alerts and Notifications","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/#introduction","text":"When a person or a system needs to be informed of something discovered on the node by another microservice on the node, EdgeX Foundry's Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services might need to broadcast include sensor data detected outside of certain parameters, usually detected by a Rules Engine service, or a system or service malfunction, usually detected by system management services. https://github.com/edgexfoundry/support-notifications/blob/master/raml/support-notifications.raml Alerts and Notifications V1 API Swagger Documentation","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesLogging/","text":"Logging Architecture Reference For a description of the architecture, see Logging Introduction Logging Service is a microservice featuring REST API for other microservices to add, query, and delete logging requests. Two options of persistence--file or mongodb--are supported and configurable through the property file or remote consul configuration service. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-logging.raml Logging V1 API Swagger Documentation","title":"Logging"},{"location":"api/supporting/Ch-APISupportingServicesLogging/#logging","text":"","title":"Logging"},{"location":"api/supporting/Ch-APISupportingServicesLogging/#architecture-reference","text":"For a description of the architecture, see Logging","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesLogging/#introduction","text":"Logging Service is a microservice featuring REST API for other microservices to add, query, and delete logging requests. Two options of persistence--file or mongodb--are supported and configurable through the property file or remote consul configuration service. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-logging.raml Logging V1 API Swagger Documentation","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/","text":"Rules Engine Architecture Reference For a description of the architecture, see Rules Engine Introduction EdgeX Foundry Rules Engine Microservice receives data from the Export Service through 0MQ, and then triggers actuation based on event data it receives and analyzes. Built on Drools technology. https://github.com/edgexfoundry/support-rulesengine/blob/master/raml/support-rulesengine.raml","title":"Rules Engine"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/#rules-engine","text":"","title":"Rules Engine"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/#architecture-reference","text":"For a description of the architecture, see Rules Engine","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/#introduction","text":"EdgeX Foundry Rules Engine Microservice receives data from the Export Service through 0MQ, and then triggers actuation based on event data it receives and analyzes. Built on Drools technology. https://github.com/edgexfoundry/support-rulesengine/blob/master/raml/support-rulesengine.raml","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/","text":"Scheduling Architecture Reference For a description of the architecture, see Scheduling Introduction The following API RESTful Web Service(s) for EdgeX Foundry. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-scheduling.raml Scheduling V1 API Swagger Documentation Description Scheduler Service - a service that can be used to schedule invocation of a URL. Requires the use of interval(s), and interval action(s). Interval(s) : - name - unique name of the service. - start - identifies when the operation starts. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means now. - end - identifies when the operation ends. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means never - frequency - identifies the interval between invocations. Expressed in ISO 8601 PxYxMxDTxHxMxS format. Empty means no frequency. Interval Action(s) : - name - unique name of the interval action. - interval - unique name of an existing interval. - target - the recipient of the interval action (ergo service or name). - protocol - the protocol type to be used to contact the target. (example HTTP). - httpMethod - HTTP protocol verb. - address - the endpoint server host. - port - the desired port. - path - the api path which will be acted on. - parameters - (optional) parameters which will be included in the BODY tag for HttpMethods. Any parameters that should be provided to the call, e.g. {\"milliseconds\":86400000} Examples Create an interval upon which the scheduler will operate : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"midnight\", \"start\": \"20180101T000000\", \"frequency\": \"P1D\"}' \" http://localhost:48081/api/v1/interval \" Example of a second interval which will run every 20 seconds : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"every20s\", \"start\":\"20000101T000000\", \"end\":\"\", \"frequency\":\"PT20S\"}' \" http://localhost:48081/api/v1/interval \" Create an interval action that will invoke the interval action (drive the scrubber) in core-data : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"scrub-pushed-events\", \"interval\": \"midnight\", \"target\": \"core-data\", \"protocol\": \"http\", \"httpMethod\": \"DELETE\", \"address\": \"localhost\", \"port\": 48080, \"path\": \"/api/v1/event/scrub\"}' \" http://localhost:48085/api/v1/intervalaction \" This is a Random-Boolean-Device which created by edgex-device-virtual that connects every 20 seconds. : curl -X POST -H \"Content-Type: application/json\" -d '{ : \"name\": \"put-action\", \"interval\": \"every20s\", \"target\": \"edgex-device-modbus\", \"protocol\": \"http\", \"httpMethod\": \"PUT\", \"address\": \"localhost\", \"port\": 49990, \"path\":\"/api/v1/device/name/Random-Boolean-Device/RandomValue_Bool\", \"parameters\": \"{\"RandomValue_Bool\": \"true\",\"EnableRandomization_Bool\": \"true\"}\" }' \" http://localhost:48085/api/v1/intervalaction \"","title":"Scheduling"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#scheduling","text":"","title":"Scheduling"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#architecture-reference","text":"For a description of the architecture, see Scheduling","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#introduction","text":"The following API RESTful Web Service(s) for EdgeX Foundry. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-scheduling.raml Scheduling V1 API Swagger Documentation","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#description","text":"Scheduler Service - a service that can be used to schedule invocation of a URL. Requires the use of interval(s), and interval action(s). Interval(s) : - name - unique name of the service. - start - identifies when the operation starts. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means now. - end - identifies when the operation ends. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means never - frequency - identifies the interval between invocations. Expressed in ISO 8601 PxYxMxDTxHxMxS format. Empty means no frequency. Interval Action(s) : - name - unique name of the interval action. - interval - unique name of an existing interval. - target - the recipient of the interval action (ergo service or name). - protocol - the protocol type to be used to contact the target. (example HTTP). - httpMethod - HTTP protocol verb. - address - the endpoint server host. - port - the desired port. - path - the api path which will be acted on. - parameters - (optional) parameters which will be included in the BODY tag for HttpMethods. Any parameters that should be provided to the call, e.g. {\"milliseconds\":86400000}","title":"Description"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#examples","text":"Create an interval upon which the scheduler will operate : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"midnight\", \"start\": \"20180101T000000\", \"frequency\": \"P1D\"}' \" http://localhost:48081/api/v1/interval \" Example of a second interval which will run every 20 seconds : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"every20s\", \"start\":\"20000101T000000\", \"end\":\"\", \"frequency\":\"PT20S\"}' \" http://localhost:48081/api/v1/interval \" Create an interval action that will invoke the interval action (drive the scrubber) in core-data : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"scrub-pushed-events\", \"interval\": \"midnight\", \"target\": \"core-data\", \"protocol\": \"http\", \"httpMethod\": \"DELETE\", \"address\": \"localhost\", \"port\": 48080, \"path\": \"/api/v1/event/scrub\"}' \" http://localhost:48085/api/v1/intervalaction \" This is a Random-Boolean-Device which created by edgex-device-virtual that connects every 20 seconds. : curl -X POST -H \"Content-Type: application/json\" -d '{ : \"name\": \"put-action\", \"interval\": \"every20s\", \"target\": \"edgex-device-modbus\", \"protocol\": \"http\", \"httpMethod\": \"PUT\", \"address\": \"localhost\", \"port\": 49990, \"path\":\"/api/v1/device/name/Random-Boolean-Device/RandomValue_Bool\", \"parameters\": \"{\"RandomValue_Bool\": \"true\",\"EnableRandomization_Bool\": \"true\"}\" }' \" http://localhost:48085/api/v1/intervalaction \"","title":"Examples"},{"location":"design/","text":"Architecture Decision Records Folder This folder contains EdgeX Foundry decision records (ADR) and legacy design / requirement documents. /design /adr (architecture decision Records) /legacy-design (legacy design documents) /legacy-requirements (legacy requirement documents) At the root of the ADR folder (/design/adr) are decisions that are relevant to multiple parts of the project (aka \ufffd cross cutting concerns ). Sub folders under the ADR folder contain decisions relevant to the specific area of the project and essentially set up along working group lines (security, core, application, etc.). Naming and Formatting ADR documents are requested to follow RFC (request for comments) naming standard. Specifically, authors should name their documents with a sequentially increasing integer (or serial number) and then the architectural design topic: (sequence number - topic). Example: 0001-SeparateConfigurationInterface. The sequence is a global sequence for all EdgeX ADR. Per RFC and Michael Nygard suggestions the makeup of the ADR document should generally include: Title Status (proposed, accepted, rejected, deprecated, superseded, etc.) Context and Proposed Design Decision Consequences/considerations References Document history is maintained via Github history. Ownership EdgeX WG chairman own the sub folder and included documents associated to their work group. The EdgeX TSC chair/vice chair are responsible for the root level, cross cutting concern documents. Review and Approval ADR\u2019s shall be submitted as PRs to the appropriate edgex-docs folder based on the Architecture Decision Records Folder section above. The status of the PR (inside the document) shall be listed as proposed during this period. The PRs shall be left open (not merged) so that comments against the PR can be collected during the proposal period. The PRs can be approved and merged only after a formal vote of approval is conducted by the TSC. On approval of the ADR by the TSC, the status of the ADR should be changed to accepted . If the ADR is not approved by the TSC, the status in the document should be changed to rejected and the PR closed. Legacy A separate folder (/design/legacy-design) is used for legacy design/architecture decisions. A separate folder (/design/legacy-requirements) is used for legacy requirements documents. WG chairman take the responsibility for posting legacy material in to the applicable folders. Table of Contents A README with a table of contents for current documents is located here . Legacy Design and Requirements have their own Table of Contents as well and are located in their respective directories at /legacy-design and /legacy-requirements . Document authors are asked to keep the TOC updated with each new document entry.","title":"Architecture Decision Records Folder"},{"location":"design/#architecture-decision-records-folder","text":"This folder contains EdgeX Foundry decision records (ADR) and legacy design / requirement documents. /design /adr (architecture decision Records) /legacy-design (legacy design documents) /legacy-requirements (legacy requirement documents) At the root of the ADR folder (/design/adr) are decisions that are relevant to multiple parts of the project (aka \ufffd cross cutting concerns ). Sub folders under the ADR folder contain decisions relevant to the specific area of the project and essentially set up along working group lines (security, core, application, etc.).","title":"Architecture Decision Records Folder"},{"location":"design/#naming-and-formatting","text":"ADR documents are requested to follow RFC (request for comments) naming standard. Specifically, authors should name their documents with a sequentially increasing integer (or serial number) and then the architectural design topic: (sequence number - topic). Example: 0001-SeparateConfigurationInterface. The sequence is a global sequence for all EdgeX ADR. Per RFC and Michael Nygard suggestions the makeup of the ADR document should generally include: Title Status (proposed, accepted, rejected, deprecated, superseded, etc.) Context and Proposed Design Decision Consequences/considerations References Document history is maintained via Github history.","title":"Naming and Formatting"},{"location":"design/#ownership","text":"EdgeX WG chairman own the sub folder and included documents associated to their work group. The EdgeX TSC chair/vice chair are responsible for the root level, cross cutting concern documents.","title":"Ownership"},{"location":"design/#review-and-approval","text":"ADR\u2019s shall be submitted as PRs to the appropriate edgex-docs folder based on the Architecture Decision Records Folder section above. The status of the PR (inside the document) shall be listed as proposed during this period. The PRs shall be left open (not merged) so that comments against the PR can be collected during the proposal period. The PRs can be approved and merged only after a formal vote of approval is conducted by the TSC. On approval of the ADR by the TSC, the status of the ADR should be changed to accepted . If the ADR is not approved by the TSC, the status in the document should be changed to rejected and the PR closed.","title":"Review and Approval"},{"location":"design/#legacy","text":"A separate folder (/design/legacy-design) is used for legacy design/architecture decisions. A separate folder (/design/legacy-requirements) is used for legacy requirements documents. WG chairman take the responsibility for posting legacy material in to the applicable folders.","title":"Legacy"},{"location":"design/#table-of-contents","text":"A README with a table of contents for current documents is located here . Legacy Design and Requirements have their own Table of Contents as well and are located in their respective directories at /legacy-design and /legacy-requirements . Document authors are asked to keep the TOC updated with each new document entry.","title":"Table of Contents"},{"location":"design/TOC/","text":"ADR Table of Contents Name/Link Short Description 0001 Registry Refactor Separate out Registry and Configuration APIs 0002 Array Datatypes Allow Arrays to be held in Readings 0003 V2 API Principles Principles and Goals of V2 API Design 0004 Feature Flags Feature Flag Implementation 0005 Service Self Config Init Service Self Config Init & Config Seed Removal","title":"ADR Table of Contents"},{"location":"design/TOC/#adr-table-of-contents","text":"Name/Link Short Description 0001 Registry Refactor Separate out Registry and Configuration APIs 0002 Array Datatypes Allow Arrays to be held in Readings 0003 V2 API Principles Principles and Goals of V2 API Design 0004 Feature Flags Feature Flag Implementation 0005 Service Self Config Init Service Self Config Init & Config Seed Removal","title":"ADR Table of Contents"},{"location":"design/adr/0001-Registy-Refactor/","text":"Registry Refactoring Design Status Context Proposed Design Decision Consequences References Status Approved Context Currently the Registry Client in go-mod-registry module provides Service Configuration and Service Registration functionality. The goal of this design is to refactor the go-mod-registry module for separation of concerns. The Service Registry functionality will stay in the go-mod-registry module and the Service Configuration functionality will be separated out into a new go-mod-configuration module. This allows for implementations for deferent providers for each, another aspect of separation of concerns. Proposed Design Provider Connection information An aspect of using the current Registry Client is \" Where do the services get the Registry Provider connection information? \" Currently all services either pull this connection information from the local configuration file or from the edgex_registry environment variable. Device Services also have the option to specify this connection information on the command line. With the refactoring for separation of concerns, this issue changes to \" Where do the services get the Configuration Provider connection information? \" There have been concerns voiced by some in the EdgeX community that storing this Configuration Provider connection information in the configuration which ultimately is provided by that provider is not the right design. This design proposes that all services will use the command line option approach with the ability to override with an environment variable. The Configuration Provider information will not be stored in each service's local configuration file. The edgex_registry environment variable will be deprecated. The Registry Provider connection information will continue to be stored in each service's configuration either locally or from the Configuration Provider same as all other EdgeX Client and Database connection information. Command line option changes The new -cp/-configProvider command line option will be added to each service which will have a value specified using the format {type}.{protocol}://{host}:{port} e.g consul.http://localhost:8500 . This new command line option will be overridden by the edgex_configuration_provider environment variable when it is set. This environment variable's value has the same format as the command line option value. If no value is provided to the -cp/-configProvider option, i.e. just -cp , and no environment variable override is specified, the default value of consul.http://localhost:8500 will be used. if -cp/-configProvider not used and no environment variable override is specified the local configuration file is used, as is it now. All services will log the Configuration Provider connection information that is used. The existing -r/-registry command line option will be retained as a Boolean flag to indicate to use the Registry. Bootstrap Changes All services in the edgex-go mono repo use the new common bootstrap functionality. The plan is to move this code to a go module for the Device Service and App Functions SDKs to also use. The current bootstrap modules pkg/bootstrap/configuration/registry.go and pkg/bootstrap/container/registry.go will be refactored to use the new Configuration Client and be renamed appropriately. New bootstrap modules will be created for using the revised version of Registry Client . The current use of useRegistry and registryClient for service configuration will be change to appropriate names for using the new Configuration Client . The current use of useRegistry and registryClient for service registration will be retained for service registration. Call to the new Unregister() API will be added to shutdown code for all services. Config-Seed Changes The conf-seed service will have similar changes for specifying the Configuration Provider connection information since it doesn't use the common bootstrap package. Beyond that it will have minor changes for switching to using the Configuration Client interface, which will just be imports and appropriate name refactoring. Config Endpoint Changes Since the Configuration Provider connection information will no longer be in the service's configuration struct, the config endpoint processing will be modified to add the Configuration Provider connection information to the resulting JSON create from service's configuration. Client Interfaces changes Current Registry Client This following is the current Registry Client Interface type Client interface { Register () error HasConfiguration () ( bool , error ) PutConfigurationToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error } New Configuration Client This following is the new Configuration Client Interface which contains the Service Configuration specific portion from the above current Registry Client . type Client interface { HasConfiguration () ( bool , error ) PutConfigurationFromToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error } Revised Registry Client This following is the revised Registry Client Interface, which contains the Service Registry specific portion from the above current Registry Client . The UnRegister() API has been added per issue #20 type Client interface { Register () error UnRegister () error IsAlive () bool GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error } Client Configuration Structs Current Registry Client Config The following is the current struct used to configure the current Registry Client type Config struct { Protocol string Host string Port int Type string Stem string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string } New Configuration Client Config The following is the new struct the will be used to configure the new Configuration Client from the command line option or environment variable values. The Service Registry portion has been removed from the above existing Registry Client Config type Config struct { Protocol string Host string Port int Type string BasePath string ServiceKey string } New Registry Client Config The following is the revised struct the will be used to configure the new Registry Client from the information in the service's configuration. This is mostly unchanged from the existing Registry Client Config , except that the Stem for configuration has been removed type Config struct { Protocol string Host string Port int Type string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string } Provider Implementations The current Consul implementation of the Registry Client will be split up into implementations for the new Configuration Client in the new go-mod-configuration module and the revised Registry Client in the existing go-mod-registry module. Decision It was decided to move forward with the above design After initial ADR was approved, it was decided to retain the -r/--registry command-line flag and not add the Enabled field in the Registry provider configuration. Consequences Once the refactoring of go-mod-registry and go-mod-configuration are complete, they will need to be integrated into the new go-mod-bootstrap. Part of this integration will be the Command line option changes above. At this point the edgex-go services will be integrated with the new Registry and Configuration providers. The App Services SDK and Device Services SDK will then need to integrate go-mod-bootstrap to take advantage of these new providers. References Registry Abstraction - Decouple EdgeX services from Consul (Previous design)","title":"Registry Refactoring Design"},{"location":"design/adr/0001-Registy-Refactor/#registry-refactoring-design","text":"Status Context Proposed Design Decision Consequences References","title":"Registry Refactoring Design"},{"location":"design/adr/0001-Registy-Refactor/#status","text":"Approved","title":"Status"},{"location":"design/adr/0001-Registy-Refactor/#context","text":"Currently the Registry Client in go-mod-registry module provides Service Configuration and Service Registration functionality. The goal of this design is to refactor the go-mod-registry module for separation of concerns. The Service Registry functionality will stay in the go-mod-registry module and the Service Configuration functionality will be separated out into a new go-mod-configuration module. This allows for implementations for deferent providers for each, another aspect of separation of concerns.","title":"Context"},{"location":"design/adr/0001-Registy-Refactor/#proposed-design","text":"","title":"Proposed Design"},{"location":"design/adr/0001-Registy-Refactor/#provider-connection-information","text":"An aspect of using the current Registry Client is \" Where do the services get the Registry Provider connection information? \" Currently all services either pull this connection information from the local configuration file or from the edgex_registry environment variable. Device Services also have the option to specify this connection information on the command line. With the refactoring for separation of concerns, this issue changes to \" Where do the services get the Configuration Provider connection information? \" There have been concerns voiced by some in the EdgeX community that storing this Configuration Provider connection information in the configuration which ultimately is provided by that provider is not the right design. This design proposes that all services will use the command line option approach with the ability to override with an environment variable. The Configuration Provider information will not be stored in each service's local configuration file. The edgex_registry environment variable will be deprecated. The Registry Provider connection information will continue to be stored in each service's configuration either locally or from the Configuration Provider same as all other EdgeX Client and Database connection information.","title":"Provider Connection information"},{"location":"design/adr/0001-Registy-Refactor/#command-line-option-changes","text":"The new -cp/-configProvider command line option will be added to each service which will have a value specified using the format {type}.{protocol}://{host}:{port} e.g consul.http://localhost:8500 . This new command line option will be overridden by the edgex_configuration_provider environment variable when it is set. This environment variable's value has the same format as the command line option value. If no value is provided to the -cp/-configProvider option, i.e. just -cp , and no environment variable override is specified, the default value of consul.http://localhost:8500 will be used. if -cp/-configProvider not used and no environment variable override is specified the local configuration file is used, as is it now. All services will log the Configuration Provider connection information that is used. The existing -r/-registry command line option will be retained as a Boolean flag to indicate to use the Registry.","title":"Command line option changes"},{"location":"design/adr/0001-Registy-Refactor/#bootstrap-changes","text":"All services in the edgex-go mono repo use the new common bootstrap functionality. The plan is to move this code to a go module for the Device Service and App Functions SDKs to also use. The current bootstrap modules pkg/bootstrap/configuration/registry.go and pkg/bootstrap/container/registry.go will be refactored to use the new Configuration Client and be renamed appropriately. New bootstrap modules will be created for using the revised version of Registry Client . The current use of useRegistry and registryClient for service configuration will be change to appropriate names for using the new Configuration Client . The current use of useRegistry and registryClient for service registration will be retained for service registration. Call to the new Unregister() API will be added to shutdown code for all services.","title":"Bootstrap Changes"},{"location":"design/adr/0001-Registy-Refactor/#config-seed-changes","text":"The conf-seed service will have similar changes for specifying the Configuration Provider connection information since it doesn't use the common bootstrap package. Beyond that it will have minor changes for switching to using the Configuration Client interface, which will just be imports and appropriate name refactoring.","title":"Config-Seed Changes"},{"location":"design/adr/0001-Registy-Refactor/#config-endpoint-changes","text":"Since the Configuration Provider connection information will no longer be in the service's configuration struct, the config endpoint processing will be modified to add the Configuration Provider connection information to the resulting JSON create from service's configuration.","title":"Config Endpoint Changes"},{"location":"design/adr/0001-Registy-Refactor/#client-interfaces-changes","text":"","title":"Client Interfaces changes"},{"location":"design/adr/0001-Registy-Refactor/#current-registry-client","text":"This following is the current Registry Client Interface type Client interface { Register () error HasConfiguration () ( bool , error ) PutConfigurationToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error }","title":"Current Registry Client"},{"location":"design/adr/0001-Registy-Refactor/#new-configuration-client","text":"This following is the new Configuration Client Interface which contains the Service Configuration specific portion from the above current Registry Client . type Client interface { HasConfiguration () ( bool , error ) PutConfigurationFromToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error }","title":"New Configuration Client"},{"location":"design/adr/0001-Registy-Refactor/#revised-registry-client","text":"This following is the revised Registry Client Interface, which contains the Service Registry specific portion from the above current Registry Client . The UnRegister() API has been added per issue #20 type Client interface { Register () error UnRegister () error IsAlive () bool GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error }","title":"Revised Registry Client"},{"location":"design/adr/0001-Registy-Refactor/#client-configuration-structs","text":"","title":"Client Configuration Structs"},{"location":"design/adr/0001-Registy-Refactor/#current-registry-client-config","text":"The following is the current struct used to configure the current Registry Client type Config struct { Protocol string Host string Port int Type string Stem string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string }","title":"Current Registry Client Config"},{"location":"design/adr/0001-Registy-Refactor/#new-configuration-client-config","text":"The following is the new struct the will be used to configure the new Configuration Client from the command line option or environment variable values. The Service Registry portion has been removed from the above existing Registry Client Config type Config struct { Protocol string Host string Port int Type string BasePath string ServiceKey string }","title":"New Configuration Client Config"},{"location":"design/adr/0001-Registy-Refactor/#new-registry-client-config","text":"The following is the revised struct the will be used to configure the new Registry Client from the information in the service's configuration. This is mostly unchanged from the existing Registry Client Config , except that the Stem for configuration has been removed type Config struct { Protocol string Host string Port int Type string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string }","title":"New Registry Client Config"},{"location":"design/adr/0001-Registy-Refactor/#provider-implementations","text":"The current Consul implementation of the Registry Client will be split up into implementations for the new Configuration Client in the new go-mod-configuration module and the revised Registry Client in the existing go-mod-registry module.","title":"Provider Implementations"},{"location":"design/adr/0001-Registy-Refactor/#decision","text":"It was decided to move forward with the above design After initial ADR was approved, it was decided to retain the -r/--registry command-line flag and not add the Enabled field in the Registry provider configuration.","title":"Decision"},{"location":"design/adr/0001-Registy-Refactor/#consequences","text":"Once the refactoring of go-mod-registry and go-mod-configuration are complete, they will need to be integrated into the new go-mod-bootstrap. Part of this integration will be the Command line option changes above. At this point the edgex-go services will be integrated with the new Registry and Configuration providers. The App Services SDK and Device Services SDK will then need to integrate go-mod-bootstrap to take advantage of these new providers.","title":"Consequences"},{"location":"design/adr/0001-Registy-Refactor/#references","text":"Registry Abstraction - Decouple EdgeX services from Consul (Previous design)","title":"References"},{"location":"design/adr/0004-Feature-Flags/","text":"Feature Flag Proposal Status Accepted Context Out of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags. Feature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility. It can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time. Putting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true/false. This is similar to what is done today. Turning on/off security is an example, turning on/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as featurepkg.Register(\u201cMyNewFeature\u201d) or featurepkg.IsOn(\u201cMyNewFeature\u201d) . However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized. Decision The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf. However, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged. Consequences Allows more focus on the many more competing priorities for this release. Minimal impact to development cycles and release schedule","title":"Feature Flag Proposal"},{"location":"design/adr/0004-Feature-Flags/#feature-flag-proposal","text":"","title":"Feature Flag Proposal"},{"location":"design/adr/0004-Feature-Flags/#status","text":"Accepted","title":"Status"},{"location":"design/adr/0004-Feature-Flags/#context","text":"Out of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags. Feature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility. It can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time. Putting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true/false. This is similar to what is done today. Turning on/off security is an example, turning on/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as featurepkg.Register(\u201cMyNewFeature\u201d) or featurepkg.IsOn(\u201cMyNewFeature\u201d) . However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized.","title":"Context"},{"location":"design/adr/0004-Feature-Flags/#decision","text":"The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf. However, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged.","title":"Decision"},{"location":"design/adr/0004-Feature-Flags/#consequences","text":"Allows more focus on the many more competing priorities for this release. Minimal impact to development cycles and release schedule","title":"Consequences"},{"location":"design/adr/0005-Service-Self-Config/","text":"Service Self Config Init & Config Seed Removal Status approved - TSC vote on 3/25/20 for Geneva release NOTE: this ADR does not address high availability considerations and concerns. EdgeX, in general, has a number of unanswered questions with regard to HA architecture and this design adds to those considerations. Context Since its debut, EdgeX has had a configuration seed service (config-seed) that, on start of EdgeX, deposits configuration for all the services into Consul (our configuration/registry service). For development purposes, or on resource constrained platforms, EdgeX can be run without Consul with services simply reading configuration from the filesystem. While this process has nominally worked for several releases of EdgeX, there has always been some issues with this extra initialization process (config-seed), not least of which are: - race conditions on the part of the services, as they bootstrap, coming up before the config-seed completes its deposit of configuration into Consul - how to deal with \"overrides\" such as environmental variable provided configuration overrides. As the override is often specific to a service but has to be in place for config-seed in order to take effect. - need for an additional service that is only there for init and then dies (confusing to users) NOTE - for historical purposes, it should be noted that config-seed only writes configuration into the configuration/registry service (Consul) once on the first start of EdgeX. On subsequent starts of EdgeX, config-seed checks to see if it has already populated the configuration/registry service and will not rewrite configuration again (unless the --overwrite flag is used). The design/architectural proposal, therefore, is: - removal of the config-seed service (removing cmd/config-seed from the edgex-go repository) - have each EdgeX micro service \"self seed\" - that is seed Consul with their own required configuration on bootstrap of the service. Details of that bootstrapping process are below. Command Line Options All EdgeX services support a common set of command-line options, some combination of which are required on startup for a service to interact with the rest of EdgeX. Command line options are not set by any configuration. Command line options include: --configProvider or -cp (the configuration provider location URL) --overwrite or -o (overwrite the configuration in the configuration provider) --file or -f (the configuration filename - configuration.toml is used by default if the configuration filename is not provided) --profile or -p (the name of a sub directory in the configuration directory in which a profile-specific configuration file is found. This has no default. If not specified, the configuration file is read from the configuration directory) --confdir or -c (the directory where the configuration file is found - ./res is used by default if the confdir is not specified, where \".\" is the convention on Linux/Unix/MacOS which means current directory) --registry or -r (string indicating use of the registry) The distinction of command line options versus configuration will be important later in this ADR. Two command line options (-o for overwrite and -r for registry) are not overridable by environmental variables. NOTES: Use of the --overwrite command line option should be used sparingly and with expert knowledge of EdgeX; in particular knowledge of how it operates and where/how it gets its configuration on restarts, etc. Ordinarily, --overwrite is provided as a means to support development needs. Use of --overwrite permanently in production enviroments is highly discouraged. Configuration Initialization Each service has (or shall have if not providing it already) a local configuration file. The service may use the local configuration file on initialization of the service (aka bootstrap of the service) depending on command line options and environmental variables (see below) provided at startup. Using a configuration provider When the configuration provider is specified, the service will call on the configuration provider (Consul) and check if the top-level (root) namespace for the service exists. If configuratation at the top-level (root) namespace exists, it indicates that the service has already populated its configuration into the configuration provider in a prior startup. If the service finds the top-level (root) namespace is already populated with configuration information it will then read that configuration information from the configuration provider under namespace for that service (and ignore what is in the local configuration file). If the service finds the top-level (root) namespace is not populated with configuration information, it will read its local configuration file and populate the configuration provider (under the namespace for the service) with configuration read from the local configuration file. A configuration provider can be specified with a command line argument (the -cp / --configProvider) or environment variable (the EDGEX_CONFIGURATION_PROVIDER environmental variable which overrides the command line argument). NOTE: the environmental variables are typically uppercase but there have been inconsistencies in environmental variable casing (example: edgex_registry). This should be considered and made consistent in a future major release. Using the local configuration file When a configuration provider isn't specified, the service just uses the configuration in its local configuration file. That is the service uses the configuration in the file associated with the profile, config filename and config file directory command line options or environmental variables. In this case, the service does not contact the configuration service (Consul) for any configuration information. NOTE: As the services now self seed and deployment specific changes can be made via environment overrides, it will no longer be necessary to have a Docker profile configuration file in each of the service directories (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml). See Consequences below. It will still be possible for users to use the profile mechanism to specify a Docker configuration, but it will no longer be required and not the recommended approach to providing Docker container specific configuration. Overrides Environment variables used to override configuration always take precedence whether configuration is being sourced locally or read from the config provider/Consul. Note - this means that a configuration value that is being overridden by an environment variable will always be the source of truth, even if the same configuration is changed directly in Consul. The name of the environmental variable must match the path names in Consul. NOTES: - Environmental variables overrides remove the need to change the \"docker\" profile in the res/docker/configuration.toml files - Allowing removal of 50% of the existing configuration.toml files. - The override rules in EdgeX between environmental variables and command line options may be counter intuitive compared to other systems. There appears to be no standard practice. Indeed, web searching \"Reddit & Starting Fights Env Variables vs Command Line Args\" will layout the prevailing differences. - Environment variables used for configuration overrides are named by prepending the the configuration element with the configuration section inclusive of sub-path, where sub-path's \".\"s are replaced with underscores. These configuration environment variable overrides must be specified using camel case. Here are two examples: Registry_Host for [ Registry ] Host = 'localhost' Clients_CoreData_Host for [ Clients ] [ Clients.CoreData ] Host = 'localhost' Going forward, environmental variables that override command line options should be all uppercase. All values overriden get logged (indicating which configuration value or op param and the new value). Decision These features have been implemented (with some minor changes to be done) for consideration here: https://github.com/edgexfoundry/go-mod-bootstrap/compare/master...lenny-intel:SelfSeed2. This code branch will be removed once this ADR is approved and implemented on master. The implementation for self-seeding services and environmental overrides is already implemented (for Fuji) per this document in the application services and device services (and instituted in the SDKs of each). Backward compatibility Several aspects of this ADR contain backward compatibility issues for the device service and application service SDKs. Therefore, for the upcoming minor release, the following guidelines and expections are added to provide for backward compatibility. --registry= for Device SDKs As earlier versions of the device service SDKs accepted a URI for --registry, if specified on the command line, use the given URI as the address of the configuration provider. If both --configProvider and --registry specify URIs, then the service should log an error and exit. --registry (no \u2018=\u2019) and w/o --configProvider for both SDKs If a configProvider URI isn't specified, but --registry (w/out a URI) is specified, then the service will use the Registry provider information from its local configuration file for both configuration and registry providers. Env Var: edgex_registry= for all services (currently has been removed) Add it back and use value as if it was EDGEX_CONFIGURATION_PROVIDER and enable use of registry with same settings in URL. Default to http as it is in Fuji. Consequences Docker compose files will need to be changed to remove config seed. The main Snap will need to be changed to remove config seed. Config seed code (currently in edgex-go repo) is to be removed. Any service specific environmental overrides currently on config seed need to be moved to the specific service(s). The Docker configuration files and directory (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml) that are used to populate the config seed for Docker containers can be eliminated from all the services. In cmd/security-secretstore-setup, there is only a docker configuration.toml. This file will be moved rather than deleted. Documentation would need to reflect removal of config seed and \"self seeding\" process. Removes any potential issue with past race conditions (as experienced with the Edinburgh release) as each service is now responsible for its own configuration. There are still high availability concerns that need to be considered and not covered in this ADR at this time. Removes some confusion on the part of users as to why a service (config-seed) starts and immediately exits. Minimal impact to development cycles and release schedule Configuration endpoints in all services need to ensure the environmental variables are reflected in the configuration data returned (this is a system management impact). Docker files will need to be modified to remove setting profile=docker Docker compose files will need to be changed to add environmental overrides for removal of docker profiles. These should go in the global environment section of the compose files for those overrides that apply to all services. Example: # all common shared environment variables defined here: x-common-env-variables: &common-variables EDGEX_SECURITY_SECRET_STORE: \"false\" EDGEX_CONFIGURATION_PROVIDER: consul.http://edgex-core-consul:8500 Clients_CoreData_Host: edgex-core-data Clients_Logging_Host: edgex-support-logging Logging_EnableRemote: \"true\"","title":"Service Self Config Init & Config Seed Removal"},{"location":"design/adr/0005-Service-Self-Config/#service-self-config-init-config-seed-removal","text":"","title":"Service Self Config Init &amp; Config Seed Removal"},{"location":"design/adr/0005-Service-Self-Config/#status","text":"approved - TSC vote on 3/25/20 for Geneva release NOTE: this ADR does not address high availability considerations and concerns. EdgeX, in general, has a number of unanswered questions with regard to HA architecture and this design adds to those considerations.","title":"Status"},{"location":"design/adr/0005-Service-Self-Config/#context","text":"Since its debut, EdgeX has had a configuration seed service (config-seed) that, on start of EdgeX, deposits configuration for all the services into Consul (our configuration/registry service). For development purposes, or on resource constrained platforms, EdgeX can be run without Consul with services simply reading configuration from the filesystem. While this process has nominally worked for several releases of EdgeX, there has always been some issues with this extra initialization process (config-seed), not least of which are: - race conditions on the part of the services, as they bootstrap, coming up before the config-seed completes its deposit of configuration into Consul - how to deal with \"overrides\" such as environmental variable provided configuration overrides. As the override is often specific to a service but has to be in place for config-seed in order to take effect. - need for an additional service that is only there for init and then dies (confusing to users) NOTE - for historical purposes, it should be noted that config-seed only writes configuration into the configuration/registry service (Consul) once on the first start of EdgeX. On subsequent starts of EdgeX, config-seed checks to see if it has already populated the configuration/registry service and will not rewrite configuration again (unless the --overwrite flag is used). The design/architectural proposal, therefore, is: - removal of the config-seed service (removing cmd/config-seed from the edgex-go repository) - have each EdgeX micro service \"self seed\" - that is seed Consul with their own required configuration on bootstrap of the service. Details of that bootstrapping process are below.","title":"Context"},{"location":"design/adr/0005-Service-Self-Config/#command-line-options","text":"All EdgeX services support a common set of command-line options, some combination of which are required on startup for a service to interact with the rest of EdgeX. Command line options are not set by any configuration. Command line options include: --configProvider or -cp (the configuration provider location URL) --overwrite or -o (overwrite the configuration in the configuration provider) --file or -f (the configuration filename - configuration.toml is used by default if the configuration filename is not provided) --profile or -p (the name of a sub directory in the configuration directory in which a profile-specific configuration file is found. This has no default. If not specified, the configuration file is read from the configuration directory) --confdir or -c (the directory where the configuration file is found - ./res is used by default if the confdir is not specified, where \".\" is the convention on Linux/Unix/MacOS which means current directory) --registry or -r (string indicating use of the registry) The distinction of command line options versus configuration will be important later in this ADR. Two command line options (-o for overwrite and -r for registry) are not overridable by environmental variables. NOTES: Use of the --overwrite command line option should be used sparingly and with expert knowledge of EdgeX; in particular knowledge of how it operates and where/how it gets its configuration on restarts, etc. Ordinarily, --overwrite is provided as a means to support development needs. Use of --overwrite permanently in production enviroments is highly discouraged.","title":"Command Line Options"},{"location":"design/adr/0005-Service-Self-Config/#configuration-initialization","text":"Each service has (or shall have if not providing it already) a local configuration file. The service may use the local configuration file on initialization of the service (aka bootstrap of the service) depending on command line options and environmental variables (see below) provided at startup. Using a configuration provider When the configuration provider is specified, the service will call on the configuration provider (Consul) and check if the top-level (root) namespace for the service exists. If configuratation at the top-level (root) namespace exists, it indicates that the service has already populated its configuration into the configuration provider in a prior startup. If the service finds the top-level (root) namespace is already populated with configuration information it will then read that configuration information from the configuration provider under namespace for that service (and ignore what is in the local configuration file). If the service finds the top-level (root) namespace is not populated with configuration information, it will read its local configuration file and populate the configuration provider (under the namespace for the service) with configuration read from the local configuration file. A configuration provider can be specified with a command line argument (the -cp / --configProvider) or environment variable (the EDGEX_CONFIGURATION_PROVIDER environmental variable which overrides the command line argument). NOTE: the environmental variables are typically uppercase but there have been inconsistencies in environmental variable casing (example: edgex_registry). This should be considered and made consistent in a future major release. Using the local configuration file When a configuration provider isn't specified, the service just uses the configuration in its local configuration file. That is the service uses the configuration in the file associated with the profile, config filename and config file directory command line options or environmental variables. In this case, the service does not contact the configuration service (Consul) for any configuration information. NOTE: As the services now self seed and deployment specific changes can be made via environment overrides, it will no longer be necessary to have a Docker profile configuration file in each of the service directories (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml). See Consequences below. It will still be possible for users to use the profile mechanism to specify a Docker configuration, but it will no longer be required and not the recommended approach to providing Docker container specific configuration.","title":"Configuration Initialization"},{"location":"design/adr/0005-Service-Self-Config/#overrides","text":"Environment variables used to override configuration always take precedence whether configuration is being sourced locally or read from the config provider/Consul. Note - this means that a configuration value that is being overridden by an environment variable will always be the source of truth, even if the same configuration is changed directly in Consul. The name of the environmental variable must match the path names in Consul. NOTES: - Environmental variables overrides remove the need to change the \"docker\" profile in the res/docker/configuration.toml files - Allowing removal of 50% of the existing configuration.toml files. - The override rules in EdgeX between environmental variables and command line options may be counter intuitive compared to other systems. There appears to be no standard practice. Indeed, web searching \"Reddit & Starting Fights Env Variables vs Command Line Args\" will layout the prevailing differences. - Environment variables used for configuration overrides are named by prepending the the configuration element with the configuration section inclusive of sub-path, where sub-path's \".\"s are replaced with underscores. These configuration environment variable overrides must be specified using camel case. Here are two examples: Registry_Host for [ Registry ] Host = 'localhost' Clients_CoreData_Host for [ Clients ] [ Clients.CoreData ] Host = 'localhost' Going forward, environmental variables that override command line options should be all uppercase. All values overriden get logged (indicating which configuration value or op param and the new value).","title":"Overrides"},{"location":"design/adr/0005-Service-Self-Config/#decision","text":"These features have been implemented (with some minor changes to be done) for consideration here: https://github.com/edgexfoundry/go-mod-bootstrap/compare/master...lenny-intel:SelfSeed2. This code branch will be removed once this ADR is approved and implemented on master. The implementation for self-seeding services and environmental overrides is already implemented (for Fuji) per this document in the application services and device services (and instituted in the SDKs of each).","title":"Decision"},{"location":"design/adr/0005-Service-Self-Config/#backward-compatibility","text":"Several aspects of this ADR contain backward compatibility issues for the device service and application service SDKs. Therefore, for the upcoming minor release, the following guidelines and expections are added to provide for backward compatibility. --registry= for Device SDKs As earlier versions of the device service SDKs accepted a URI for --registry, if specified on the command line, use the given URI as the address of the configuration provider. If both --configProvider and --registry specify URIs, then the service should log an error and exit. --registry (no \u2018=\u2019) and w/o --configProvider for both SDKs If a configProvider URI isn't specified, but --registry (w/out a URI) is specified, then the service will use the Registry provider information from its local configuration file for both configuration and registry providers. Env Var: edgex_registry= for all services (currently has been removed) Add it back and use value as if it was EDGEX_CONFIGURATION_PROVIDER and enable use of registry with same settings in URL. Default to http as it is in Fuji.","title":"Backward compatibility"},{"location":"design/adr/0005-Service-Self-Config/#consequences","text":"Docker compose files will need to be changed to remove config seed. The main Snap will need to be changed to remove config seed. Config seed code (currently in edgex-go repo) is to be removed. Any service specific environmental overrides currently on config seed need to be moved to the specific service(s). The Docker configuration files and directory (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml) that are used to populate the config seed for Docker containers can be eliminated from all the services. In cmd/security-secretstore-setup, there is only a docker configuration.toml. This file will be moved rather than deleted. Documentation would need to reflect removal of config seed and \"self seeding\" process. Removes any potential issue with past race conditions (as experienced with the Edinburgh release) as each service is now responsible for its own configuration. There are still high availability concerns that need to be considered and not covered in this ADR at this time. Removes some confusion on the part of users as to why a service (config-seed) starts and immediately exits. Minimal impact to development cycles and release schedule Configuration endpoints in all services need to ensure the environmental variables are reflected in the configuration data returned (this is a system management impact). Docker files will need to be modified to remove setting profile=docker Docker compose files will need to be changed to add environmental overrides for removal of docker profiles. These should go in the global environment section of the compose files for those overrides that apply to all services. Example: # all common shared environment variables defined here: x-common-env-variables: &common-variables EDGEX_SECURITY_SECRET_STORE: \"false\" EDGEX_CONFIGURATION_PROVIDER: consul.http://edgex-core-consul:8500 Clients_CoreData_Host: edgex-core-data Clients_Logging_Host: edgex-support-logging Logging_EnableRemote: \"true\"","title":"Consequences"},{"location":"design/adr/core/0003-V2-API-Principles/","text":"Geneva API Guiding Principles Status Accepted by EdgeX Foundry working groups as of Core Working Group meeting 16-Jan-2020 Context A redesign of the EdgeX Foundry API is proposed for the Geneva release. This is understood by the community to warrant a 2.0 release that will not be backward compatible. The goal is to rework the API using solid principles that will allow for extension over the course of several release cycles, avoiding the necessity of yet another major release version in a short period of time. Briefly, this effort grew from the acknowledgement that the current models used to facilitate requests and responses via the EdgeX Foundry API were legacy definitions that were once used as internal representations of state within the EdgeX services themselves. Thus if you want to add or update a device, you populate a full device model rather than a specific Add/UpdateDeviceRequest. Currently, your request model has the same definition, and thus validation constraints, as the response model because they are one and the same! It is desirable to separate and be specific about what is required for a given request, as well as its state validity, and the bare minimum that must be returned within a response. Following from that central need, other considerations have been used when designing this proposed API. These will be enumerated and briefly explained below. 1.) Transport-agnostic Define the request/response data transfer objects (DTO) in a manner whereby they can be used independent of transport. For example, although an OpenAPI doc is implicitly coupled to HTTP/REST, define the DTOs in such a way that they could also be used if the platform were to evolve to a pub/sub architecture. 2.) Support partial updates via PATCH Given a request to, for example, update a device the user should be able to update only some properties of the device. Previously this would require an endpoint for each individual property to be updated since the \"update device\" endpoint, facilitated by a PUT, would perform a complete replacement of the device's data. If you only wanted to update the LastConnected timestamp, then a separate endpoint for that property was required. We will leverage PATCH in order to update an entity and only those properties populated on the request will be considered. Properties that are missing or left blank will not be touched. 3.) Support multiple requests at once Endpoints for the addition or updating of data (POST/PATCH) should accept multiple requests at once. If it were desirable to add or update multiple devices with one request, for example, the API should facilitate this. 4.) Support multiple correlated responses at once Following from #3 above, each request sent to the endpoint must result in a corresponding response. In the case of HTTP/REST, this means if four requests are sent to a POST operation, the return payload will have four responses. Each response must expose a \"code\" property containing a numeric result for what occurred. These could be equivalent to HTTP status codes, for example. So while the overall call might succeed, one or more of the child requests may not have. It is up to the caller to examine each response and handle accordingly. In order to correlate each response to its original request, each request must be assigned its own ID (in GUID format). The caller can then tie a response to an individual request and handle the result accordingly, or otherwise track that a response to a given request was not received. 5.) Use of 207 HTTP Status (Multi-Result) In the case where an endpoint can support multiple responses, the returned HTTP code from a REST API will be 207 (Multi-status) 6.) Each service should provide a \"batch\" request endpoint In addition to use-case specific endpoints that you'd find in any REST API, each service should provide a \"batch\" endpoint that can take any kind of request. This is a generic endpoint that allows you to group requests of different types within a single call. For example, instead of having to call two endpoints to get two jobs done, you can call a single endpoint passing the specific requests and have them routed appropriately within the service. Also, when considering agnostic transport, the batch endpoint would allow for the definition and handling of \"GET\" equivalent DTOs which are now implicit in the format of a URL. 7.) GET endpoints returning a list of items must support pagination URL parameters must be supported for every GET endpoint to support pagination. These parameters should indicate the current page of results and the number of results on a page. Decision Commnunity has accepted the reasoning for the new API and the design principles outlined above. The approach will be to gradually implement the V2 API side-by-side with the current V1 APIs. We believe it will take more than a single release cycle to implement the new specification. Releases of that occur prior to the V2 API implementation completion will continue to be major versioned as 1.x. Subsequent to completion, releases will be major versioned as 2.x. Consequences Backward incompatibility with EdgeX Foundry's V1 API requires a major version increment (e.g. v2.x). Service-level testing (e.g. blackbox tests) needs to be rewritten. Specification-first development allows for different implementations of EdgeX services to be certified as \"EdgeX Compliant\" in reference to an objective standard. Transport-agnostic focus enables different architectural patterns (pub/sub versus REST) using the same data representation.","title":"Geneva API Guiding Principles"},{"location":"design/adr/core/0003-V2-API-Principles/#geneva-api-guiding-principles","text":"","title":"Geneva API Guiding Principles"},{"location":"design/adr/core/0003-V2-API-Principles/#status","text":"Accepted by EdgeX Foundry working groups as of Core Working Group meeting 16-Jan-2020","title":"Status"},{"location":"design/adr/core/0003-V2-API-Principles/#context","text":"A redesign of the EdgeX Foundry API is proposed for the Geneva release. This is understood by the community to warrant a 2.0 release that will not be backward compatible. The goal is to rework the API using solid principles that will allow for extension over the course of several release cycles, avoiding the necessity of yet another major release version in a short period of time. Briefly, this effort grew from the acknowledgement that the current models used to facilitate requests and responses via the EdgeX Foundry API were legacy definitions that were once used as internal representations of state within the EdgeX services themselves. Thus if you want to add or update a device, you populate a full device model rather than a specific Add/UpdateDeviceRequest. Currently, your request model has the same definition, and thus validation constraints, as the response model because they are one and the same! It is desirable to separate and be specific about what is required for a given request, as well as its state validity, and the bare minimum that must be returned within a response. Following from that central need, other considerations have been used when designing this proposed API. These will be enumerated and briefly explained below. 1.) Transport-agnostic Define the request/response data transfer objects (DTO) in a manner whereby they can be used independent of transport. For example, although an OpenAPI doc is implicitly coupled to HTTP/REST, define the DTOs in such a way that they could also be used if the platform were to evolve to a pub/sub architecture. 2.) Support partial updates via PATCH Given a request to, for example, update a device the user should be able to update only some properties of the device. Previously this would require an endpoint for each individual property to be updated since the \"update device\" endpoint, facilitated by a PUT, would perform a complete replacement of the device's data. If you only wanted to update the LastConnected timestamp, then a separate endpoint for that property was required. We will leverage PATCH in order to update an entity and only those properties populated on the request will be considered. Properties that are missing or left blank will not be touched. 3.) Support multiple requests at once Endpoints for the addition or updating of data (POST/PATCH) should accept multiple requests at once. If it were desirable to add or update multiple devices with one request, for example, the API should facilitate this. 4.) Support multiple correlated responses at once Following from #3 above, each request sent to the endpoint must result in a corresponding response. In the case of HTTP/REST, this means if four requests are sent to a POST operation, the return payload will have four responses. Each response must expose a \"code\" property containing a numeric result for what occurred. These could be equivalent to HTTP status codes, for example. So while the overall call might succeed, one or more of the child requests may not have. It is up to the caller to examine each response and handle accordingly. In order to correlate each response to its original request, each request must be assigned its own ID (in GUID format). The caller can then tie a response to an individual request and handle the result accordingly, or otherwise track that a response to a given request was not received. 5.) Use of 207 HTTP Status (Multi-Result) In the case where an endpoint can support multiple responses, the returned HTTP code from a REST API will be 207 (Multi-status) 6.) Each service should provide a \"batch\" request endpoint In addition to use-case specific endpoints that you'd find in any REST API, each service should provide a \"batch\" endpoint that can take any kind of request. This is a generic endpoint that allows you to group requests of different types within a single call. For example, instead of having to call two endpoints to get two jobs done, you can call a single endpoint passing the specific requests and have them routed appropriately within the service. Also, when considering agnostic transport, the batch endpoint would allow for the definition and handling of \"GET\" equivalent DTOs which are now implicit in the format of a URL. 7.) GET endpoints returning a list of items must support pagination URL parameters must be supported for every GET endpoint to support pagination. These parameters should indicate the current page of results and the number of results on a page.","title":"Context"},{"location":"design/adr/core/0003-V2-API-Principles/#decision","text":"Commnunity has accepted the reasoning for the new API and the design principles outlined above. The approach will be to gradually implement the V2 API side-by-side with the current V1 APIs. We believe it will take more than a single release cycle to implement the new specification. Releases of that occur prior to the V2 API implementation completion will continue to be major versioned as 1.x. Subsequent to completion, releases will be major versioned as 2.x.","title":"Decision"},{"location":"design/adr/core/0003-V2-API-Principles/#consequences","text":"Backward incompatibility with EdgeX Foundry's V1 API requires a major version increment (e.g. v2.x). Service-level testing (e.g. blackbox tests) needs to be rewritten. Specification-first development allows for different implementations of EdgeX services to be certified as \"EdgeX Compliant\" in reference to an objective standard. Transport-agnostic focus enables different architectural patterns (pub/sub versus REST) using the same data representation.","title":"Consequences"},{"location":"design/adr/device-service/0002-Array-Datatypes/","text":"Array Datatypes Design Status Context Decision Consequences Status Proposed Context The current data model does not directly provide for devices which provide array data. Small fixed-length arrays may be handled by defining multiple device resources - one for each element - and aggregating them via a resource command. Other array data may be passed using the Binary type. Neither of these approaches is ideal: the binary data is opaque and any service processing it would need specific knowledge to do so, and aggregation presents the device service implementation with a multiple-read request that could in many cases be better handled by a single request. This design adds arrays of primitives to the range of supported types in EdgeX. It comprises an extension of the DeviceProfile model, and an update to the definition of Reading. Decision DeviceProfile extension The permitted values of the Type field in PropertyValue are extended to include: \"BoolArray\", \"StringArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\" Readings Implementation in v2 API The value field of SimpleReading becomes an array of strings. For non-array types, an array of length 1 is created. Fallback position for v1 API In the v1 API, Reading.Value is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie [\"value1\", \"value2\", ...] Consequences Any service which processes Readings will need to be reworked to account for the new Reading type. Device Service considerations The API used for interfacing between device SDKs and devices service implementations contains a local representation of reading values. This will need to be updated in line with the changes outlined here. For C, this will involve an extension of the existing union type. For Go, additional fields may be added to the CommandValue structure.","title":"Array Datatypes Design"},{"location":"design/adr/device-service/0002-Array-Datatypes/#array-datatypes-design","text":"Status Context Decision Consequences","title":"Array Datatypes Design"},{"location":"design/adr/device-service/0002-Array-Datatypes/#status","text":"Proposed","title":"Status"},{"location":"design/adr/device-service/0002-Array-Datatypes/#context","text":"The current data model does not directly provide for devices which provide array data. Small fixed-length arrays may be handled by defining multiple device resources - one for each element - and aggregating them via a resource command. Other array data may be passed using the Binary type. Neither of these approaches is ideal: the binary data is opaque and any service processing it would need specific knowledge to do so, and aggregation presents the device service implementation with a multiple-read request that could in many cases be better handled by a single request. This design adds arrays of primitives to the range of supported types in EdgeX. It comprises an extension of the DeviceProfile model, and an update to the definition of Reading.","title":"Context"},{"location":"design/adr/device-service/0002-Array-Datatypes/#decision","text":"","title":"Decision"},{"location":"design/adr/device-service/0002-Array-Datatypes/#deviceprofile-extension","text":"The permitted values of the Type field in PropertyValue are extended to include: \"BoolArray\", \"StringArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"","title":"DeviceProfile extension"},{"location":"design/adr/device-service/0002-Array-Datatypes/#readings","text":"","title":"Readings"},{"location":"design/adr/device-service/0002-Array-Datatypes/#implementation-in-v2-api","text":"The value field of SimpleReading becomes an array of strings. For non-array types, an array of length 1 is created.","title":"Implementation in v2 API"},{"location":"design/adr/device-service/0002-Array-Datatypes/#fallback-position-for-v1-api","text":"In the v1 API, Reading.Value is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie [\"value1\", \"value2\", ...]","title":"Fallback position for v1 API"},{"location":"design/adr/device-service/0002-Array-Datatypes/#consequences","text":"Any service which processes Readings will need to be reworked to account for the new Reading type.","title":"Consequences"},{"location":"design/adr/device-service/0002-Array-Datatypes/#device-service-considerations","text":"The API used for interfacing between device SDKs and devices service implementations contains a local representation of reading values. This will need to be updated in line with the changes outlined here. For C, this will involve an extension of the existing union type. For Go, additional fields may be added to the CommandValue structure.","title":"Device Service considerations"},{"location":"design/adr/devops/0007-Release-Automation/","text":"Release Automation Status In Review for Geneva Context EdgeX Foundry is a framework composed of microservices to ease development of IoT/Edge solutions. With the framework getting richer, project growth, the number of artifacts to be released has increased. This proposal outlines a method for automating the release process for the base artifacts. Requirements Release Artifact Definition For the scope of Geneva release artifact types are defined as: GitHub tags in the repositories. Docker images in our Nexus repository and Docker hub. Snaps in the Snapcraft store. This list is likely to expand in future releases. General Requirements As the EdgeX Release Czar I gathered the following requirements for automating this part of the release. The release automation needs a manual trigger to be triggered by the EdgeX Release Czar or the Linux Foundation Release Engineers. The goal of this automation is to have a \"push button\" release mechanism to reduce human error in our release process. Release artifacts can come from one or more GitHub repositories at a time. GitHub repositories can have one or more release artifact types to release. GitHub repositories can have one or more artifacts of a specific type to release. (For example: The mono repository, edgex-go, has more than 20 docker images to release.) GitHub repositories may be released at different times. (For example: Application and Device service repositories can be released on a different day than the Core services in the mono repository.) Ability to track multiple release streams for the project. An audit trail history for releases. Location The code that will manage the release automation for EdgeX Foundry will live in a repository called cd-management . This repository will have a branch named release that will track the releases of artifacts off the master branch of the EdgeX Foundry repositories. Multiple Release Streams EdgeX Foundry has this idea of multple release streams that basically coincides with different named branches in GitHub. For the majority of the main releases we will be targeting those off the master branch. In our cd-management repository we will have a release branch that will track the master branches EdgeX repositories. In the future we will mark a specific release for long term support (LTS). When this happens we will have to branch off master in the EdgeX repositories and create a separate release stream for the LTS. The suggestion at that point will be to branch off the release branch in cd-management as well and use this new release branch to track the LTS branches in the EdgeX repositories. Release Flow Go Modules, Device and Application SDKs During Development Go modules, Application and Device SDKs only release a GitHub tag as their release. Go modules are set up to automatically release a new patch version on every merge into the master branch of the repository. (IE: 1.0.0 -> 1.0.1) For Application and Device SDKs we increment a developmental version tag. (IE: 1.0.0-dev.1 -> 1.0.0-dev.2) Release The release automation for go modules is used to set a new version that increases the major or minor version for the module. (IE: 1.0.0 -> 1.1.0) For the Application and Device SDKs it is used to set the final release version. (IE: 1.0.0-dev.X -> 1.0.0) Application, Device Services and Supporting Docker Images During Development For the Device, Application services and supporting docker images we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging) and snaps will be pushed to the edge (experimental) and beta (QA) channels in the Snapcraft store. Release The release automation for the Application and Device services is the same as the services found in the edgex-go repository. Core Services During Development For the Core services we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging). Snaps will be pushed daily to the edge (experimental) and beta (QA) channels in the Snapcraft store because the core services snap can take up to a hour and half to build. Release The release automation will need to do the following: Set version tag on GitHub. (IE: 1.0.0-dev.X -> 1.0.0) Promote docker images in our Nexus repository from docker.staging to docker.release and public Docker hub. Promote snaps from the beta (QA) channel to the release channel in the Snapcraft store.","title":"Release Automation"},{"location":"design/adr/devops/0007-Release-Automation/#release-automation","text":"","title":"Release Automation"},{"location":"design/adr/devops/0007-Release-Automation/#status","text":"In Review for Geneva","title":"Status"},{"location":"design/adr/devops/0007-Release-Automation/#context","text":"EdgeX Foundry is a framework composed of microservices to ease development of IoT/Edge solutions. With the framework getting richer, project growth, the number of artifacts to be released has increased. This proposal outlines a method for automating the release process for the base artifacts.","title":"Context"},{"location":"design/adr/devops/0007-Release-Automation/#requirements","text":"","title":"Requirements"},{"location":"design/adr/devops/0007-Release-Automation/#release-artifact-definition","text":"For the scope of Geneva release artifact types are defined as: GitHub tags in the repositories. Docker images in our Nexus repository and Docker hub. Snaps in the Snapcraft store. This list is likely to expand in future releases.","title":"Release Artifact Definition"},{"location":"design/adr/devops/0007-Release-Automation/#general-requirements","text":"As the EdgeX Release Czar I gathered the following requirements for automating this part of the release. The release automation needs a manual trigger to be triggered by the EdgeX Release Czar or the Linux Foundation Release Engineers. The goal of this automation is to have a \"push button\" release mechanism to reduce human error in our release process. Release artifacts can come from one or more GitHub repositories at a time. GitHub repositories can have one or more release artifact types to release. GitHub repositories can have one or more artifacts of a specific type to release. (For example: The mono repository, edgex-go, has more than 20 docker images to release.) GitHub repositories may be released at different times. (For example: Application and Device service repositories can be released on a different day than the Core services in the mono repository.) Ability to track multiple release streams for the project. An audit trail history for releases.","title":"General Requirements"},{"location":"design/adr/devops/0007-Release-Automation/#location","text":"The code that will manage the release automation for EdgeX Foundry will live in a repository called cd-management . This repository will have a branch named release that will track the releases of artifacts off the master branch of the EdgeX Foundry repositories.","title":"Location"},{"location":"design/adr/devops/0007-Release-Automation/#multiple-release-streams","text":"EdgeX Foundry has this idea of multple release streams that basically coincides with different named branches in GitHub. For the majority of the main releases we will be targeting those off the master branch. In our cd-management repository we will have a release branch that will track the master branches EdgeX repositories. In the future we will mark a specific release for long term support (LTS). When this happens we will have to branch off master in the EdgeX repositories and create a separate release stream for the LTS. The suggestion at that point will be to branch off the release branch in cd-management as well and use this new release branch to track the LTS branches in the EdgeX repositories.","title":"Multiple Release Streams"},{"location":"design/adr/devops/0007-Release-Automation/#release-flow","text":"","title":"Release Flow"},{"location":"design/adr/devops/0007-Release-Automation/#go-modules-device-and-application-sdks","text":"","title":"Go Modules, Device and Application SDKs"},{"location":"design/adr/devops/0007-Release-Automation/#during-development","text":"Go modules, Application and Device SDKs only release a GitHub tag as their release. Go modules are set up to automatically release a new patch version on every merge into the master branch of the repository. (IE: 1.0.0 -> 1.0.1) For Application and Device SDKs we increment a developmental version tag. (IE: 1.0.0-dev.1 -> 1.0.0-dev.2)","title":"During Development"},{"location":"design/adr/devops/0007-Release-Automation/#release","text":"The release automation for go modules is used to set a new version that increases the major or minor version for the module. (IE: 1.0.0 -> 1.1.0) For the Application and Device SDKs it is used to set the final release version. (IE: 1.0.0-dev.X -> 1.0.0)","title":"Release"},{"location":"design/adr/devops/0007-Release-Automation/#application-device-services-and-supporting-docker-images","text":"","title":"Application, Device Services and Supporting Docker Images"},{"location":"design/adr/devops/0007-Release-Automation/#during-development_1","text":"For the Device, Application services and supporting docker images we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging) and snaps will be pushed to the edge (experimental) and beta (QA) channels in the Snapcraft store.","title":"During Development"},{"location":"design/adr/devops/0007-Release-Automation/#release_1","text":"The release automation for the Application and Device services is the same as the services found in the edgex-go repository.","title":"Release"},{"location":"design/adr/devops/0007-Release-Automation/#core-services","text":"","title":"Core Services"},{"location":"design/adr/devops/0007-Release-Automation/#during-development_2","text":"For the Core services we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging). Snaps will be pushed daily to the edge (experimental) and beta (QA) channels in the Snapcraft store because the core services snap can take up to a hour and half to build.","title":"During Development"},{"location":"design/adr/devops/0007-Release-Automation/#release_2","text":"The release automation will need to do the following: Set version tag on GitHub. (IE: 1.0.0-dev.X -> 1.0.0) Promote docker images in our Nexus repository from docker.staging to docker.release and public Docker hub. Promote snaps from the beta (QA) channel to the release channel in the Snapcraft store.","title":"Release"},{"location":"design/legacy-design/","text":"Legacy Design Documents Name/Link Short Description Registry Abstraction Decouple EdgeX services from Consul device-service/Discovery Dynamically discover new devices","title":"Legacy Design Documents"},{"location":"design/legacy-design/#legacy-design-documents","text":"Name/Link Short Description Registry Abstraction Decouple EdgeX services from Consul device-service/Discovery Dynamically discover new devices","title":"Legacy Design Documents"},{"location":"design/legacy-design/device-service/discovery/","text":"Dynamic Device Discovery Overview Some device protocols allow for devices to be discovered automatically. A Device Service may include a capability for discovering devices and creating the corresponding Device objects within EdgeX. A framework for doing so will be implemented in the Device Service SDKs. The discovery process will operate as follows: Discovery is triggered either on an internal timer or by a call to a REST endpoint The SDK will call a function provided by the DS implementation to request a device scan The implementation calls back to the SDK with details of devices which it has found The SDK filters these devices against a set of acceptance criteria The SDK adds accepted devices in core-metadata. These are now available in the EdgeX system Triggering Discovery A boolean configuration value Device/Discovery/Enabled defaults to false. If this value is set true, and the DS implementation supports discovery, discovery is enabled. The SDK will respond to POST requests on the the /discovery endpoint. No content is required in the request. This call will return one of the following codes: 202: discovery has been triggered or is already running. The response should indicate which, and contain the correlation id that will be used by any resulting requests for device addition 423: the service is locked (admin state) or disabled (operating state) 500: unknown or unanticipated issues exist 501: discovery is not supported by this protocol implementation 503: discovery is disabled by configuration In each of the failure cases a meaningful error message should be returned. In the case where discovery is triggered, the discovery process will run in a new thread or goroutine, so that the REST call may return immediately. An integer configuration value Device/Discovery/Interval defaults to zero. If this value is set to a positive value, and discovery is enabled, the discovery process will be triggered at the specified interval (in seconds). Finding Devices When discovery is triggered, the SDK calls the implementation function provided by the Device Service. This should perform whatever protocol-specific procedure is necessary to find devices, and pass these devices into the SDK by calling the SDK's filtered device addition function. Note: The implementation should call back for every device found. The SDK is to take responsibility for filtering out devices which have already been added. The information required for a found device is as follows: An autogenerated device name The Protocol Properties of the device Optionally, a description string Optionally, a list of label strings The filtered device addition function will take as an argument a collection of structs containing the above data. An implementation may choose to make one call per discovered device, but implementors are encouraged to batch the devices if practical, as in future EdgeX versions it will be possible for the SDK to create all required new devices in a single call to core-metadata. Rationale: An alternative design would have the implementation function return the collection of discovered devices to the SDK. Using a callback mechanism instead has the following advantages: Allows for asynchronous operation. In this mode the DS implementation will intiate discovery and return immediately. For example discovery may be initiated by sending a broadcast packet. Devices will then send return packets indicating their existence. The thread handling inbound network traffic can on receipt of such packets call the filtered device addition function directly. Allows DS implementations where devices self-announce to call the filtered device addition function independent of the discovery process Filtered Device Addition The filter criteria for discovered devices are represented by Provision Watchers. A Provision Watcher contains the following fields: Identifiers : A set of name-value pairs against which a new device's ProtocolProperties are matched BlockingIdentifiers : A further set of name-value pairs which are also matched against a new device's ProtocolProperties Profile : The name of a DeviceProfile which should be assigned to new devices which pass this ProvisionWatcher AdminState : The initial Administrative State for new devices which pass this ProvisionWatcher A candidate new device passes a ProvisionWatcher if all of the Identifiers match, and none of the BlockingIdentifiers . The values specified in Identifiers are regular expressions. Note: the above is a whitelist+blacklist scheme. If a discovered Device is manually removed from EdgeX, it will be necessary to adjust the ProvisionWatcher via which it was added, either by making the Identifiers more specific or by adding BlockingIdentifiers , otherwise the Device will be re-added the next time Discovery is initiated. Note: ProvisionWatchers are stored in core-metadata. A facility for managing ProvisionWatchers is needed, eg edgex-cli could be extended","title":"Discovery"},{"location":"design/legacy-design/device-service/discovery/#dynamic-device-discovery","text":"","title":"Dynamic Device Discovery"},{"location":"design/legacy-design/device-service/discovery/#overview","text":"Some device protocols allow for devices to be discovered automatically. A Device Service may include a capability for discovering devices and creating the corresponding Device objects within EdgeX. A framework for doing so will be implemented in the Device Service SDKs. The discovery process will operate as follows: Discovery is triggered either on an internal timer or by a call to a REST endpoint The SDK will call a function provided by the DS implementation to request a device scan The implementation calls back to the SDK with details of devices which it has found The SDK filters these devices against a set of acceptance criteria The SDK adds accepted devices in core-metadata. These are now available in the EdgeX system","title":"Overview"},{"location":"design/legacy-design/device-service/discovery/#triggering-discovery","text":"A boolean configuration value Device/Discovery/Enabled defaults to false. If this value is set true, and the DS implementation supports discovery, discovery is enabled. The SDK will respond to POST requests on the the /discovery endpoint. No content is required in the request. This call will return one of the following codes: 202: discovery has been triggered or is already running. The response should indicate which, and contain the correlation id that will be used by any resulting requests for device addition 423: the service is locked (admin state) or disabled (operating state) 500: unknown or unanticipated issues exist 501: discovery is not supported by this protocol implementation 503: discovery is disabled by configuration In each of the failure cases a meaningful error message should be returned. In the case where discovery is triggered, the discovery process will run in a new thread or goroutine, so that the REST call may return immediately. An integer configuration value Device/Discovery/Interval defaults to zero. If this value is set to a positive value, and discovery is enabled, the discovery process will be triggered at the specified interval (in seconds).","title":"Triggering Discovery"},{"location":"design/legacy-design/device-service/discovery/#finding-devices","text":"When discovery is triggered, the SDK calls the implementation function provided by the Device Service. This should perform whatever protocol-specific procedure is necessary to find devices, and pass these devices into the SDK by calling the SDK's filtered device addition function. Note: The implementation should call back for every device found. The SDK is to take responsibility for filtering out devices which have already been added. The information required for a found device is as follows: An autogenerated device name The Protocol Properties of the device Optionally, a description string Optionally, a list of label strings The filtered device addition function will take as an argument a collection of structs containing the above data. An implementation may choose to make one call per discovered device, but implementors are encouraged to batch the devices if practical, as in future EdgeX versions it will be possible for the SDK to create all required new devices in a single call to core-metadata. Rationale: An alternative design would have the implementation function return the collection of discovered devices to the SDK. Using a callback mechanism instead has the following advantages: Allows for asynchronous operation. In this mode the DS implementation will intiate discovery and return immediately. For example discovery may be initiated by sending a broadcast packet. Devices will then send return packets indicating their existence. The thread handling inbound network traffic can on receipt of such packets call the filtered device addition function directly. Allows DS implementations where devices self-announce to call the filtered device addition function independent of the discovery process","title":"Finding Devices"},{"location":"design/legacy-design/device-service/discovery/#filtered-device-addition","text":"The filter criteria for discovered devices are represented by Provision Watchers. A Provision Watcher contains the following fields: Identifiers : A set of name-value pairs against which a new device's ProtocolProperties are matched BlockingIdentifiers : A further set of name-value pairs which are also matched against a new device's ProtocolProperties Profile : The name of a DeviceProfile which should be assigned to new devices which pass this ProvisionWatcher AdminState : The initial Administrative State for new devices which pass this ProvisionWatcher A candidate new device passes a ProvisionWatcher if all of the Identifiers match, and none of the BlockingIdentifiers . The values specified in Identifiers are regular expressions. Note: the above is a whitelist+blacklist scheme. If a discovered Device is manually removed from EdgeX, it will be necessary to adjust the ProvisionWatcher via which it was added, either by making the Identifiers more specific or by adding BlockingIdentifiers , otherwise the Device will be re-added the next time Discovery is initiated. Note: ProvisionWatchers are stored in core-metadata. A facility for managing ProvisionWatchers is needed, eg edgex-cli could be extended","title":"Filtered Device Addition"},{"location":"design/legacy-requirements/","text":"Legacy Requirements","title":"Legacy Requirements"},{"location":"design/legacy-requirements/#legacy-requirements","text":"","title":"Legacy Requirements"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/","text":"MQTT EdgeX - Edinburgh Release Overview In this example, we use the simulator instead of real device. This provides a straight-forward way to test the device-mqtt features. Run an MQTT Broker Eclipse Mosquitto is an open source (EPL/EDL licensed) message broker that implements the MQTT protocol versions 5.0, 3.1.1 and 3.1. Run Mosquitto using the following docker command: docker run -d --rm --name broker -p 1883:1883 eclipse-mosquitto Run an MQTT Device Simulator This simulator has three behaviors: Publish random number data every 15 seconds Receive the reading request, then return the response Receive the put request, then change the device value We created the following script to simulate the MQTT device: // mock - device . js function getRandomFloat ( min , max ) { return Math . random () * ( max - min ) + min ; } const deviceName = \"MQ_DEVICE\" ; let message = \"test-message\" ; // 1 . Publish random number every 15 seconds schedule ( '*/15 * * * * *' , () => { let body = { \"name\" : deviceName , \"cmd\" : \"randnum\" , \"randnum\" : getRandomFloat ( 25 , 29 ). toFixed ( 1 ) } ; publish ( 'DataTopic' , JSON . stringify ( body )); } ); // 2 . Receive the reading request , then return the response // 3 . Receive the put request , then change the device value subscribe ( \"CommandTopic\" , ( topic , val ) => { var data = val ; if ( data . method == \"set\" ) { message = data [ data . cmd ] } else { switch ( data . cmd ) { case \"ping\" : data . ping = \"pong\" ; break ; case \"message\" : data . message = message ; break ; case \"randnum\" : data . randnum = 12 . 123 ; break ; } } publish ( \"ResponseTopic\" , JSON . stringify ( data )); } ); To run the device simulator, enter the commands shown below with the following changes: Replace the /path/to/mqtt-scripts in the example mv command with the correct path Replace the mqtt-broker-ip in the example docker run command with the correct broker IP: mv mock-device.js /path/to/mqtt-scripts docker run -d --restart=always --name=mqtt-scripts \\ -v /path/to/mqtt-scripts:/scripts \\ dersimn/mqtt-scripts --url mqtt://mqtt-broker-ip --dir /scripts Setup In this section, we create a folder that contains files required for deployment: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml Device Profile (mqtt.test.device.profile.yml) The DeviceProfile defines the device's values and operation method, which can be Read or Write. Create a device profile, named mqtt.test.device.profile.yml, with the following content: # mqtt . test . device . profile . yml name : \"Test.Device.MQTT.Profile\" manufacturer : \"iot\" model : \"MQTT-DEVICE\" description : \"Test device profile\" labels : - \"mqtt\" - \"test\" deviceResources : - name : randnum description : \"device random number\" properties : value : { type : \"Float64\" , size : \"4\" , readWrite : \"R\" , floatEncoding : \"eNotation\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : ping description : \"device awake\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"R\" , defaultValue : \"pong\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : message description : \"device message\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"W\" , scale : \"\" , offset : \"\" , base : \"\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } deviceCommands : - name : testrandnum get : - { index : \"1\" , operation : \"get\" , object : \"randnum\" , parameter : \"randnum\" } - name : testping get : - { index : \"1\" , operation : \"get\" , object : \"ping\" , parameter : \"ping\" } - name : testmessage get : - { index : \"1\" , operation : \"get\" , object : \"message\" , parameter : \"message\" } set : - { index : \"1\" , operation : \"set\" , object : \"message\" , parameter : \"message\" } coreCommands : - name : testrandnum get : path : \"/api/v1/device/{deviceId}/testrandnum\" responses : - code : \"200\" description : \"get the random value\" expectedValues : [ \"randnum\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testping get : path : \"/api/v1/device/{deviceId}/testping\" responses : - code : \"200\" description : \"ping the device\" expectedValues : [ \"ping\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testmessage get : path : \"/api/v1/device/{deviceId}/testmessage\" responses : - code : \"200\" description : \"get the message\" expectedValues : [ \"message\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] put : path : \"/api/v1/device/{deviceId}/testmessage\" parameterNames : [ \"message\" ] responses : - code : \"204\" description : \"set the message.\" expectedValues : [] - code : \"503\" description : \"service unavailable\" expectedValues : [] Device Service Configuration (configuration.toml) Use this configuration file to define devices and schedule jobs. device-mqtt generates a relative instance on start-up. MQTT is subscribe/publish pattern, so we must define the MQTT connection information in the [DeviceList.Protocols] section of the configuration file. Create the configuration file, named configuration.toml, as shown below replacing the host IP with your host address: # configuration . toml [ Writable ] LogLevel = 'DEBUG' [ Service ] Host = \"edgex-device-mqtt\" Port = 49982 ConnectRetries = 3 Labels = [] OpenMsg = \"device mqtt started\" Timeout = 5000 EnableAsyncReadings = true AsyncBufferSize = 16 [ Registry ] Host = \"edgex-core-consul\" Port = 8500 CheckInterval = \"10s\" FailLimit = 3 FailWaitTime = 10 Type = \"consul\" [ Logging ] EnableRemote = false File = \"./device-mqtt.log\" [ Clients ] [ Clients.Data ] Name = \"edgex-core-data\" Protocol = \"http\" Host = \"edgex-core-data\" Port = 48080 Timeout = 50000 [ Clients.Metadata ] Name = \"edgex-core-metadata\" Protocol = \"http\" Host = \"edgex-core-metadata\" Port = 48081 Timeout = 50000 [ Clients.Logging ] Name = \"edgex-support-logging\" Protocol = \"http\" Host = \"edgex-support-logging\" Port = 48061 [ Device ] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" # Driver configs [ Driver ] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" ResponseSchema = \"tcp\" ResponseHost = \"192.168.16.68\" ResponsePort = \"1883\" ResponseUser = \"\" ResponsePassword = \"\" ResponseQos = \"0\" ResponseKeepAlive = \"3600\" ResponseClientId = \"CommandResponseSubscriber\" ResponseTopic = \"ResponseTopic\" In the Driver configs section : IncomingXxx defines the DataTopic for receiving an async value from the device ResponseXxx defines the ResponseTopic for receiving a command response from the device Add Device Service to docker-compose File (docker-compose.yml) Download the docker-compose file from https://github.com/edgexfoundry/developer-scripts/blob/master/releases/fuji/compose-files/docker-compose-fuji.yml . Because we deploy EdgeX using docker-compose, we must add device-mqtt to the docker-compose file. If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-mqtt internal use. This is illustrated in the following docker-compose file snippet: device-mqtt: image: edgexfoundry/docker-device-mqtt-go:1.0.0 ports: - \"49982:49982\" container_name: edgex-device-mqtt hostname: edgex-device-mqtt networks: - edgex-network volumes: - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - ./mqtt:/custom-config depends_on: - data - command entrypoint: - /device-mqtt - --registry=consul://edgex-core-consul:8500 - --confdir=/custom-config When using Device Services, the user has to provide the registry URL in --registry argument. Start EdgeX Foundry on Docker Once the following folder has been populated, we can deploy EdgeX: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml Deploy EdgeX using the following commands: cd path/to/device-service-demo docker-compose pull docker-compose up -d After the services start, check the consul dashboard as follows: Execute Commands Now we're ready to run some commands. Find Executable Commands Use the following query to find executable commands: $ curl http : // your - edgex - server - ip : 48082 / api / v1 / device | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1972 100 1972 0 0 64349 0 --:--:-- --:--:-- --:--:-- 65733 [ { \"location\" : null , \"adminState\" : \"UNLOCKED\" , \"commands\" : [ { ... }, { ... }, { \"get\" : { \"responses\" : [ { \"code\" : \"503\" , \"description\" : \"service unavailable\" } ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"modified\" : 1559195042046 , \"name\" : \"testmessage\" , \"put\" : { \"parameterNames\" : [ \"message\" ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"created\" : 1559195042046 , \"id\" : \"0c257a37-2f72-4d23-b2b1-2c08e895060a\" } ], \"lastReported\" : 0 , \"operatingState\" : \"ENABLED\" , \"name\" : \"MQ_DEVICE\" , \"lastConnected\" : 0 , \"id\" : \"ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff\" , \"labels\" : [ \"MQTT\" ] } ] Execute put Command Execute a put command according to the url and parameterNames, replacing [host] with the server IP when running the edgex-core-command. This can be done in either of the following ways: $ curl http://your-edgex-server-ip:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a \\ -H \"Content-Type:application/json\" -X PUT \\ -d '{\"message\":\"Hello!\"}' or \\$ curl \" http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage \" -H \"Content-Type:application/json\" -X PUT -d '{\"message\":\"Hello!\"}' Execute get Command Execute a get command as follows: $ curl \"http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage\" | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 139 100 139 0 0 132 0 0 : 00 : 01 0 : 00 : 01 --:--:-- 132 { \"readings\" : [ { \"name\" : \"message\" , \"device\" : \"MQ_DEVICE\" , \"value\" : \"Hello!\" , \"origin\" : 1559196276732 } ], \"device\" : \"MQ_DEVICE\" , \"origin\" : 1559196276738 } Schedule Job The schedule job is defined in the [[DeviceList.AutoEvents]] section of the TOML configuration file: # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" After the service starts, query core-data's reading API. The results show that the service auto-executes the command every 30 secs, as shown below: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1613 100 1613 0 0 372 k 0 --:--:-- --:--:-- --:--:-- 393 k [ { \"value\" : \"1.212300e+01\" , \"origin\" : 1559197206092 , \"modified\" : 1559197206104 , \"id\" : \"59f2a768-ad72-49a1-9df9-700d8599a890\" , \"created\" : 1559197206104 , \"device\" : \"MQ_DEVICE\" , \"name\" : \"randnum\" }, { ... }, { \"name\" : \"randnum\" , \"device\" : \"MQ_DEVICE\" , \"modified\" : 1559197175109 , \"created\" : 1559197175109 , \"id\" : \"f9dc39e0-5326-45d0-831d-fd0cd106fe2f\" , \"origin\" : 1559197175098 , \"value\" : \"1.212300e+01\" }, ] Async Device Reading device-mqtt subscribes to a DataTopic , which is wait for real device* to send value to broker , then device-mqtt parses the value and sends it back to core-data . The data format contains the following values: name = device name cmd = deviceResource name method = get or put cmd = device reading You must define this connection information in the driver configuration file, as follows: [Driver] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" The following results show that the mock device sent the reading every 15 secs: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 539 100 539 0 0 169 k 0 --:--:-- --:--:-- --:--:-- 175 k [ { ... }, { \"name\" : \"randnum\" , \"created\" : 1559197140013 , \"origin\" : 1559197140006 , \"modified\" : 1559197140013 , \"id\" : \"286cc305-42f6-4bca-ad41-3af52301c9f7\" , \"value\" : \"2.830000e+01\" , \"device\" : \"MQ_DEVICE\" }, { \"modified\" : 1559197125011 , \"name\" : \"randnum\" , \"created\" : 1559197125011 , \"origin\" : 1559197125004 , \"device\" : \"MQ_DEVICE\" , \"value\" : \"2.690000e+01\" , \"id\" : \"c243e8c6-a904-4102-baff-8a5e4829c4f6\" } ]","title":"MQTT"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#mqtt","text":"EdgeX - Edinburgh Release","title":"MQTT"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#overview","text":"In this example, we use the simulator instead of real device. This provides a straight-forward way to test the device-mqtt features.","title":"Overview"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#run-an-mqtt-broker","text":"Eclipse Mosquitto is an open source (EPL/EDL licensed) message broker that implements the MQTT protocol versions 5.0, 3.1.1 and 3.1. Run Mosquitto using the following docker command: docker run -d --rm --name broker -p 1883:1883 eclipse-mosquitto","title":"Run an MQTT Broker"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#run-an-mqtt-device-simulator","text":"This simulator has three behaviors: Publish random number data every 15 seconds Receive the reading request, then return the response Receive the put request, then change the device value We created the following script to simulate the MQTT device: // mock - device . js function getRandomFloat ( min , max ) { return Math . random () * ( max - min ) + min ; } const deviceName = \"MQ_DEVICE\" ; let message = \"test-message\" ; // 1 . Publish random number every 15 seconds schedule ( '*/15 * * * * *' , () => { let body = { \"name\" : deviceName , \"cmd\" : \"randnum\" , \"randnum\" : getRandomFloat ( 25 , 29 ). toFixed ( 1 ) } ; publish ( 'DataTopic' , JSON . stringify ( body )); } ); // 2 . Receive the reading request , then return the response // 3 . Receive the put request , then change the device value subscribe ( \"CommandTopic\" , ( topic , val ) => { var data = val ; if ( data . method == \"set\" ) { message = data [ data . cmd ] } else { switch ( data . cmd ) { case \"ping\" : data . ping = \"pong\" ; break ; case \"message\" : data . message = message ; break ; case \"randnum\" : data . randnum = 12 . 123 ; break ; } } publish ( \"ResponseTopic\" , JSON . stringify ( data )); } ); To run the device simulator, enter the commands shown below with the following changes: Replace the /path/to/mqtt-scripts in the example mv command with the correct path Replace the mqtt-broker-ip in the example docker run command with the correct broker IP: mv mock-device.js /path/to/mqtt-scripts docker run -d --restart=always --name=mqtt-scripts \\ -v /path/to/mqtt-scripts:/scripts \\ dersimn/mqtt-scripts --url mqtt://mqtt-broker-ip --dir /scripts","title":"Run an MQTT Device Simulator"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#setup","text":"In this section, we create a folder that contains files required for deployment: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml","title":"Setup"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#device-profile-mqtttestdeviceprofileyml","text":"The DeviceProfile defines the device's values and operation method, which can be Read or Write. Create a device profile, named mqtt.test.device.profile.yml, with the following content: # mqtt . test . device . profile . yml name : \"Test.Device.MQTT.Profile\" manufacturer : \"iot\" model : \"MQTT-DEVICE\" description : \"Test device profile\" labels : - \"mqtt\" - \"test\" deviceResources : - name : randnum description : \"device random number\" properties : value : { type : \"Float64\" , size : \"4\" , readWrite : \"R\" , floatEncoding : \"eNotation\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : ping description : \"device awake\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"R\" , defaultValue : \"pong\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : message description : \"device message\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"W\" , scale : \"\" , offset : \"\" , base : \"\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } deviceCommands : - name : testrandnum get : - { index : \"1\" , operation : \"get\" , object : \"randnum\" , parameter : \"randnum\" } - name : testping get : - { index : \"1\" , operation : \"get\" , object : \"ping\" , parameter : \"ping\" } - name : testmessage get : - { index : \"1\" , operation : \"get\" , object : \"message\" , parameter : \"message\" } set : - { index : \"1\" , operation : \"set\" , object : \"message\" , parameter : \"message\" } coreCommands : - name : testrandnum get : path : \"/api/v1/device/{deviceId}/testrandnum\" responses : - code : \"200\" description : \"get the random value\" expectedValues : [ \"randnum\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testping get : path : \"/api/v1/device/{deviceId}/testping\" responses : - code : \"200\" description : \"ping the device\" expectedValues : [ \"ping\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testmessage get : path : \"/api/v1/device/{deviceId}/testmessage\" responses : - code : \"200\" description : \"get the message\" expectedValues : [ \"message\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] put : path : \"/api/v1/device/{deviceId}/testmessage\" parameterNames : [ \"message\" ] responses : - code : \"204\" description : \"set the message.\" expectedValues : [] - code : \"503\" description : \"service unavailable\" expectedValues : []","title":"Device Profile (mqtt.test.device.profile.yml)"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#device-service-configuration-configurationtoml","text":"Use this configuration file to define devices and schedule jobs. device-mqtt generates a relative instance on start-up. MQTT is subscribe/publish pattern, so we must define the MQTT connection information in the [DeviceList.Protocols] section of the configuration file. Create the configuration file, named configuration.toml, as shown below replacing the host IP with your host address: # configuration . toml [ Writable ] LogLevel = 'DEBUG' [ Service ] Host = \"edgex-device-mqtt\" Port = 49982 ConnectRetries = 3 Labels = [] OpenMsg = \"device mqtt started\" Timeout = 5000 EnableAsyncReadings = true AsyncBufferSize = 16 [ Registry ] Host = \"edgex-core-consul\" Port = 8500 CheckInterval = \"10s\" FailLimit = 3 FailWaitTime = 10 Type = \"consul\" [ Logging ] EnableRemote = false File = \"./device-mqtt.log\" [ Clients ] [ Clients.Data ] Name = \"edgex-core-data\" Protocol = \"http\" Host = \"edgex-core-data\" Port = 48080 Timeout = 50000 [ Clients.Metadata ] Name = \"edgex-core-metadata\" Protocol = \"http\" Host = \"edgex-core-metadata\" Port = 48081 Timeout = 50000 [ Clients.Logging ] Name = \"edgex-support-logging\" Protocol = \"http\" Host = \"edgex-support-logging\" Port = 48061 [ Device ] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" # Driver configs [ Driver ] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" ResponseSchema = \"tcp\" ResponseHost = \"192.168.16.68\" ResponsePort = \"1883\" ResponseUser = \"\" ResponsePassword = \"\" ResponseQos = \"0\" ResponseKeepAlive = \"3600\" ResponseClientId = \"CommandResponseSubscriber\" ResponseTopic = \"ResponseTopic\" In the Driver configs section : IncomingXxx defines the DataTopic for receiving an async value from the device ResponseXxx defines the ResponseTopic for receiving a command response from the device","title":"Device Service Configuration (configuration.toml)"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#add-device-service-to-docker-compose-file-docker-composeyml","text":"Download the docker-compose file from https://github.com/edgexfoundry/developer-scripts/blob/master/releases/fuji/compose-files/docker-compose-fuji.yml . Because we deploy EdgeX using docker-compose, we must add device-mqtt to the docker-compose file. If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-mqtt internal use. This is illustrated in the following docker-compose file snippet: device-mqtt: image: edgexfoundry/docker-device-mqtt-go:1.0.0 ports: - \"49982:49982\" container_name: edgex-device-mqtt hostname: edgex-device-mqtt networks: - edgex-network volumes: - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - ./mqtt:/custom-config depends_on: - data - command entrypoint: - /device-mqtt - --registry=consul://edgex-core-consul:8500 - --confdir=/custom-config When using Device Services, the user has to provide the registry URL in --registry argument.","title":"Add Device Service to docker-compose File (docker-compose.yml)"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#start-edgex-foundry-on-docker","text":"Once the following folder has been populated, we can deploy EdgeX: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml Deploy EdgeX using the following commands: cd path/to/device-service-demo docker-compose pull docker-compose up -d After the services start, check the consul dashboard as follows:","title":"Start EdgeX Foundry on Docker"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#execute-commands","text":"Now we're ready to run some commands.","title":"Execute Commands"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#find-executable-commands","text":"Use the following query to find executable commands: $ curl http : // your - edgex - server - ip : 48082 / api / v1 / device | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1972 100 1972 0 0 64349 0 --:--:-- --:--:-- --:--:-- 65733 [ { \"location\" : null , \"adminState\" : \"UNLOCKED\" , \"commands\" : [ { ... }, { ... }, { \"get\" : { \"responses\" : [ { \"code\" : \"503\" , \"description\" : \"service unavailable\" } ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"modified\" : 1559195042046 , \"name\" : \"testmessage\" , \"put\" : { \"parameterNames\" : [ \"message\" ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"created\" : 1559195042046 , \"id\" : \"0c257a37-2f72-4d23-b2b1-2c08e895060a\" } ], \"lastReported\" : 0 , \"operatingState\" : \"ENABLED\" , \"name\" : \"MQ_DEVICE\" , \"lastConnected\" : 0 , \"id\" : \"ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff\" , \"labels\" : [ \"MQTT\" ] } ]","title":"Find Executable Commands"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#execute-put-command","text":"Execute a put command according to the url and parameterNames, replacing [host] with the server IP when running the edgex-core-command. This can be done in either of the following ways: $ curl http://your-edgex-server-ip:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a \\ -H \"Content-Type:application/json\" -X PUT \\ -d '{\"message\":\"Hello!\"}' or \\$ curl \" http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage \" -H \"Content-Type:application/json\" -X PUT -d '{\"message\":\"Hello!\"}'","title":"Execute put Command"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#execute-get-command","text":"Execute a get command as follows: $ curl \"http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage\" | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 139 100 139 0 0 132 0 0 : 00 : 01 0 : 00 : 01 --:--:-- 132 { \"readings\" : [ { \"name\" : \"message\" , \"device\" : \"MQ_DEVICE\" , \"value\" : \"Hello!\" , \"origin\" : 1559196276732 } ], \"device\" : \"MQ_DEVICE\" , \"origin\" : 1559196276738 }","title":"Execute get Command"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#schedule-job","text":"The schedule job is defined in the [[DeviceList.AutoEvents]] section of the TOML configuration file: # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" After the service starts, query core-data's reading API. The results show that the service auto-executes the command every 30 secs, as shown below: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1613 100 1613 0 0 372 k 0 --:--:-- --:--:-- --:--:-- 393 k [ { \"value\" : \"1.212300e+01\" , \"origin\" : 1559197206092 , \"modified\" : 1559197206104 , \"id\" : \"59f2a768-ad72-49a1-9df9-700d8599a890\" , \"created\" : 1559197206104 , \"device\" : \"MQ_DEVICE\" , \"name\" : \"randnum\" }, { ... }, { \"name\" : \"randnum\" , \"device\" : \"MQ_DEVICE\" , \"modified\" : 1559197175109 , \"created\" : 1559197175109 , \"id\" : \"f9dc39e0-5326-45d0-831d-fd0cd106fe2f\" , \"origin\" : 1559197175098 , \"value\" : \"1.212300e+01\" }, ]","title":"Schedule Job"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#async-device-reading","text":"device-mqtt subscribes to a DataTopic , which is wait for real device* to send value to broker , then device-mqtt parses the value and sends it back to core-data . The data format contains the following values: name = device name cmd = deviceResource name method = get or put cmd = device reading You must define this connection information in the driver configuration file, as follows: [Driver] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" The following results show that the mock device sent the reading every 15 secs: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 539 100 539 0 0 169 k 0 --:--:-- --:--:-- --:--:-- 175 k [ { ... }, { \"name\" : \"randnum\" , \"created\" : 1559197140013 , \"origin\" : 1559197140006 , \"modified\" : 1559197140013 , \"id\" : \"286cc305-42f6-4bca-ad41-3af52301c9f7\" , \"value\" : \"2.830000e+01\" , \"device\" : \"MQ_DEVICE\" }, { \"modified\" : 1559197125011 , \"name\" : \"randnum\" , \"created\" : 1559197125011 , \"origin\" : 1559197125004 , \"device\" : \"MQ_DEVICE\" , \"value\" : \"2.690000e+01\" , \"id\" : \"c243e8c6-a904-4102-baff-8a5e4829c4f6\" } ]","title":"Async Device Reading"},{"location":"examples/Ch-ExamplesAddingModbusDevice/","text":"Modbus EdgeX - Delhi Release PowerScout 3037 Power Submeter https://shop.dentinstruments.com/products/powerscout-3037-ps3037 https://www.dentinstruments.com/hs-fs/hub/472997/file-2378482732-pdf/Pdf_Files/PS3037_Manual.pdf In this example, we simulate the PowerScout meter instead of using a real device. This provides a straight-forward way to test the device-modbus features. Environment You can use any operating system that can install docker and docker-compose. In this example, we use Photon OS to delpoy EdgeX using docker. The system requirements can be found at https://docs.edgexfoundry.org/Ch-GettingStartedUsers.html#what-you-need . Modbus Device (Simulator) http://modbuspal.sourceforge.net/ To simulate sensors, such as temperature and humidity, do the following: Add two mock devices: Add registers according to the device manual: Add the ModbusPal support value auto-generator, which can bind to registers: Set Up Before Starting Services The following sections describe how to complete the set up before starting the services. If you prefer to start the services and then add the device, see Set Up After Starting Services Set Up Device Profile The DeviceProfile defines the device's values and operation method, which can be Read or Write. You can download and use the provided modbus.test.device.profile.yml <modbus.test.device.profile.yml> {.interpreted-text role=\"download\"}. In the Modbus protocol, we must define attributes: primaryTable : HOLDING_REGISTERS, INPUT_REGISTERS, COILS, DISCRETES_INPUT startingAddress specifies the address in Modbus device The Property value type decides how many registers will be read. Like Holding registers, a register has 16 bits. If the device manual specifies that a value has two registers, define it as FLOAT32 or INT32 or UINT32 in the deviceProfile. Once we execute a command, device-modbus knows its value type and register type, startingAddress, and register length. So it can read or write value using the modbus protocol. | | Set Up Device Service Configuration Use this configuration file to define devices and schedule jobs. The device-modbus generates a relative instance on startup. device-modbus offers two types of protocol, Modbus TCP and Modbus RTU. An addressable can be defined as shown below: protocol Name Protocol Address Port Path Modbus TCP Gateway address TCP 10.211.55.6 502 1 1 Modbus RTU Gateway address RTU /tmp/slave,19200,8,1,0 502 2 2 Path defines the Modbus device's unit ID (or slave ID). In the RTU protocol, address is defined in five comma-separated parts: serial port baud rate data bits stop bits parity (N - None is 0, O - Odd is 1, E - Even is 2, default is E). [Logging] EnableRemote = false File = \"./device-Modbus.log\" Level = \"DEBUG\" [Device] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre-define Devices [[DeviceList]] Name = \"Modbus TCP test device\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"1\" [[DeviceList]] Name = \"Modbus TCP test device 2\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"2\" # Pre-define Schedule Configuration : [[Schedules]] Name = \"20sec-schedule\" Frequency = \"PT20S\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Read Switch status\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"GET\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Put Configuration\" Parameters = \"\\[{\" DemandWindowSize \": \" 110 \"},{\" LineFrequency \": \" 50 \"}\\]\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"Put\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" You can download and use the provided EdgeX_ExampleModbus_configuration.toml <EdgeX_ExampleModbus_configuration.toml> {.interpreted-text role=\"download\"}. Add Device Service to docker-compose File Because we deploy EdgeX using docker-compose, we must add the device-modbus to the docker-compose file ( https://github.com/edgexfoundry/developer-scripts/blob/master/releases/fuji/compose-files/docker-compose-fuji.yml ). If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-modbus internal use. Start EdgeX Foundry on Docker Finally, we can deploy EdgeX in the Photon OS. Prepare configuration files by moving the files to the Photon OS Deploy EdgeX using the following commands: docker-compose pull docker-compose up -d Check the consul dashboard Set Up After Starting Services If the services are already running and you want to add a device, you can use the Core Metadata API as outlined in this section. If you set up the device profile and Service as described in Set Up Before Starting Services , you can skip this section. To add a device after starting the services, complete the following steps: Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body in form-data format, and the created ID will be returned. The following figure is an example if you use Postman to send the request Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : a. If IP connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\", \"address\": \"10.0.1.29\", \"port\": 502 } b. If serially connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services. Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" } , \"labels\" : [ ], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" } , \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1. Execute Commands Now we're ready to run some commands. Find Executable Commands Use the following query to find executable commands: photon-ip:48082/api/v1/device | Execute GET command Replace \\<host> with the server IP when running the edgex-core-command. Execute PUT command Execute PUT command according to url and parameterNames . Schedule Job After service startup, query core-data's reading API. The results show that the service auto-executes the command every 20 seconds.","title":"Modbus"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#modbus","text":"EdgeX - Delhi Release PowerScout 3037 Power Submeter https://shop.dentinstruments.com/products/powerscout-3037-ps3037 https://www.dentinstruments.com/hs-fs/hub/472997/file-2378482732-pdf/Pdf_Files/PS3037_Manual.pdf In this example, we simulate the PowerScout meter instead of using a real device. This provides a straight-forward way to test the device-modbus features.","title":"Modbus"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#environment","text":"You can use any operating system that can install docker and docker-compose. In this example, we use Photon OS to delpoy EdgeX using docker. The system requirements can be found at https://docs.edgexfoundry.org/Ch-GettingStartedUsers.html#what-you-need .","title":"Environment"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#modbus-device-simulator","text":"http://modbuspal.sourceforge.net/ To simulate sensors, such as temperature and humidity, do the following: Add two mock devices: Add registers according to the device manual: Add the ModbusPal support value auto-generator, which can bind to registers:","title":"Modbus Device (Simulator)"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-before-starting-services","text":"The following sections describe how to complete the set up before starting the services. If you prefer to start the services and then add the device, see Set Up After Starting Services","title":"Set Up Before Starting Services"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-device-profile","text":"The DeviceProfile defines the device's values and operation method, which can be Read or Write. You can download and use the provided modbus.test.device.profile.yml <modbus.test.device.profile.yml> {.interpreted-text role=\"download\"}. In the Modbus protocol, we must define attributes: primaryTable : HOLDING_REGISTERS, INPUT_REGISTERS, COILS, DISCRETES_INPUT startingAddress specifies the address in Modbus device The Property value type decides how many registers will be read. Like Holding registers, a register has 16 bits. If the device manual specifies that a value has two registers, define it as FLOAT32 or INT32 or UINT32 in the deviceProfile. Once we execute a command, device-modbus knows its value type and register type, startingAddress, and register length. So it can read or write value using the modbus protocol.","title":"Set Up Device Profile"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#_1","text":"","title":"|"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#_2","text":"","title":"|"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-device-service-configuration","text":"Use this configuration file to define devices and schedule jobs. The device-modbus generates a relative instance on startup. device-modbus offers two types of protocol, Modbus TCP and Modbus RTU. An addressable can be defined as shown below: protocol Name Protocol Address Port Path Modbus TCP Gateway address TCP 10.211.55.6 502 1 1 Modbus RTU Gateway address RTU /tmp/slave,19200,8,1,0 502 2 2 Path defines the Modbus device's unit ID (or slave ID). In the RTU protocol, address is defined in five comma-separated parts: serial port baud rate data bits stop bits parity (N - None is 0, O - Odd is 1, E - Even is 2, default is E). [Logging] EnableRemote = false File = \"./device-Modbus.log\" Level = \"DEBUG\" [Device] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre-define Devices [[DeviceList]] Name = \"Modbus TCP test device\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"1\" [[DeviceList]] Name = \"Modbus TCP test device 2\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"2\" # Pre-define Schedule Configuration : [[Schedules]] Name = \"20sec-schedule\" Frequency = \"PT20S\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Read Switch status\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"GET\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Put Configuration\" Parameters = \"\\[{\" DemandWindowSize \": \" 110 \"},{\" LineFrequency \": \" 50 \"}\\]\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"Put\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" You can download and use the provided EdgeX_ExampleModbus_configuration.toml <EdgeX_ExampleModbus_configuration.toml> {.interpreted-text role=\"download\"}.","title":"Set Up Device Service Configuration"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#add-device-service-to-docker-compose-file","text":"Because we deploy EdgeX using docker-compose, we must add the device-modbus to the docker-compose file ( https://github.com/edgexfoundry/developer-scripts/blob/master/releases/fuji/compose-files/docker-compose-fuji.yml ). If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-modbus internal use.","title":"Add Device Service to docker-compose File"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#start-edgex-foundry-on-docker","text":"Finally, we can deploy EdgeX in the Photon OS. Prepare configuration files by moving the files to the Photon OS Deploy EdgeX using the following commands: docker-compose pull docker-compose up -d Check the consul dashboard","title":"Start EdgeX Foundry on Docker"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-after-starting-services","text":"If the services are already running and you want to add a device, you can use the Core Metadata API as outlined in this section. If you set up the device profile and Service as described in Set Up Before Starting Services , you can skip this section. To add a device after starting the services, complete the following steps: Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body in form-data format, and the created ID will be returned. The following figure is an example if you use Postman to send the request Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : a. If IP connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\", \"address\": \"10.0.1.29\", \"port\": 502 } b. If serially connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services. Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" } , \"labels\" : [ ], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" } , \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1.","title":"Set Up After Starting Services"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#execute-commands","text":"Now we're ready to run some commands.","title":"Execute Commands"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#find-executable-commands","text":"Use the following query to find executable commands: photon-ip:48082/api/v1/device |","title":"Find Executable Commands"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#execute-get-command","text":"Replace \\<host> with the server IP when running the edgex-core-command.","title":"Execute GET command"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#execute-put-command","text":"Execute PUT command according to url and parameterNames .","title":"Execute PUT command"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#schedule-job","text":"After service startup, query core-data's reading API. The results show that the service auto-executes the command every 20 seconds.","title":"Schedule Job"},{"location":"examples/Ch-ExamplesAddingSNMPDevice/","text":"SNMP EdgeX - Barcelona Release Ubuntu Desktop 16.04 with Docker/Docker-Compose Adding a new SNMP Device Moxa ioLogik E2210 Smart Ethernet Remote I/O with 12 DIs, 8 Dos Project Components Needed Hardware needed X86 computer with native RS485 communication device or RS485 adapter Moxa E2210 Ethernet IO -- https://www.moxa.com/product/ioLogik-E2210.htm Software needed Ubuntu Desktop 16.04 - new installation The following software was installed via the \"apt-get install\" command (ubuntu default) git curl vim (or your favorite editor) java (I used openjdk-8-jdk - 1.8.0_131) maven docker docker-compose The following software was installed from 3rd parties Postman (Linux 64bit) -- https://www.getpostman.com/ EdgeX - barcelona-docker-compose.yaml -- https://github.com/edgexfoundry/developer-scripts/blob/master/releases/barcelona/compose-files/docker-compose-barcelona-0.2.1.yml SNMP - Device documentation Device: Moxa E2210 (Smart Ethernet Remote I/O with 12 DIs, 8 DOs) https://www.moxa.com/product/ioLogik-E2210.htm Ensuring success Verify the following, prior to following the instruction on the following pages Do you know the IP address of the E2210? Do you know what port number of the E2210 is using? Does the E2210 power on? With a separate utility, can you read(from)/write(to) the E2210? Creating the Modbus yaml file An example SNMP device yaml file can be found here: SNMP device yaml . The SNMP device yaml file used in this example can be found here: this example SNMP device yaml . When you are creating your yaml file you will need to know what command options are available to use, they can be found here: https://github.com/edgexfoundry/core-domain/blob/master/src/main/java/org/edgexfoundry/domain/meta/PropertyValue.java With your favorite file editor, open the file Modify the following fields name \\<-- A/a \\~Z/z and 0 \\~ 9 && this will be needed in the future manufacturer \\<-- A/a \\~Z/z and 0 \\~ 9 model \\<-- A/a \\~Z/z and 0 \\~ 9 description \\<-- A/a \\~Z/z and 0 \\~ 9 labels \\<-- A/a \\~Z/z and 0 \\~ 9 deviceResources name: \\<-- A/a \\~Z/z and 0 \\~ 9 description: \\<-- A/a \\~Z/z and 0 \\~ 9 attributes: only edit the text inside the parenthesis value: only edit the text inside the parenthesis units: only edit the text inside the parenthesis resources name: \\<-- A/a \\~Z/z and 0 \\~ 9 get : only edit the text inside the parenthesis set: only edit the text inside the parenthesis commands name: \\<-- A/a \\~Z/z and 0 \\~ 9 path: \"/api/v1/device/{deviceId}/OnlyEditThisWord\" \\<-- A/a \\~Z/z and 0 \\~ 9 Code \"200\" expectedvalues: [make same as OnlyEditThisWord] Code \"500\" Do not edit this section Bringing up EdgeX via Docker Starting with following system configuration: A fresh installation of Ubuntu Desktop 16.04 with all the available system updates. A working directory > /home/tester/Development/edgex Verify your Docker installation Verify that Docker is installed and working as expected. >\\$ sudo docker run hello-world Verify that the image is on the system >\\$ sudo docker ps -a Download docker-compose file Download the barcelona-docker-compose.yaml file from the EdgeX Wiki Go to \" https://wiki.edgexfoundry.org/display/FA/Barcelona \" Scroll to the bottom a look for the \"barcelona-docker-compose.yml\" file. Once downloaded, rename the file to \"docker-compose.yml\" Once the file is download, move the file into your desired working directory. Create a copy of the file and rename the copy \"docker-compose.yml\" Verify the version of dockerized EdgeX that you will be running With your favorite file editor, open the docker-compose.yml file Within the first couple of lines you will see the word \"Version\", next to that you will see a number - it should be \"2\". Version 2 refers to the Barcelona release Enable SNMP in the Docker Compose file With your favorite file editor, open the docker-compose file Find the section \"device-snmp\" section, which will be commented out with \"#\" symbols. Uncomment the entire section Save your changes and exit out of the editor Starting EdgeX Docker components Start EdgeX by using the following commands Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds docker-compose up -d config-seed Start and populate the configuration/registry microservice which all services must register with and get their configuration from 60 seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries 1 minute docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices 30 seconds docker-compose up -d metadata Start the Core Metadata microservice 1 minute docker-compose up -d data Start the Core Data microservice 1 minute docker-compose up -d command Start the Core Command microservice 1 minute docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices 1 minute docker-compose up -d export -client Start the Export Client registration microservice 1 minute docker-compose up -d export -distro Start the Export Distribution microservice 1 minute docker-compose up -d rulesengine Start the Rules Engine microservice 1 minute docker-compose up -d device -virtual Start the virtual device service 1 minute docker-compose up -d device -snmp Start the SNMP device service 1 minute Check the containers status Run a \"docker ps -a\" command to confirm that all the containers have been downloaded and started Show containers To get a list of all the EdgeX containers, you can use \"docker-compose config --services\" Stop Containers To stop (but not remove) all containers, issue \"docker-compose stop\". To stop an individual container, you can use \"docker-compose stop [compose-container-name]\". Start Containers To start all the containers (after a stop) issue \"docker-compose start\" to re-start To start an individual container, you can use \"docker-compose start [compose-container-name]\" (after that container has been stopped). Delete Containers * DANGER * To stop all the containers running and DELETE them, you can use \"docker-compose down\" EdgeX Foundry Container Logs To view the log of any container, use the command: \"docker-compose logs -f compose-container-name\" (ex. docker-compose logs -f edgex-device-snmp) At this point the Dockerized version of EdgeX is running. Adding the Device to EdgeX Importing APIs In this section you will be using the program Postman to interact with EdgeX. You will also need to have the file \"core-metadata.raml\" available to load into the Postman application. The file \"core-metadata.raml\" can be found here: \"edgex/core-metadata..../src/test/resources/raml/core-metadata.raml\" Viewing available APIs Open Postman Click on the Import button Add the file to the import dialog box - the application will take a about 30 seconds to digest the file you added. If a list of API commands do not show up on the left hand side of the application then click on the \"Collections\" tab to the right of the \"History\" tab. Create an addressable In the \"Collections\" tab, select the option \"POST /addressable action Open the body tab Modify its contents name: moxa-e2210-address protocol: HTTP (needs to be in ALL CAPS) address: 192.168.1.103 (IPV4 address) port: 161 (port # of snmp) path: empty (remove all text between parentheses) publisher, user, password, topic - do not need to be modified Press the \"Send\" button when you are finished Note the addressable id Upload the profile In the \"Collections\" tab select the option \"POST /deviceprofile/uploadfile Open the body tab Under \"Key\", look for the drop down menu for \"text\". Be sure to write \"file\" in the open box. Under \"Value\" click \"Choose Files\", locate your profile file. Press Upload Press the \"Send\" button when you are finished Note the profile id Post the device In the \"Collections\" tab select the option \"POST /device Click on the \"Body\" tab Modify its contents There are three components that are required to be modified. They are: \"Service\" \"Profile\" \"Addressable\" The others can be modified, however they are not required for operation name: moxa-e2210-device description: snmp smart ethernet io addressable: name: moxa-e2210-address (same as used in addressable) labels: labels: \"snmp\", \"rtu\",\"io\" (same as used in snmp device profile) service: name: edgex-device-snmp profile: name: name: moxa-iologik-e2210 (same as used in snmp device profile) Press the \"Send\" button when you are finished Note the addressable id What if a Mistake is Made Get device id Delete device id Get device profile id Delete device profile id Get addressable id Delete addressable id Verify Device Added Check the edgex-device-snmp logs to see if the device was added without issue \"sudo docker logs -f --tail 100 edgex-device-snmp\" Verify device is sending data In the \"Collections\" tab select the option \"GET /device Change the port number form \"48081\" http://localhost:48081/api/v1/device to port number \"48082\" http://localhost:48082/api/v1/device Press Send You should see something similar to { \"id\" : \"5a1d6f8ae4b0c3936013120f\" , \"name\" : \"diStatus0\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a1d7134e4b0c39360131212/command/5a1d6f8ae4b0c3936013120f\" , < -- This \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , Double click on the \"url\" and a new tab within Postman should open, Press Send If all went well you should see something similar to the following: {\"diStatus0\":\"0\"} If all did not go well the you will see an error or may \"{ }\" then you will need check the information you entered. If the data/result displayed was as expected, go ahead and proceed to creating a scheduled event Creating a Scheduled Event This is used to regularly get & push data to another service or for regularly viewing data. Gathering information for the addressable Got to http://localhost:48082/api/v1/device Look for the id or the device that you want to schedule an event for [ { \"name\" : \"moxa-e2210-device\" , \"id\" : \"5a280a0be4b0c39393ec7780\" , < --- This \"description\" : \"snmp smart ethernet io\" , \"labels\" : [ \"snmp\" , \"rtu\" , \"io\" ], \"adminState\" : \"unlocked\" , In this case the id is \"5a280a0be4b0c39393ec7780\" Next you want to get the \"name\" of the command you want to schedule an event for \"commands\" : [ { \"id\" : \"5a2808e6e4b0c39393ec777c\" , \"name\" : \"serverModel\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777c\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get server model number.\" , \"expectedValues\" : [ \"serverModel\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , { \"id\" : \"5a2808e6e4b0c39393ec777d\" , \"name\" : \"diStatus0\" , < --- This \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777d\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] In this example the name is \"diStatus0\". Create addressable In this section you will need to supply a path the the item you want to schedule. The path outline is: /api/v1/device/{device id}/{command name} In this case, the address would be / api / v1 / device / XXXX / diStatus0 / POST addressable \u201c name \u201d : \u201d schedule - moxa - di \u201d \u201c protocol \u201d : \u201c HTTP \u201d \u201c address \u201d : \u201c edgex - device - snmp \u201d \u201c port \u201d : \u201c xxxxx \u201d \u201c path \u201d : \u201d / api / v1 / device / { device id } / { command name }\u201d \u201c method \u201d : \u201c GET \u201d *** This will need to be added *** Create a schedule / POST schedule \u201c name \u201d : \u201d interval - moxa - di0 \u201d \u201c start \u201d : null ( remove parenthesis and replace ) \u201c end \u201d : null ( remove parenthesis and replace ) \u201c frequency \u201d : \u201c PT5S \u201d Create an event that will use the schedule / POST scheduleevent \u201c name \u201d : \u201c device - moxa - di0 \u201d \u201c addressable \u201d : {\u201c name \u201d : \u201d schedule - moxa - di \u201d} \u201c schedule \u201d : \u201d interval - moxa - di0 \u201d \u201c service \u201d : \u201c edgex - device - snmp \u201d *** This will need to be added ***","title":"SNMP"},{"location":"examples/Ch-ExamplesAddingSNMPDevice/#snmp","text":"EdgeX - Barcelona Release Ubuntu Desktop 16.04 with Docker/Docker-Compose Adding a new SNMP Device Moxa ioLogik E2210 Smart Ethernet Remote I/O with 12 DIs, 8 Dos","title":"SNMP"},{"location":"examples/Ch-ExamplesAddingSNMPDevice/#project-components-needed","text":"Hardware needed X86 computer with native RS485 communication device or RS485 adapter Moxa E2210 Ethernet IO -- https://www.moxa.com/product/ioLogik-E2210.htm Software needed Ubuntu Desktop 16.04 - new installation The following software was installed via the \"apt-get install\" command (ubuntu default) git curl vim (or your favorite editor) java (I used openjdk-8-jdk - 1.8.0_131) maven docker docker-compose The following software was installed from 3rd parties Postman (Linux 64bit) -- https://www.getpostman.com/ EdgeX - barcelona-docker-compose.yaml -- https://github.com/edgexfoundry/developer-scripts/blob/master/releases/barcelona/compose-files/docker-compose-barcelona-0.2.1.yml SNMP - Device documentation Device: Moxa E2210 (Smart Ethernet Remote I/O with 12 DIs, 8 DOs) https://www.moxa.com/product/ioLogik-E2210.htm Ensuring success Verify the following, prior to following the instruction on the following pages Do you know the IP address of the E2210? Do you know what port number of the E2210 is using? Does the E2210 power on? With a separate utility, can you read(from)/write(to) the E2210? Creating the Modbus yaml file An example SNMP device yaml file can be found here: SNMP device yaml . The SNMP device yaml file used in this example can be found here: this example SNMP device yaml . When you are creating your yaml file you will need to know what command options are available to use, they can be found here: https://github.com/edgexfoundry/core-domain/blob/master/src/main/java/org/edgexfoundry/domain/meta/PropertyValue.java With your favorite file editor, open the file Modify the following fields name \\<-- A/a \\~Z/z and 0 \\~ 9 && this will be needed in the future manufacturer \\<-- A/a \\~Z/z and 0 \\~ 9 model \\<-- A/a \\~Z/z and 0 \\~ 9 description \\<-- A/a \\~Z/z and 0 \\~ 9 labels \\<-- A/a \\~Z/z and 0 \\~ 9 deviceResources name: \\<-- A/a \\~Z/z and 0 \\~ 9 description: \\<-- A/a \\~Z/z and 0 \\~ 9 attributes: only edit the text inside the parenthesis value: only edit the text inside the parenthesis units: only edit the text inside the parenthesis resources name: \\<-- A/a \\~Z/z and 0 \\~ 9 get : only edit the text inside the parenthesis set: only edit the text inside the parenthesis commands name: \\<-- A/a \\~Z/z and 0 \\~ 9 path: \"/api/v1/device/{deviceId}/OnlyEditThisWord\" \\<-- A/a \\~Z/z and 0 \\~ 9 Code \"200\" expectedvalues: [make same as OnlyEditThisWord] Code \"500\" Do not edit this section Bringing up EdgeX via Docker Starting with following system configuration: A fresh installation of Ubuntu Desktop 16.04 with all the available system updates. A working directory > /home/tester/Development/edgex Verify your Docker installation Verify that Docker is installed and working as expected. >\\$ sudo docker run hello-world Verify that the image is on the system >\\$ sudo docker ps -a Download docker-compose file Download the barcelona-docker-compose.yaml file from the EdgeX Wiki Go to \" https://wiki.edgexfoundry.org/display/FA/Barcelona \" Scroll to the bottom a look for the \"barcelona-docker-compose.yml\" file. Once downloaded, rename the file to \"docker-compose.yml\" Once the file is download, move the file into your desired working directory. Create a copy of the file and rename the copy \"docker-compose.yml\" Verify the version of dockerized EdgeX that you will be running With your favorite file editor, open the docker-compose.yml file Within the first couple of lines you will see the word \"Version\", next to that you will see a number - it should be \"2\". Version 2 refers to the Barcelona release Enable SNMP in the Docker Compose file With your favorite file editor, open the docker-compose file Find the section \"device-snmp\" section, which will be commented out with \"#\" symbols. Uncomment the entire section Save your changes and exit out of the editor Starting EdgeX Docker components Start EdgeX by using the following commands Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds docker-compose up -d config-seed Start and populate the configuration/registry microservice which all services must register with and get their configuration from 60 seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries 1 minute docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices 30 seconds docker-compose up -d metadata Start the Core Metadata microservice 1 minute docker-compose up -d data Start the Core Data microservice 1 minute docker-compose up -d command Start the Core Command microservice 1 minute docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices 1 minute docker-compose up -d export -client Start the Export Client registration microservice 1 minute docker-compose up -d export -distro Start the Export Distribution microservice 1 minute docker-compose up -d rulesengine Start the Rules Engine microservice 1 minute docker-compose up -d device -virtual Start the virtual device service 1 minute docker-compose up -d device -snmp Start the SNMP device service 1 minute Check the containers status Run a \"docker ps -a\" command to confirm that all the containers have been downloaded and started Show containers To get a list of all the EdgeX containers, you can use \"docker-compose config --services\" Stop Containers To stop (but not remove) all containers, issue \"docker-compose stop\". To stop an individual container, you can use \"docker-compose stop [compose-container-name]\". Start Containers To start all the containers (after a stop) issue \"docker-compose start\" to re-start To start an individual container, you can use \"docker-compose start [compose-container-name]\" (after that container has been stopped). Delete Containers * DANGER * To stop all the containers running and DELETE them, you can use \"docker-compose down\" EdgeX Foundry Container Logs To view the log of any container, use the command: \"docker-compose logs -f compose-container-name\" (ex. docker-compose logs -f edgex-device-snmp) At this point the Dockerized version of EdgeX is running. Adding the Device to EdgeX Importing APIs In this section you will be using the program Postman to interact with EdgeX. You will also need to have the file \"core-metadata.raml\" available to load into the Postman application. The file \"core-metadata.raml\" can be found here: \"edgex/core-metadata..../src/test/resources/raml/core-metadata.raml\" Viewing available APIs Open Postman Click on the Import button Add the file to the import dialog box - the application will take a about 30 seconds to digest the file you added. If a list of API commands do not show up on the left hand side of the application then click on the \"Collections\" tab to the right of the \"History\" tab. Create an addressable In the \"Collections\" tab, select the option \"POST /addressable action Open the body tab Modify its contents name: moxa-e2210-address protocol: HTTP (needs to be in ALL CAPS) address: 192.168.1.103 (IPV4 address) port: 161 (port # of snmp) path: empty (remove all text between parentheses) publisher, user, password, topic - do not need to be modified Press the \"Send\" button when you are finished Note the addressable id Upload the profile In the \"Collections\" tab select the option \"POST /deviceprofile/uploadfile Open the body tab Under \"Key\", look for the drop down menu for \"text\". Be sure to write \"file\" in the open box. Under \"Value\" click \"Choose Files\", locate your profile file. Press Upload Press the \"Send\" button when you are finished Note the profile id Post the device In the \"Collections\" tab select the option \"POST /device Click on the \"Body\" tab Modify its contents There are three components that are required to be modified. They are: \"Service\" \"Profile\" \"Addressable\" The others can be modified, however they are not required for operation name: moxa-e2210-device description: snmp smart ethernet io addressable: name: moxa-e2210-address (same as used in addressable) labels: labels: \"snmp\", \"rtu\",\"io\" (same as used in snmp device profile) service: name: edgex-device-snmp profile: name: name: moxa-iologik-e2210 (same as used in snmp device profile) Press the \"Send\" button when you are finished Note the addressable id What if a Mistake is Made Get device id Delete device id Get device profile id Delete device profile id Get addressable id Delete addressable id Verify Device Added Check the edgex-device-snmp logs to see if the device was added without issue \"sudo docker logs -f --tail 100 edgex-device-snmp\" Verify device is sending data In the \"Collections\" tab select the option \"GET /device Change the port number form \"48081\" http://localhost:48081/api/v1/device to port number \"48082\" http://localhost:48082/api/v1/device Press Send You should see something similar to { \"id\" : \"5a1d6f8ae4b0c3936013120f\" , \"name\" : \"diStatus0\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a1d7134e4b0c39360131212/command/5a1d6f8ae4b0c3936013120f\" , < -- This \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , Double click on the \"url\" and a new tab within Postman should open, Press Send If all went well you should see something similar to the following: {\"diStatus0\":\"0\"} If all did not go well the you will see an error or may \"{ }\" then you will need check the information you entered. If the data/result displayed was as expected, go ahead and proceed to creating a scheduled event Creating a Scheduled Event This is used to regularly get & push data to another service or for regularly viewing data. Gathering information for the addressable Got to http://localhost:48082/api/v1/device Look for the id or the device that you want to schedule an event for [ { \"name\" : \"moxa-e2210-device\" , \"id\" : \"5a280a0be4b0c39393ec7780\" , < --- This \"description\" : \"snmp smart ethernet io\" , \"labels\" : [ \"snmp\" , \"rtu\" , \"io\" ], \"adminState\" : \"unlocked\" , In this case the id is \"5a280a0be4b0c39393ec7780\" Next you want to get the \"name\" of the command you want to schedule an event for \"commands\" : [ { \"id\" : \"5a2808e6e4b0c39393ec777c\" , \"name\" : \"serverModel\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777c\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get server model number.\" , \"expectedValues\" : [ \"serverModel\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , { \"id\" : \"5a2808e6e4b0c39393ec777d\" , \"name\" : \"diStatus0\" , < --- This \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777d\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] In this example the name is \"diStatus0\". Create addressable In this section you will need to supply a path the the item you want to schedule. The path outline is: /api/v1/device/{device id}/{command name} In this case, the address would be / api / v1 / device / XXXX / diStatus0 / POST addressable \u201c name \u201d : \u201d schedule - moxa - di \u201d \u201c protocol \u201d : \u201c HTTP \u201d \u201c address \u201d : \u201c edgex - device - snmp \u201d \u201c port \u201d : \u201c xxxxx \u201d \u201c path \u201d : \u201d / api / v1 / device / { device id } / { command name }\u201d \u201c method \u201d : \u201c GET \u201d *** This will need to be added *** Create a schedule / POST schedule \u201c name \u201d : \u201d interval - moxa - di0 \u201d \u201c start \u201d : null ( remove parenthesis and replace ) \u201c end \u201d : null ( remove parenthesis and replace ) \u201c frequency \u201d : \u201c PT5S \u201d Create an event that will use the schedule / POST scheduleevent \u201c name \u201d : \u201c device - moxa - di0 \u201d \u201c addressable \u201d : {\u201c name \u201d : \u201d schedule - moxa - di \u201d} \u201c schedule \u201d : \u201d interval - moxa - di0 \u201d \u201c service \u201d : \u201c edgex - device - snmp \u201d *** This will need to be added ***","title":"Project Components Needed"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/","text":"Modbus - Data Type Conversion In use cases where the Device Resource uses an integer data type with a float scale, precision can be lost following transformation. For example, a Modbus device stores the temperature and humidity in an INT16 data type with a float scale of 0.01. If the temperature is 26.53, the read value is 2653. However, following transformation, the value is 26. To avoid this scenario, the device resource data type must differ from the value descriptor data type. This is achieved using the optional rawType attribute in the device profile to define the binary data read from the Modbus device, and a value type to indicate what data type the user wants to receive. If the rawType attribute exists, the Device Service parses the binary data according to the defined rawType , then casts the value according to the value type defined in the properties of the Device Resources . The following extract from a device profile defines the rawType as INT16 and the value type as FLOAT32: deviceResources : - name : \"humidity\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"1\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"%RH\" } - name : \"temperature\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"2\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"degrees Celsius\" } Read Command A Read command is executed as follows: The Device Service executes the Read command to read binary data The binary reading data is parsed as an INT16 data type The integer value is cast to a FLOAT32 value Write Command A Write command is executed as follows: The Device Service cast the requested FLOAT32 value to an integer value The integer value is converted to binary data The Device Service executes the Write command When to Transform Data You generally need to transform data when scaling readings between a 16-bit integer and a float value. The following limitations apply: rawType supports only INT16 and UINT16 data types The corresponding value type must be FLOAT32 or FLOAT64 If an unsupported data type is defined for the rawType attribute, the Device Service throws an exception similar to the following: Handler - execReadCmd: error for Device: Modbus-TCP-Device cmd: readAll, the raw type INT32 is not supported /api/v1/device/91f6430d-9268-43e3-88a6-19dbe7f98dad/readAll Supported Transformations The supported transformations are as follows: From rawType To value type INT16 FLOAT32 INT16 FLOAT64 UINT16 FLOAT32 UINT16 FLOAT64","title":"Modbus - Data Type Conversion"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#modbus-data-type-conversion","text":"In use cases where the Device Resource uses an integer data type with a float scale, precision can be lost following transformation. For example, a Modbus device stores the temperature and humidity in an INT16 data type with a float scale of 0.01. If the temperature is 26.53, the read value is 2653. However, following transformation, the value is 26. To avoid this scenario, the device resource data type must differ from the value descriptor data type. This is achieved using the optional rawType attribute in the device profile to define the binary data read from the Modbus device, and a value type to indicate what data type the user wants to receive. If the rawType attribute exists, the Device Service parses the binary data according to the defined rawType , then casts the value according to the value type defined in the properties of the Device Resources . The following extract from a device profile defines the rawType as INT16 and the value type as FLOAT32: deviceResources : - name : \"humidity\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"1\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"%RH\" } - name : \"temperature\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"2\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"degrees Celsius\" }","title":"Modbus - Data Type Conversion"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#read-command","text":"A Read command is executed as follows: The Device Service executes the Read command to read binary data The binary reading data is parsed as an INT16 data type The integer value is cast to a FLOAT32 value","title":"Read Command"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#write-command","text":"A Write command is executed as follows: The Device Service cast the requested FLOAT32 value to an integer value The integer value is converted to binary data The Device Service executes the Write command","title":"Write Command"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#when-to-transform-data","text":"You generally need to transform data when scaling readings between a 16-bit integer and a float value. The following limitations apply: rawType supports only INT16 and UINT16 data types The corresponding value type must be FLOAT32 or FLOAT64 If an unsupported data type is defined for the rawType attribute, the Device Service throws an exception similar to the following: Handler - execReadCmd: error for Device: Modbus-TCP-Device cmd: readAll, the raw type INT32 is not supported /api/v1/device/91f6430d-9268-43e3-88a6-19dbe7f98dad/readAll","title":"When to Transform Data"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#supported-transformations","text":"The supported transformations are as follows: From rawType To value type INT16 FLOAT32 INT16 FLOAT64 UINT16 FLOAT32 UINT16 FLOAT64","title":"Supported Transformations"},{"location":"examples/Ch-ExamplesProvisionDevice/","text":"Provison a Device - Modbus Example For this example we will use the GS1-10P5 Modbus motor profile we have available as reference GS1 Profile , device reference: Marathon Electric MicroMax motors via PLC ( http://www.marathon-motors.com/Inverter-Vector-Duty-C-Face-Footed-TEFC-Micromax-Motor_c333.htm )). I would recommend using a tool like Postman for simplifying interactions with the REST APIs (refer to the \"Device and Device Service Setup (aka Device Service Creation and Device Provisioning)\" section for further details at API Demo Walkthrough , all REST content is JSON). Also note that Postman is capable of importing RAML documents for API framing (RAML docs for the EdgeX services may be found in src/test/resources/raml or on the wiki). Note that this specific example can be tweaked for use with the other Device Services. Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : If IP connected, the body will look something like: { \"name\":\"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\",\"address\": \"10.0.1.29\", \"port\": 502 } If serially connected, the body will look something like: {\"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" }, \"labels\" : [], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" }, \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1. Further deep dives on the different microservices and layers can be found in our EdgeX Tech Talks series ( EdgeX Tech Talks .) where Jim and I cover some of the intricacies of various services. Of particular relevance here is the Metadata Part 2 discussion covering Device Profiles and Device Provisioning.","title":"Provison a Device - Modbus Example"},{"location":"examples/Ch-ExamplesProvisionDevice/#provison-a-device-modbus-example","text":"For this example we will use the GS1-10P5 Modbus motor profile we have available as reference GS1 Profile , device reference: Marathon Electric MicroMax motors via PLC ( http://www.marathon-motors.com/Inverter-Vector-Duty-C-Face-Footed-TEFC-Micromax-Motor_c333.htm )). I would recommend using a tool like Postman for simplifying interactions with the REST APIs (refer to the \"Device and Device Service Setup (aka Device Service Creation and Device Provisioning)\" section for further details at API Demo Walkthrough , all REST content is JSON). Also note that Postman is capable of importing RAML documents for API framing (RAML docs for the EdgeX services may be found in src/test/resources/raml or on the wiki). Note that this specific example can be tweaked for use with the other Device Services. Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : If IP connected, the body will look something like: { \"name\":\"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\",\"address\": \"10.0.1.29\", \"port\": 502 } If serially connected, the body will look something like: {\"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" }, \"labels\" : [], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" }, \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1. Further deep dives on the different microservices and layers can be found in our EdgeX Tech Talks series ( EdgeX Tech Talks .) where Jim and I cover some of the intricacies of various services. Of particular relevance here is the Metadata Part 2 discussion covering Device Profiles and Device Provisioning.","title":"Provison a Device - Modbus Example"},{"location":"examples/Ch-ExamplesRandomDeviceService/","text":"Random Integer Device Service Example The Random Integer Device Service is a sample Device Service that can run directly with other EdgeX services. It has a default, pre-defined device profile (see the device.random.yaml file), device and schedule events (see the configuration.toml file). After the EdgeX Core Service and Random Integer Device Service start, the following Core Service APIs can be viewed in the browser: Core Service API URL Description Core Metadata http://[host]:48081/api/v1/deviceservice/device-random Device Service created Core Metadata http://[host]:48081/api/v1/deviceprofile/Random-Integer-Generator Device profile created Core Metadata http://[host]:48081/api/v1/device/Random-Integer-Generator01 Device created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int16 Schedule created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int32 Schedule created Core Data http://[host]:48080/api/v1/event GenerateRandomValue_Int16 and GenerateRandomValue_Int32 called every 5 seconds to produce events and readings according to the readValue_int16 and readValue_int32 Core Command http://[host]:48082/api/v1/device The following commands are available for GET and PUT methods: - GenerateRandomValue_Int8 - GenerateRandomValue_Int16 - GenerateRandomValue_Int32 Running Commands The command execution URLs can be acquired using a Core Command API inquiry. The command URL is http://[host]:48082/api/v1/device/[device id]/command/[command id] . For example: GET Command If you replace the host and run the GET command for GenerateRandomValue_Int8, you receive an event with a random reading value between -128 and 127, as illustrated below: PUT Command PUT commands can adjust the minimum and maximum values for future random reading values, but they must be valid values for the data type. For example, the minimum value for GenerateRandomValue_Int16 cannot be less than -32768. In the following example, the PUT command limits the future reading value of GenerateRandomValue_Int8 to a range of -2 to 2: Note The parameter of the PUT command body is defined in the parameterNames field of the Command model. To validate the result, send the following GET command:","title":"Random Integer Device Service Example"},{"location":"examples/Ch-ExamplesRandomDeviceService/#random-integer-device-service-example","text":"The Random Integer Device Service is a sample Device Service that can run directly with other EdgeX services. It has a default, pre-defined device profile (see the device.random.yaml file), device and schedule events (see the configuration.toml file). After the EdgeX Core Service and Random Integer Device Service start, the following Core Service APIs can be viewed in the browser: Core Service API URL Description Core Metadata http://[host]:48081/api/v1/deviceservice/device-random Device Service created Core Metadata http://[host]:48081/api/v1/deviceprofile/Random-Integer-Generator Device profile created Core Metadata http://[host]:48081/api/v1/device/Random-Integer-Generator01 Device created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int16 Schedule created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int32 Schedule created Core Data http://[host]:48080/api/v1/event GenerateRandomValue_Int16 and GenerateRandomValue_Int32 called every 5 seconds to produce events and readings according to the readValue_int16 and readValue_int32 Core Command http://[host]:48082/api/v1/device The following commands are available for GET and PUT methods: - GenerateRandomValue_Int8 - GenerateRandomValue_Int16 - GenerateRandomValue_Int32","title":"Random Integer Device Service Example"},{"location":"examples/Ch-ExamplesRandomDeviceService/#running-commands","text":"The command execution URLs can be acquired using a Core Command API inquiry. The command URL is http://[host]:48082/api/v1/device/[device id]/command/[command id] . For example:","title":"Running Commands"},{"location":"examples/Ch-ExamplesRandomDeviceService/#get-command","text":"If you replace the host and run the GET command for GenerateRandomValue_Int8, you receive an event with a random reading value between -128 and 127, as illustrated below:","title":"GET Command"},{"location":"examples/Ch-ExamplesRandomDeviceService/#put-command","text":"PUT commands can adjust the minimum and maximum values for future random reading values, but they must be valid values for the data type. For example, the minimum value for GenerateRandomValue_Int16 cannot be less than -32768. In the following example, the PUT command limits the future reading value of GenerateRandomValue_Int8 to a range of -2 to 2: Note The parameter of the PUT command body is defined in the parameterNames field of the Command model. To validate the result, send the following GET command:","title":"PUT Command"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/","text":"Sending and Consuming Binary Data From EdgeX Device Services EdgeX - Fuji Release Overview In this example, we will demonstrate how to send EdgeX Events and Readings that contain arbitrary binary data. DeviceService Implementation Device Profile To indicate that a deviceResource represents a Binary type, the following format is used: - name : \"camera_snapshot\" description : \"snapshot from camera\" properties : value : { type : \"Binary\" , readWrite : \"R\" } units : { type : \"Binary\" , readWrite : \"R\" , defaultValue : \"CameraSnapshot\" } Device Service Here is a snippet from a hypothetical Device Service's HandleReadCommands() method that produces an event that represents a JPEG image captured from a camera: if req . DeviceResourceName == \"camera_snapshot\" { data , err : = cameraClient . GetSnapshot () // returns ( [] byte , error ) check ( err ) cv , err : = sdkModel . NewBinaryValue ( reqs [ i ] . DeviceResourceName , 0 , data ) check ( err ) responses [ i ] = cv } Calling Device Service Command Querying core-metadata for the Device's Commands and DeviceID provides the following as the URL to request a reading from the snapshot command: http://localhost:49990/api/v1/device/3469a658-c3b8-46f1-9098-7d19973af402/OnvifSnapshot Unlike with non-binary Events, making a request to this URL will return an event in CBOR representation. CBOR is a representation of binary data loosely based off of the JSON data model. This Event will not be human-readable. Parsing CBOR Encoded Events To access the data enclosed in these Events and Readings, they will first need to be decoded from CBOR. The following is a simple Go program that reads in the CBOR response from a file containing the response from the previous HTTP request. The Go library recommended for parsing these events can be found at https://github.com/ugorji/go package main import ( \u201c io / ioutil \u201d contracts \u201c github . com / edgexfoundry / go - mod - core - contracts / models \u201d \u201c github . com / ugorji / go / codec \u201d ) func check ( e error ) { if e != nil { panic ( e ) } } func main () { // Read in our cbor data fileBytes , err : = ioutil . ReadFile ( \u201c / Users / johndoe / Desktop / image . cbor \u201d ) check ( err ) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec . Handle = new ( codec . CborHandle ) var dec * codec . Decoder = codec . NewDecoderBytes ( fileBytes , h ) // Decode into an EdgeX Event var event contracts . Event err = dec . Decode ( & event ) check ( err ) // Grab binary data and write to a file imgBytes : = event . Readings [ 0 ] . BinaryValue ioutil . WriteFile ( \u201c / Users / johndoe / Desktop / image . jpeg \u201d , imgBytes , 0644 ) } In the code above, the CBOR data is read into a buffer, a code.Decoder is created to decode the CBOR data, an EdgeX Event struct is created, and a pointer is passed into the decoder's Decode() method to be filled in. Finally, the binary payload is written to a file from the BinaryValue field of the Reading. This method would work as well for decoding Events off the EdgeX message bus. Encoding Arbitrary Structures in Events The Device SDK's NewBinaryValue() function above only accepts a byte slice as binary data. Any arbitrary Go structure can be encoded in a binary reading by first encoding the structure into a byte slice using CBOR. The following illustrates this method: // DeviceService HandleReadCommands () code : foo : = struct { X int Y int Z int Bar string } { X : 7 , Y : 3 , Z : 100 , Bar : \"Hello world!\" , } buffer : = new ( bytes . Buffer ) ch : = new ( codec . CborHandle ) encoder : = codec . NewEncoder ( buffer , ch ) err = encoder . Encode ( & foo ) check ( err ) cv , err : = sdkModel . NewBinaryValue ( reqs [ i ] . DeviceResourceName , 0 , buffer . Bytes ()) responses [ i ] = cv This code takes the anonymous struct with fields X, Y, Z, and Bar (of different types) and serializes it into a byte slice using the same codec library, and passing the output to NewBinaryValue() . When consuming these events, another level of decoding will need to take place to get the structure out of the binary payload. func main () { // Read in our cbor data fileBytes , err : = ioutil . ReadFile ( \u201c / Users / johndoe / Desktop / image . cbor \u201d ) check ( err ) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec . Handle = new ( codec . CborHandle ) var dec * codec . Decoder = codec . NewDecoderBytes ( fileBytes , h ) // Decode into an EdgeX Event var event contracts . Event err = dec . Decode ( & event ) check ( err ) // Decode into arbitrary type foo : = struct { X int Y int Z int Bar string }{} dec = codec . NewDecoderBytes ( event . Readings [ 0 ]. BinaryValue , h ) err = dec . Decode ( & foo ) check ( err ) fmt . Println ( foo ) } This code takes a command response in the same format as the previous example, but uses the codec library to decode the CBOR data inside the EdgeX Reading's BinaryValue field. Using this approach, an Event can be sent containing data containing an arbitrary, flexible structure. Use cases could be a Reading containing multiple images, a variable length list of integer read-outs, etc. Creating a CBOR Payload for use with PUT Commands To create a CBOR payload that, for example, can be used with PUT commands, we first need to set up some content which will be used to create the CBOR data. Then we encode that content and finally write the CBOR-encoded data to a file, followed by using that file with an example PUT command. The relevant data structures are as follows, containing details of the key and the corresponding value, where you should note in particular the Put field in the Command struct, and below the Command struct is the Put struct itself. More details available at https://github.com/edgexfoundry/go-mod-core-contracts/blob/master/models/put.go: type Command struct { Timestamps ` yaml : \",inline\" ` Id string ` json : \"id\" yaml : \"id,omitempty\" ` // Id is a unique identifier , such as a UUID Name string ` json : \"name\" yaml : \"name,omitempty\" ` // Command name ( unique on the profile ) Get Get ` json : \"get\" yaml : \"get,omitempty\" ` // Get Command Put Put ` json : \"put\" yaml : \"put,omitempty\" ` // Put Command isValidated bool // internal member used for validation check } type Put struct { Action ` yaml : \",inline\" ` ParameterNames [] string ` json : \"parameterNames,omitempty\" yaml : \"parameterNames,omitempty\" ` } What follows below is the accompanying golang code that accomplishes the steps above: package main import ( \"io/ioutil\" \"github.com/ugorji/go/codec\" ) const ( fileLocation = \"/Users/johnpoe/Desktop/CBOR_Binary\" ) const ( enableRandomizationBinary = \"EnableRandomization_Binary\" path = \"Path\" url = \"Url\" ) func main () { // Set up some records which will be used to create the CBOR data cborContents : = make ( map [ string ] string ) // The user should put values in the cborContents variable above , which will // be converted to CBOR . Please refer to the earlier note containing details // of the key and the corresponding value each keys represent . What follows // below is an example of populating the \"ParameterNames\" ( aka \"Put\" ) cborContents [ enableRandomizationBinary ] = \"true\" cborContents [ path ] = \"/api/v1/device/9f872d68/Binary\" cborContents [ url ] = \"http://localhost:48082/api/v1/device/9f872d68/command/7ff8d51ea50d\" // Encode the contents that were set up above . input : = make ([] byte , 0 ) check ( codec . NewEncoderBytes ( & input , & codec . CborHandle {}) . Encode ( cborContents )) // Write the CBOR - encoded data to a file . ioutil . WriteFile ( fileLocation , input , 0644 ) } func check ( e error ) { if e != nil { panic ( e ) } } In the code above, as a final step, the CBOR payload has been written to the filesystem, into a file that we are calling CBOR_Binary . Here is how to use the PUT command: Via the --data-binary flag in cURL , supply as follows the CBOR-encoded file created above. You will want to replace the fileLocation (i.e. '@/Users/johnpoe/Desktop/CBOR_Binary' by a suitably-located local file on your filesystem: curl --location --request PUT 'http://localhost:48082/api/v1/device/9f872d68-2281-4af4-959d-29e4d51c2192/command/b349df4a-6c3d-4218-b8bc-7ff8d51ea50d' \\ --header 'Content-Type: application/cbor' \\ --data-binary '@/Users/johnpoe/Desktop/CBOR_Binary'","title":"Sending and Consuming Binary Data From EdgeX Device Services"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#sending-and-consuming-binary-data-from-edgex-device-services","text":"EdgeX - Fuji Release","title":"Sending and Consuming Binary Data From EdgeX Device Services"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#overview","text":"In this example, we will demonstrate how to send EdgeX Events and Readings that contain arbitrary binary data.","title":"Overview"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#deviceservice-implementation","text":"","title":"DeviceService Implementation"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#device-profile","text":"To indicate that a deviceResource represents a Binary type, the following format is used: - name : \"camera_snapshot\" description : \"snapshot from camera\" properties : value : { type : \"Binary\" , readWrite : \"R\" } units : { type : \"Binary\" , readWrite : \"R\" , defaultValue : \"CameraSnapshot\" }","title":"Device Profile"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#device-service","text":"Here is a snippet from a hypothetical Device Service's HandleReadCommands() method that produces an event that represents a JPEG image captured from a camera: if req . DeviceResourceName == \"camera_snapshot\" { data , err : = cameraClient . GetSnapshot () // returns ( [] byte , error ) check ( err ) cv , err : = sdkModel . NewBinaryValue ( reqs [ i ] . DeviceResourceName , 0 , data ) check ( err ) responses [ i ] = cv }","title":"Device Service"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#calling-device-service-command","text":"Querying core-metadata for the Device's Commands and DeviceID provides the following as the URL to request a reading from the snapshot command: http://localhost:49990/api/v1/device/3469a658-c3b8-46f1-9098-7d19973af402/OnvifSnapshot Unlike with non-binary Events, making a request to this URL will return an event in CBOR representation. CBOR is a representation of binary data loosely based off of the JSON data model. This Event will not be human-readable.","title":"Calling Device Service Command"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#parsing-cbor-encoded-events","text":"To access the data enclosed in these Events and Readings, they will first need to be decoded from CBOR. The following is a simple Go program that reads in the CBOR response from a file containing the response from the previous HTTP request. The Go library recommended for parsing these events can be found at https://github.com/ugorji/go package main import ( \u201c io / ioutil \u201d contracts \u201c github . com / edgexfoundry / go - mod - core - contracts / models \u201d \u201c github . com / ugorji / go / codec \u201d ) func check ( e error ) { if e != nil { panic ( e ) } } func main () { // Read in our cbor data fileBytes , err : = ioutil . ReadFile ( \u201c / Users / johndoe / Desktop / image . cbor \u201d ) check ( err ) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec . Handle = new ( codec . CborHandle ) var dec * codec . Decoder = codec . NewDecoderBytes ( fileBytes , h ) // Decode into an EdgeX Event var event contracts . Event err = dec . Decode ( & event ) check ( err ) // Grab binary data and write to a file imgBytes : = event . Readings [ 0 ] . BinaryValue ioutil . WriteFile ( \u201c / Users / johndoe / Desktop / image . jpeg \u201d , imgBytes , 0644 ) } In the code above, the CBOR data is read into a buffer, a code.Decoder is created to decode the CBOR data, an EdgeX Event struct is created, and a pointer is passed into the decoder's Decode() method to be filled in. Finally, the binary payload is written to a file from the BinaryValue field of the Reading. This method would work as well for decoding Events off the EdgeX message bus.","title":"Parsing CBOR Encoded Events"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#encoding-arbitrary-structures-in-events","text":"The Device SDK's NewBinaryValue() function above only accepts a byte slice as binary data. Any arbitrary Go structure can be encoded in a binary reading by first encoding the structure into a byte slice using CBOR. The following illustrates this method: // DeviceService HandleReadCommands () code : foo : = struct { X int Y int Z int Bar string } { X : 7 , Y : 3 , Z : 100 , Bar : \"Hello world!\" , } buffer : = new ( bytes . Buffer ) ch : = new ( codec . CborHandle ) encoder : = codec . NewEncoder ( buffer , ch ) err = encoder . Encode ( & foo ) check ( err ) cv , err : = sdkModel . NewBinaryValue ( reqs [ i ] . DeviceResourceName , 0 , buffer . Bytes ()) responses [ i ] = cv This code takes the anonymous struct with fields X, Y, Z, and Bar (of different types) and serializes it into a byte slice using the same codec library, and passing the output to NewBinaryValue() . When consuming these events, another level of decoding will need to take place to get the structure out of the binary payload. func main () { // Read in our cbor data fileBytes , err : = ioutil . ReadFile ( \u201c / Users / johndoe / Desktop / image . cbor \u201d ) check ( err ) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec . Handle = new ( codec . CborHandle ) var dec * codec . Decoder = codec . NewDecoderBytes ( fileBytes , h ) // Decode into an EdgeX Event var event contracts . Event err = dec . Decode ( & event ) check ( err ) // Decode into arbitrary type foo : = struct { X int Y int Z int Bar string }{} dec = codec . NewDecoderBytes ( event . Readings [ 0 ]. BinaryValue , h ) err = dec . Decode ( & foo ) check ( err ) fmt . Println ( foo ) } This code takes a command response in the same format as the previous example, but uses the codec library to decode the CBOR data inside the EdgeX Reading's BinaryValue field. Using this approach, an Event can be sent containing data containing an arbitrary, flexible structure. Use cases could be a Reading containing multiple images, a variable length list of integer read-outs, etc.","title":"Encoding Arbitrary Structures in Events"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#creating-a-cbor-payload-for-use-with-put-commands","text":"To create a CBOR payload that, for example, can be used with PUT commands, we first need to set up some content which will be used to create the CBOR data. Then we encode that content and finally write the CBOR-encoded data to a file, followed by using that file with an example PUT command. The relevant data structures are as follows, containing details of the key and the corresponding value, where you should note in particular the Put field in the Command struct, and below the Command struct is the Put struct itself. More details available at https://github.com/edgexfoundry/go-mod-core-contracts/blob/master/models/put.go: type Command struct { Timestamps ` yaml : \",inline\" ` Id string ` json : \"id\" yaml : \"id,omitempty\" ` // Id is a unique identifier , such as a UUID Name string ` json : \"name\" yaml : \"name,omitempty\" ` // Command name ( unique on the profile ) Get Get ` json : \"get\" yaml : \"get,omitempty\" ` // Get Command Put Put ` json : \"put\" yaml : \"put,omitempty\" ` // Put Command isValidated bool // internal member used for validation check } type Put struct { Action ` yaml : \",inline\" ` ParameterNames [] string ` json : \"parameterNames,omitempty\" yaml : \"parameterNames,omitempty\" ` } What follows below is the accompanying golang code that accomplishes the steps above: package main import ( \"io/ioutil\" \"github.com/ugorji/go/codec\" ) const ( fileLocation = \"/Users/johnpoe/Desktop/CBOR_Binary\" ) const ( enableRandomizationBinary = \"EnableRandomization_Binary\" path = \"Path\" url = \"Url\" ) func main () { // Set up some records which will be used to create the CBOR data cborContents : = make ( map [ string ] string ) // The user should put values in the cborContents variable above , which will // be converted to CBOR . Please refer to the earlier note containing details // of the key and the corresponding value each keys represent . What follows // below is an example of populating the \"ParameterNames\" ( aka \"Put\" ) cborContents [ enableRandomizationBinary ] = \"true\" cborContents [ path ] = \"/api/v1/device/9f872d68/Binary\" cborContents [ url ] = \"http://localhost:48082/api/v1/device/9f872d68/command/7ff8d51ea50d\" // Encode the contents that were set up above . input : = make ([] byte , 0 ) check ( codec . NewEncoderBytes ( & input , & codec . CborHandle {}) . Encode ( cborContents )) // Write the CBOR - encoded data to a file . ioutil . WriteFile ( fileLocation , input , 0644 ) } func check ( e error ) { if e != nil { panic ( e ) } } In the code above, as a final step, the CBOR payload has been written to the filesystem, into a file that we are calling CBOR_Binary . Here is how to use the PUT command: Via the --data-binary flag in cURL , supply as follows the CBOR-encoded file created above. You will want to replace the fileLocation (i.e. '@/Users/johnpoe/Desktop/CBOR_Binary' by a suitably-located local file on your filesystem: curl --location --request PUT 'http://localhost:48082/api/v1/device/9f872d68-2281-4af4-959d-29e4d51c2192/command/b349df4a-6c3d-4218-b8bc-7ff8d51ea50d' \\ --header 'Content-Type: application/cbor' \\ --data-binary '@/Users/johnpoe/Desktop/CBOR_Binary'","title":"Creating a CBOR Payload for use with PUT Commands"},{"location":"examples/Ch-ExamplesVirtualDeviceService/","text":"Using the Virtual Device Service Overview The Virtual Device Service GO can simulate different kinds of devices to generate Events and Readings to the Core Data Micro Service. Furthermore, users can send commands and get responses through the Command and Control Micro Service. The Virtual Device Service allows you to execute functional or performance tests without any real devices. This version of the Virtual Device Service is implemented based on Device SDK GO , and uses ql (an embedded SQL database engine) to simulate virtual resources. Sequence Diagram Virtual Resource Table Schema Column Type DEVICE_NAME STRING COMMAND_NAME STRING DEVICE_RESOURCE_NAME STRING ENABLE_RANDOMIZATION BOOL DATA_TYPE STRING VALUE STRING How to Use The Virtual Device Service depends on the EdgeX Core Services. If you're going to download the source code and run the Virtual Device Service in dev mode, make sure that the EdgeX Core Services are up before starting the Virtual Device Service. The Virtual Device Service currently contains four pre-defined devices (see the configuration.toml ) as random value generators: Device Name Device Profile Random-Boolean-Device device.virtual.bool.yaml Random-Float-Device device.virtual.float.yaml Random-Integer-Device device.virtual.int.yaml Random-UnsignedInteger-Device device.virtual.uint.yaml Restricted: To control the randomization of device resource values, it has to add additional device resources with the prefix \"EnableRandomization_\" for each device resource. (Need to do the same for device commands and core commands) Please find the above default device profiles for example. Acquire the executable commands information by inquiring the Core Command API: http://[host]:48082/api/v1/device/name/Random-Boolean-Device http://[host]:48082/api/v1/device/name/Random-Integer-Device http://[host]:48082/api/v1/device/name/Random-UnsignedInteger-Device http://[host]:48082/api/v1/device/name/Random-Float-Device GET command example curl -X GET localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 ` { \"device\" : \"Random-Integer-Device\" , \"origin\" : 1574325994604494491 , \"readings\" : [ { \"origin\" : 1574325994572380549 , \"device\" : \"Random-Integer-Device\" , \"name\" : \"Int8\" , \"value\" : \"42\" } ], \"EncodedEvent\" : null } PUT command example - Assign a value to a resource The value must be a valid value for the data type. For example, the minimum value of Int8 cannot be less than -128 and the maximum value cannot be greater than 127. curl -X PUT -d '{\"Int8\": \"123\"}' \\ localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 PUT command example - Enable/Disable the randomization of the resource curl -X PUT -d '{\"EnableRandomization_Int8\": \"false\"}' \\ localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 Note The value of the resource's EnableRandomization property is simultaneously updated to false when sending a put command to assign a specified value to the resource The minimum and maximum values of the resource can be defined in the property value field of the Device Resource model, for example: deviceResources: - name: \"Int8\" description: \"Generate random int8 value\" properties: value: { type: \"Int8\", readWrite: \"R\", minimum: \"-100\", maximum: \"100\", defaultValue: \"0\" } units: { type: \"String\", readWrite: \"R\", defaultValue: \"random int8 value\" } Manipulate Virtual Resources Using the command ql Tool Install command ql If the Virtual Device Service runs in a Docker container, it must mount the directory (/db) that contains the ql database in the container. For example: device-virtual : image : edgexfoundry/docker-device-virtual-go:1.1.0 ports : - \"49990:49990\" container_name : device-virtual hostname : device-virtual networks : - edgex-network volumes : - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - /mnt/hgfs/EdgeX/DeviceVirtualDB:/db # Mount ql database directory depends_on : - data - command If the Virtual Device Service runs in dev mode, the ql database directory is under the driver directory Command examples: Query all data: ql -db /path-to-the-ql-db-folder/deviceVirtual.db -fld \"select * from VIRTUAL_RESOURCE\" Update Enable_Randomization: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set ENABLE_RANDOMIZATION=false where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \" Update Value: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set VALUE=\" 26 \" where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \"","title":"Using the Virtual Device Service"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#using-the-virtual-device-service","text":"","title":"Using the Virtual Device Service"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#overview","text":"The Virtual Device Service GO can simulate different kinds of devices to generate Events and Readings to the Core Data Micro Service. Furthermore, users can send commands and get responses through the Command and Control Micro Service. The Virtual Device Service allows you to execute functional or performance tests without any real devices. This version of the Virtual Device Service is implemented based on Device SDK GO , and uses ql (an embedded SQL database engine) to simulate virtual resources.","title":"Overview"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#sequence-diagram","text":"","title":"Sequence Diagram"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#virtual-resource-table-schema","text":"Column Type DEVICE_NAME STRING COMMAND_NAME STRING DEVICE_RESOURCE_NAME STRING ENABLE_RANDOMIZATION BOOL DATA_TYPE STRING VALUE STRING","title":"Virtual Resource Table Schema"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#how-to-use","text":"The Virtual Device Service depends on the EdgeX Core Services. If you're going to download the source code and run the Virtual Device Service in dev mode, make sure that the EdgeX Core Services are up before starting the Virtual Device Service. The Virtual Device Service currently contains four pre-defined devices (see the configuration.toml ) as random value generators: Device Name Device Profile Random-Boolean-Device device.virtual.bool.yaml Random-Float-Device device.virtual.float.yaml Random-Integer-Device device.virtual.int.yaml Random-UnsignedInteger-Device device.virtual.uint.yaml Restricted: To control the randomization of device resource values, it has to add additional device resources with the prefix \"EnableRandomization_\" for each device resource. (Need to do the same for device commands and core commands) Please find the above default device profiles for example. Acquire the executable commands information by inquiring the Core Command API: http://[host]:48082/api/v1/device/name/Random-Boolean-Device http://[host]:48082/api/v1/device/name/Random-Integer-Device http://[host]:48082/api/v1/device/name/Random-UnsignedInteger-Device http://[host]:48082/api/v1/device/name/Random-Float-Device","title":"How to Use"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#get-command-example","text":"curl -X GET localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 ` { \"device\" : \"Random-Integer-Device\" , \"origin\" : 1574325994604494491 , \"readings\" : [ { \"origin\" : 1574325994572380549 , \"device\" : \"Random-Integer-Device\" , \"name\" : \"Int8\" , \"value\" : \"42\" } ], \"EncodedEvent\" : null }","title":"GET command example"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#put-command-example-assign-a-value-to-a-resource","text":"The value must be a valid value for the data type. For example, the minimum value of Int8 cannot be less than -128 and the maximum value cannot be greater than 127. curl -X PUT -d '{\"Int8\": \"123\"}' \\ localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4","title":"PUT command example - Assign a value to a resource"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#put-command-example-enabledisable-the-randomization-of-the-resource","text":"curl -X PUT -d '{\"EnableRandomization_Int8\": \"false\"}' \\ localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 Note The value of the resource's EnableRandomization property is simultaneously updated to false when sending a put command to assign a specified value to the resource The minimum and maximum values of the resource can be defined in the property value field of the Device Resource model, for example: deviceResources: - name: \"Int8\" description: \"Generate random int8 value\" properties: value: { type: \"Int8\", readWrite: \"R\", minimum: \"-100\", maximum: \"100\", defaultValue: \"0\" } units: { type: \"String\", readWrite: \"R\", defaultValue: \"random int8 value\" }","title":"PUT command example - Enable/Disable the randomization of the resource"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#manipulate-virtual-resources-using-the-command-ql-tool","text":"Install command ql If the Virtual Device Service runs in a Docker container, it must mount the directory (/db) that contains the ql database in the container. For example: device-virtual : image : edgexfoundry/docker-device-virtual-go:1.1.0 ports : - \"49990:49990\" container_name : device-virtual hostname : device-virtual networks : - edgex-network volumes : - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - /mnt/hgfs/EdgeX/DeviceVirtualDB:/db # Mount ql database directory depends_on : - data - command If the Virtual Device Service runs in dev mode, the ql database directory is under the driver directory Command examples: Query all data: ql -db /path-to-the-ql-db-folder/deviceVirtual.db -fld \"select * from VIRTUAL_RESOURCE\" Update Enable_Randomization: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set ENABLE_RANDOMIZATION=false where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \" Update Value: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set VALUE=\" 26 \" where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \"","title":"Manipulate Virtual Resources Using the command ql Tool"},{"location":"getting-started/","text":"Getting Started To get started you need to obtain EdgeX Foundry either as a User or as a Developer/Contributor. User If you simply want to obtain the EdgeX platform and run it (but do not intend to modify or add to the existing code base at this time) then you are considered a \"User\". You will want to follow the Getting Started Users guide. The Getting Started Users guide will take you through the process of getting the latest release EdgeX Docker Containers from Docker Hub. If you wish to get the latest EdgeX containers (those built from the current ongoing development efforts prior to release), then see Getting Started Users - Nexus . Warning Containers used from Nexus are considered \"work in progress\". There is no guarantee that these containers will function properly or function properly with other containers from the current release. Contributor/Developer If you want to modify, add to or at least build the existing EdgeX code base, then you are considered a \"Developer\". \"Contributors\" are developers that further wish to contribute their code back into the EdgeX open source effort. You will want to follow the Getting Started for Developers guide. Hybrid See Getting Started Hybrid if you are developing or working on a particular micro service, but want to run the other micro services via Docker Containers. When working on something like an analytics service (as a developer or contributor) you may not wish to download, build and run all the EdgeX code - you only want to work with the code of your service. Your new service may still need to communicate with other services while you test your new service. Unless you want to get and build all the services, developers will often get and run the containers for the other EdgeX micro services and run only their service natively in a development environment. The EdgeX community refers to this as Hybrid development. Device Service Developer As a developer, if you intend to connect IoT objects (device, sensor or other \"thing\") that are not currently connected to EdgeX Foundry, you may also want to obtain the Device Service Software Development Kit (DS SDK) and create new device services. The DS SDK creates all the scaffolding code for a new EdgeX Foundry device service; allowing you to focus on the details of interfacing with the device in its native protocol. See Getting Started with Device SDK for help on using the DS SDK to create a new device service. Learn more about Device Services and the Device Service SDK at Device Services . Application Service Developer As a developer, if you intend to get EdgeX sensor data to external systems (be that an enterprise application, on-prem server or Cloud platform like Azure IoT Hub, AWS IoT, Google Cloud IOT, etc.), you will likely want to obtain the Application Functions SDK (App Func SDK) and create new application services. The App Func SDK creates all the scaffolding code for a new EdgeX Foundry application service; allowing you to focus on the details of data transformation, filtering, and otherwise prepare the sensor data for the external endpoint. Learn more about Application Services and the Application Functions SDK at Application Services .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"To get started you need to obtain EdgeX Foundry either as a User or as a Developer/Contributor. User If you simply want to obtain the EdgeX platform and run it (but do not intend to modify or add to the existing code base at this time) then you are considered a \"User\". You will want to follow the Getting Started Users guide. The Getting Started Users guide will take you through the process of getting the latest release EdgeX Docker Containers from Docker Hub. If you wish to get the latest EdgeX containers (those built from the current ongoing development efforts prior to release), then see Getting Started Users - Nexus . Warning Containers used from Nexus are considered \"work in progress\". There is no guarantee that these containers will function properly or function properly with other containers from the current release. Contributor/Developer If you want to modify, add to or at least build the existing EdgeX code base, then you are considered a \"Developer\". \"Contributors\" are developers that further wish to contribute their code back into the EdgeX open source effort. You will want to follow the Getting Started for Developers guide. Hybrid See Getting Started Hybrid if you are developing or working on a particular micro service, but want to run the other micro services via Docker Containers. When working on something like an analytics service (as a developer or contributor) you may not wish to download, build and run all the EdgeX code - you only want to work with the code of your service. Your new service may still need to communicate with other services while you test your new service. Unless you want to get and build all the services, developers will often get and run the containers for the other EdgeX micro services and run only their service natively in a development environment. The EdgeX community refers to this as Hybrid development. Device Service Developer As a developer, if you intend to connect IoT objects (device, sensor or other \"thing\") that are not currently connected to EdgeX Foundry, you may also want to obtain the Device Service Software Development Kit (DS SDK) and create new device services. The DS SDK creates all the scaffolding code for a new EdgeX Foundry device service; allowing you to focus on the details of interfacing with the device in its native protocol. See Getting Started with Device SDK for help on using the DS SDK to create a new device service. Learn more about Device Services and the Device Service SDK at Device Services . Application Service Developer As a developer, if you intend to get EdgeX sensor data to external systems (be that an enterprise application, on-prem server or Cloud platform like Azure IoT Hub, AWS IoT, Google Cloud IOT, etc.), you will likely want to obtain the Application Functions SDK (App Func SDK) and create new application services. The App Func SDK creates all the scaffolding code for a new EdgeX Foundry application service; allowing you to focus on the details of data transformation, filtering, and otherwise prepare the sensor data for the external endpoint. Learn more about Application Services and the Application Functions SDK at Application Services .","title":"Getting Started"},{"location":"getting-started/Ch-GettingStartedDevelopers/","text":"Getting Started - Developers Introduction These instructions are for Developers and Contributors to obtain and run EdgeX Foundry. (Users should read: Getting Started Users ) EdgeX Foundry is a collection of more than a dozen microservices that can be deployed to provide a minimal edge platform capability. EdgeX Foundry consists of a collection of microservices and SDK tools. The microservices and SDKs are mostly written in Go or C with some legacy services written in Java (EdgeX was originally written in Java). These documentation pages provide a developer with the information and instructions to get and run EdgeX Foundry in development mode - that is running natively outside of containers and with the intent of adding to or changing the existing code base. What You Need Hardware EdgeX Foundry is an operating system (OS)-agnostic and hardware (HW)-agnostic edge software platform. Minimum platform requirements are being established. At this time use the following recommended characteristics: Memory: minimum of 1 GB Hard drive space: minimum of 3 GB of space to run the EdgeX Foundry containers, but you may want more depending on how long sensor and device data is retained OS: EdgeX Foundry has been run successfully on many systems including, but not limited to the following systems Windows (ver 7 - 10) Ubuntu Desktop (ver 14-16) Ubuntu Server (ver 14) Ubuntu Core (ver 16) Mac OS X 10 Software Developers will need to install the following software in order to get, run and develop EdgeX Foundry microservices: git - a free and open source version control (SVC) system used to download (and upload) the EdgeX Foundry source code from the project's GitHub repository. See https://git-scm.com/downloads for download and install instructions. Alternative tools (Easy Git for example) could be used, but this document assumes use of git and leaves how to use alternative SVC tools to the reader. MongoDB - by default, EdgeX Foundry uses MongoDB (version 4.2 as of this writing) as the persistence mechanism for sensor data as well as metadata about the devices/sensors that are connected. See https://www.mongodb.com/download-center?jmp=nav#community for download and installation instructions. Redis - is an alternate open source (BSD Licensed) database that can be used with EdgeX in place of MongoDB for many services. Starting with the Geneva release, Redis will be the default EdgeX persistence mechanism for sensor, metadata, etc. EdgeX works with Redis 5.0 as of this writing. See https://redis.io/ for download and installation instructions. ZeroMQ ZeroMQ - several EdgeX Foundry services depend on ZeroMQ for communications by default. The easiest way to get and install ZeroMQ on Linux is to use this setup script: https://gist.github.com/katopz/8b766a5cb0ca96c816658e9407e83d00 . Do note that the script assumes bash is available on your system and the bash executable is located in /usr/bin. Before running the script at the link, run which bash at your Linux terminal to insure that bash is located in /usr/bin. If not, change the first line of the script so that it points to the correct location of bash. For macOS, use brew to install ZeroMQ. brew install zeromq For directions installing ZeroMQ on Windows, please see the Windows documentation: https://github.com/edgexfoundry/edgex-go/blob/master/ZMQWindows.md","title":"Getting Started - Developers"},{"location":"getting-started/Ch-GettingStartedDevelopers/#getting-started-developers","text":"","title":"Getting Started - Developers"},{"location":"getting-started/Ch-GettingStartedDevelopers/#introduction","text":"These instructions are for Developers and Contributors to obtain and run EdgeX Foundry. (Users should read: Getting Started Users ) EdgeX Foundry is a collection of more than a dozen microservices that can be deployed to provide a minimal edge platform capability. EdgeX Foundry consists of a collection of microservices and SDK tools. The microservices and SDKs are mostly written in Go or C with some legacy services written in Java (EdgeX was originally written in Java). These documentation pages provide a developer with the information and instructions to get and run EdgeX Foundry in development mode - that is running natively outside of containers and with the intent of adding to or changing the existing code base.","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedDevelopers/#what-you-need","text":"Hardware EdgeX Foundry is an operating system (OS)-agnostic and hardware (HW)-agnostic edge software platform. Minimum platform requirements are being established. At this time use the following recommended characteristics: Memory: minimum of 1 GB Hard drive space: minimum of 3 GB of space to run the EdgeX Foundry containers, but you may want more depending on how long sensor and device data is retained OS: EdgeX Foundry has been run successfully on many systems including, but not limited to the following systems Windows (ver 7 - 10) Ubuntu Desktop (ver 14-16) Ubuntu Server (ver 14) Ubuntu Core (ver 16) Mac OS X 10 Software Developers will need to install the following software in order to get, run and develop EdgeX Foundry microservices: git - a free and open source version control (SVC) system used to download (and upload) the EdgeX Foundry source code from the project's GitHub repository. See https://git-scm.com/downloads for download and install instructions. Alternative tools (Easy Git for example) could be used, but this document assumes use of git and leaves how to use alternative SVC tools to the reader. MongoDB - by default, EdgeX Foundry uses MongoDB (version 4.2 as of this writing) as the persistence mechanism for sensor data as well as metadata about the devices/sensors that are connected. See https://www.mongodb.com/download-center?jmp=nav#community for download and installation instructions. Redis - is an alternate open source (BSD Licensed) database that can be used with EdgeX in place of MongoDB for many services. Starting with the Geneva release, Redis will be the default EdgeX persistence mechanism for sensor, metadata, etc. EdgeX works with Redis 5.0 as of this writing. See https://redis.io/ for download and installation instructions. ZeroMQ ZeroMQ - several EdgeX Foundry services depend on ZeroMQ for communications by default. The easiest way to get and install ZeroMQ on Linux is to use this setup script: https://gist.github.com/katopz/8b766a5cb0ca96c816658e9407e83d00 . Do note that the script assumes bash is available on your system and the bash executable is located in /usr/bin. Before running the script at the link, run which bash at your Linux terminal to insure that bash is located in /usr/bin. If not, change the first line of the script so that it points to the correct location of bash. For macOS, use brew to install ZeroMQ. brew install zeromq For directions installing ZeroMQ on Windows, please see the Windows documentation: https://github.com/edgexfoundry/edgex-go/blob/master/ZMQWindows.md","title":"What You Need"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/","text":"Get EdgeX Foundry - Go Developers Introduction These instructions are for Go Lang Developers and Contributors to obtain, run and otherwise work with Go-based EdgeX Foundry microservices. (Users should read: Getting Started Users ) What You Need For Go Development In additional to the hardware and software listed in the Developers page, you will need the following to work with the EdgeX Go-based microservices. Go The open sourced microservices of EdgeX Foundry are written in Go 1.12. See https://golang.org/dl/ for download and installation instructions. Newer versions of Go are available and may work (as of this writing - although Go version 1.13 will work as well), but the project has not thoroughly built and tested to these newer versions of the language. Older versions of Go, especially 1.10 or older, are likely to cause issues (EdgeX now uses Go Modules which were introduced with Go Lang 1.11). For the purposes of this guide, create a set of directories for your Go work (including a /bin and /src folder) and set the GOPATH env variable to that directory. This will assist in getting your environment setup and makes it easier to understand how to find code, tools and packages. Specifically, and as shown below, create a 'go' folder with /src/github.com/edgexfoundry sub-folders. When you pull (see below) EdgeX code in from its repositories, add the repository folder under /edgexfoundry (in this example, edgex-go has been downloaded and its repository contents would comprise a new subfolder under /src/github.com/edgexfoundry). ~/go /bin /src /github.com /edgexfoundry /edgex-go In this case, your $GOPATH environment variable should be set to ~/go. Note - with the advent of modules (Go 1.11 or later), you can setup your environment differently - and specifically placing code outside of your GOPATH to avoid conflicts with solutions that are not using go modules. See https://medium.com/mindorks/create-projects-independent-of-gopath-using-go-modules-802260cdfb51 for more details. For simplicity, this guide adheres to the folders and GOPATH approach. #TODO - refactor this section or add info on how to setup independent of GOPATH EdgeX relies on many modules (some external and some EdgeX created). This directory structure allows the modules to be automatically pulled and deposited into the appropriate place in this directory tree - thereby supporting the build processes. An IDE There are many options for writing Go Lang code and one could use a simple text editor. This guide demonstrates how to get started with JetBrains Go Land. Go Land - Go Land is a popular although subscription-fee based IDE. Learn how to purchase and download Go Land here: https://www.jetbrains.com/go/ . Visual Studio Code (Optional) - As an alternative to Go Land, Visual Studio Code is a free, open source IDE developed by Microsoft. Find and download Visual Studio Code here: https://code.visualstudio.com/ . This guide does not demonstrate how to use Visual Studio Code but the steps would be very similar. Atom IDE (Optional) - As another alternative to Go Land, Atom is a free, open source IDE used with many languages. Find and download Atom here: https://ide.atom.io/ . This guide does not demonstrate how to use Atom but the steps would be very similar. Get the code This portion of the documentation assumes you wish to get and work with many \"core\" or widely used EdgeX Foundry Go-based services. This includes but is not limited to Core, Supporting, Export (archived as of the Geneva release), some security, and system management services. To work with other Go-based security services, device services, application services, SDKs, user interface, or other service you may need to pull in the other repository code. See other getting started guides for working with other Go-based services. As you will see below, you do not need to explicitly pull in dependency modules (whether EdgeX or 3rd party provided). Dependencies will automatically be pulled through the building process. To work with the core services, you will need to download the source code from the EdgeX Go repository. The EdgeX Go-based microservices are all available in a single GitHub repository download (note: the Go-based services do use a number of modules but again these dependencies will be pulled automatically for you as part of the build process). Once the code is pulled, the Go lang microservices are built and packaged as platform dependent executables and then also containerized for end user deployment/use. The EdgeX Foundry Go lang microservice code is hosted at https://github.com/edgexfoundry/edgex-go . To download or \"pull\" the EdgeX Go code, first change directories to the location where you want to download the code, and then use your git tool and request to clone this repository with the following command: git clone <https://github.com/edgexfoundry/edgex-go.git> Again, it is recommended that you clone the edgex-go mono repo to the \\$GOPATH/src/github.com/edgexfoundry folder. You will find that the project tools and configuration provided by the EdgeX community will work much better when applied to this structure. Note , if you plan to contribute code back to the EdgeX project (as a Contributor), you are going to want to fork the repositories you plan to work with and then pull your fork versus the EdgeX repositories directly. This documentation does not address the process and procedures for working with an EdgeX fork, committing changes and submitting contribution pull requests (PRs). See some of the links below in the EdgeX Wiki for assistance on how to fork and contribute EdgeX code. https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide+-+Go+Lang https://wiki.edgexfoundry.org/display/FA/Contributor+Process?searchId=AW768BAW7 Build EdgeX Foundry To build the existing services found in edgex-go, first change directories to the root of the edgex-go code cd \\$ GOPATH/src/github.com/edgexfoundry/edgex-go Second, use the provided Makefile to build all the services in a single call make build The first time EdgeX builds, it will take longer than subsequent builds as it has to download all dependencies. Depending on the size of your host, an initial build can take several minutes. Make sure the build completes successfully and has no errors. If it does build successfully, you should find new service executables in each of the service folders under \\$GOPATH/src/github.com/edgexfoundry/edgex-go/cmd/. Run EdgeX Foundry Run the Database Several of the EdgeX Foundry microservices utilize the MongoDB instance. This includes core-data, core-metadata, support-rulesengine, supporting-logging (in some situations), among others. Therefore, when working with EdgeX Foundry its a good idea to have the database up and running as a general rule. See https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/#run-mongodb-community-edition for how to run Mongo in a Linux environment (or find similar documentation for other environments). Run the core Go services With the services built successfully, and the database up and running, you can now run all the services via second make command. Simply call make run This will start all of the EdgeX go services and leave them running until you terminate the process (with Ctrl-C). The log entries from each service will start to display in the terminal. Watch the log entries for any ERROR indicators. While the EdgeX services are running you can make EdgeX API calls to localhost . Verify that EdgeX is working You can check that the microservices are working properly by calling their ping API endpoint: curl <http://localhost:48080/api/v1/ping> You should receive a pong message in response. To stop all the services, hit Control-C in the terminal. No sensor data will be collected as this just gets the core services up and running. To get sensor data flowing into EdgeX core services, you will need to get, build and run an EdgeX device service in a similar fashion. A virtual device service has been provided to test and experiment with ( https://github.com/edgexfoundry/device-virtual-go ). EdgeX Foundry in Go Land IDEs offer many code editing conveniences. Go Land was specifically built to edit and work with Go code. So if you are doing any significant code work with the EdgeX Go microservices, you will likely find it convenient to edit, build, run, test, etc. from GoLand. Import the Project The EdgeX Foundry Go mono repo contains the Go Lang source code (and many additional files such as configuration and Docker files) for all the Go-based microservices and supporting packages. As its name implies, it is the single (or mono) repository for all EdgeX Go source. To bring in the mono repo into Go Land, use the File \u2192 Open... menu option in Go Land to open the Open File or Project Window. In the \"Open File or Project\" popup, select the location of the folder containing your cloned edgex-go repo. If you are following the EdgeX recommended standards, the edgex-go folder should be located under \\$GOPATH/src/github.com/edgexfoundry/edgex-go. Open the Terminal From the View menu in Go Land, select the Terminal menu option. This will open a command terminal from which you can issue commands to install the dependencies, build the microservices, run the microservices, etc. Build the EdgeX Microservices With all the dependencies now loaded, you can build the EdgeX Foundry microservices. Run \"make build\" in the Terminal view (as shown below) to build the services. This can take a few minutes to build all the services. NOTE - in some cases, Go Land IDE may encounter an error (go: parsing \\$GOFLAGS: non-flag \"\"-X\") when building as shown below. If you encounter this issue, unset the GOFLAGS env var in GoLand. Make a call to unset GOFLAGS as shown below and then call make build again. Just as when running make build from the command line in a terminal, the microservice executables that get built in Go Land's terminal will be created under the $GOPATH/src/github.com/edgexfoundry/edgex-go/cmd/[microservice folder]/[microservice name] . So, for example, core-data would get created as the $GOPATH/src/github.com/edgexfoundry/edgex-go/cmd/core-data/core-data . Run EdgeX With all the microservices built, you can now run EdgeX. You may first want to make sure the database is running. Then issue the command make run as shown below. You can now call on the service APIs to make sure they are running correctly. Namely, call on localhost:[service port]/api/v1/ping to see each service respond to the simplest of requests.","title":"Get EdgeX Foundry - Go Developers"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#get-edgex-foundry-go-developers","text":"","title":"Get EdgeX Foundry - Go Developers"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#introduction","text":"These instructions are for Go Lang Developers and Contributors to obtain, run and otherwise work with Go-based EdgeX Foundry microservices. (Users should read: Getting Started Users )","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#what-you-need-for-go-development","text":"In additional to the hardware and software listed in the Developers page, you will need the following to work with the EdgeX Go-based microservices. Go The open sourced microservices of EdgeX Foundry are written in Go 1.12. See https://golang.org/dl/ for download and installation instructions. Newer versions of Go are available and may work (as of this writing - although Go version 1.13 will work as well), but the project has not thoroughly built and tested to these newer versions of the language. Older versions of Go, especially 1.10 or older, are likely to cause issues (EdgeX now uses Go Modules which were introduced with Go Lang 1.11). For the purposes of this guide, create a set of directories for your Go work (including a /bin and /src folder) and set the GOPATH env variable to that directory. This will assist in getting your environment setup and makes it easier to understand how to find code, tools and packages. Specifically, and as shown below, create a 'go' folder with /src/github.com/edgexfoundry sub-folders. When you pull (see below) EdgeX code in from its repositories, add the repository folder under /edgexfoundry (in this example, edgex-go has been downloaded and its repository contents would comprise a new subfolder under /src/github.com/edgexfoundry). ~/go /bin /src /github.com /edgexfoundry /edgex-go In this case, your $GOPATH environment variable should be set to ~/go. Note - with the advent of modules (Go 1.11 or later), you can setup your environment differently - and specifically placing code outside of your GOPATH to avoid conflicts with solutions that are not using go modules. See https://medium.com/mindorks/create-projects-independent-of-gopath-using-go-modules-802260cdfb51 for more details. For simplicity, this guide adheres to the folders and GOPATH approach. #TODO - refactor this section or add info on how to setup independent of GOPATH EdgeX relies on many modules (some external and some EdgeX created). This directory structure allows the modules to be automatically pulled and deposited into the appropriate place in this directory tree - thereby supporting the build processes. An IDE There are many options for writing Go Lang code and one could use a simple text editor. This guide demonstrates how to get started with JetBrains Go Land. Go Land - Go Land is a popular although subscription-fee based IDE. Learn how to purchase and download Go Land here: https://www.jetbrains.com/go/ . Visual Studio Code (Optional) - As an alternative to Go Land, Visual Studio Code is a free, open source IDE developed by Microsoft. Find and download Visual Studio Code here: https://code.visualstudio.com/ . This guide does not demonstrate how to use Visual Studio Code but the steps would be very similar. Atom IDE (Optional) - As another alternative to Go Land, Atom is a free, open source IDE used with many languages. Find and download Atom here: https://ide.atom.io/ . This guide does not demonstrate how to use Atom but the steps would be very similar.","title":"What You Need For Go Development"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#get-the-code","text":"This portion of the documentation assumes you wish to get and work with many \"core\" or widely used EdgeX Foundry Go-based services. This includes but is not limited to Core, Supporting, Export (archived as of the Geneva release), some security, and system management services. To work with other Go-based security services, device services, application services, SDKs, user interface, or other service you may need to pull in the other repository code. See other getting started guides for working with other Go-based services. As you will see below, you do not need to explicitly pull in dependency modules (whether EdgeX or 3rd party provided). Dependencies will automatically be pulled through the building process. To work with the core services, you will need to download the source code from the EdgeX Go repository. The EdgeX Go-based microservices are all available in a single GitHub repository download (note: the Go-based services do use a number of modules but again these dependencies will be pulled automatically for you as part of the build process). Once the code is pulled, the Go lang microservices are built and packaged as platform dependent executables and then also containerized for end user deployment/use. The EdgeX Foundry Go lang microservice code is hosted at https://github.com/edgexfoundry/edgex-go . To download or \"pull\" the EdgeX Go code, first change directories to the location where you want to download the code, and then use your git tool and request to clone this repository with the following command: git clone <https://github.com/edgexfoundry/edgex-go.git> Again, it is recommended that you clone the edgex-go mono repo to the \\$GOPATH/src/github.com/edgexfoundry folder. You will find that the project tools and configuration provided by the EdgeX community will work much better when applied to this structure. Note , if you plan to contribute code back to the EdgeX project (as a Contributor), you are going to want to fork the repositories you plan to work with and then pull your fork versus the EdgeX repositories directly. This documentation does not address the process and procedures for working with an EdgeX fork, committing changes and submitting contribution pull requests (PRs). See some of the links below in the EdgeX Wiki for assistance on how to fork and contribute EdgeX code. https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide+-+Go+Lang https://wiki.edgexfoundry.org/display/FA/Contributor+Process?searchId=AW768BAW7","title":"Get the code"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#build-edgex-foundry","text":"To build the existing services found in edgex-go, first change directories to the root of the edgex-go code cd \\$ GOPATH/src/github.com/edgexfoundry/edgex-go Second, use the provided Makefile to build all the services in a single call make build The first time EdgeX builds, it will take longer than subsequent builds as it has to download all dependencies. Depending on the size of your host, an initial build can take several minutes. Make sure the build completes successfully and has no errors. If it does build successfully, you should find new service executables in each of the service folders under \\$GOPATH/src/github.com/edgexfoundry/edgex-go/cmd/.","title":"Build EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#run-edgex-foundry","text":"Run the Database Several of the EdgeX Foundry microservices utilize the MongoDB instance. This includes core-data, core-metadata, support-rulesengine, supporting-logging (in some situations), among others. Therefore, when working with EdgeX Foundry its a good idea to have the database up and running as a general rule. See https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/#run-mongodb-community-edition for how to run Mongo in a Linux environment (or find similar documentation for other environments). Run the core Go services With the services built successfully, and the database up and running, you can now run all the services via second make command. Simply call make run This will start all of the EdgeX go services and leave them running until you terminate the process (with Ctrl-C). The log entries from each service will start to display in the terminal. Watch the log entries for any ERROR indicators. While the EdgeX services are running you can make EdgeX API calls to localhost . Verify that EdgeX is working You can check that the microservices are working properly by calling their ping API endpoint: curl <http://localhost:48080/api/v1/ping> You should receive a pong message in response. To stop all the services, hit Control-C in the terminal. No sensor data will be collected as this just gets the core services up and running. To get sensor data flowing into EdgeX core services, you will need to get, build and run an EdgeX device service in a similar fashion. A virtual device service has been provided to test and experiment with ( https://github.com/edgexfoundry/device-virtual-go ).","title":"Run EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#edgex-foundry-in-go-land","text":"IDEs offer many code editing conveniences. Go Land was specifically built to edit and work with Go code. So if you are doing any significant code work with the EdgeX Go microservices, you will likely find it convenient to edit, build, run, test, etc. from GoLand. Import the Project The EdgeX Foundry Go mono repo contains the Go Lang source code (and many additional files such as configuration and Docker files) for all the Go-based microservices and supporting packages. As its name implies, it is the single (or mono) repository for all EdgeX Go source. To bring in the mono repo into Go Land, use the File \u2192 Open... menu option in Go Land to open the Open File or Project Window. In the \"Open File or Project\" popup, select the location of the folder containing your cloned edgex-go repo. If you are following the EdgeX recommended standards, the edgex-go folder should be located under \\$GOPATH/src/github.com/edgexfoundry/edgex-go. Open the Terminal From the View menu in Go Land, select the Terminal menu option. This will open a command terminal from which you can issue commands to install the dependencies, build the microservices, run the microservices, etc. Build the EdgeX Microservices With all the dependencies now loaded, you can build the EdgeX Foundry microservices. Run \"make build\" in the Terminal view (as shown below) to build the services. This can take a few minutes to build all the services. NOTE - in some cases, Go Land IDE may encounter an error (go: parsing \\$GOFLAGS: non-flag \"\"-X\") when building as shown below. If you encounter this issue, unset the GOFLAGS env var in GoLand. Make a call to unset GOFLAGS as shown below and then call make build again. Just as when running make build from the command line in a terminal, the microservice executables that get built in Go Land's terminal will be created under the $GOPATH/src/github.com/edgexfoundry/edgex-go/cmd/[microservice folder]/[microservice name] . So, for example, core-data would get created as the $GOPATH/src/github.com/edgexfoundry/edgex-go/cmd/core-data/core-data . Run EdgeX With all the microservices built, you can now run EdgeX. You may first want to make sure the database is running. Then issue the command make run as shown below. You can now call on the service APIs to make sure they are running correctly. Namely, call on localhost:[service port]/api/v1/ping to see each service respond to the simplest of requests.","title":"EdgeX Foundry in Go Land"},{"location":"getting-started/Ch-GettingStartedHybrid/","text":"Working in a Hybrid Environment In some cases, as a developer, you want to work on a particular micro service, but you don't want to have to download all the source code for all the micro services and run all the micro services in the development tool(s) like Go Land. In this case, you can download and run the EdgeX Docker containers for all the micro services you need and run your single micro service (the one you are presumably working on) from the developer tool of choice and have it point to the other micro services by appropriate address. Within EdgeX, we call this a \"hybrid\" environment - where part of your EdgeX platform is running from a development environment, while other parts are running from the Dockerized containers. This page outlines how to do hybrid development. As an example of this process, let's say you want to do coding work with/on the Virtual Device service in Go Land. You want the rest of the EdgeX environment up and running via Docker containers. How would you set up this hybrid environment? Let's take a look. Get and Run the EdgeX Docker Containers Per Getting Started Users , get Docker, Docker Compose setup and then pull the EdgeX docker containers. Since you are working with the virtual device, you probably don't need or want to run all the micro services. You just need the few that the Virtual Device will be communicating with or that will be required to run a minimal EdgeX environment. So you will need to run Consul, Mongo, Core Data, Core Metadata, Core Command, Support Logging, and Support Notifications. After pulling the EdgeX containers, start these containers with a docker-compose up -d command or by starting each of the services individually with the following commands in order Docker Command Description Notes docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from docker-compose up -d config-seed Populate the configuration/registry microservice docker-compose up -d mongo Start the NoSQL MongoDB container An embedded initialization script configures the database for EdgeX documents docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices docker-compose up -d metadata Start the Core Metadata microservice docker-compose up -d data Start the Core Data microservice docker-compose up -d command Start the Core Command microservice Run a \"docker-compose ps\" command to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.) Get and Run the Code In Go Land Per Getting Started Go Developers , get your development environment (GoLand et. al) setup and pull the micro service code you want to work on from GitHub. In this example, we assume you want to get the device-virtual-go project and import it into Go Land. Next, import the device-virtual-go project into Go Land. Follow the instructions in the Import the Project section of Getting Started Go Developers if you need help getting the project into Go Land. Now configure the device-virtual-go project in Go Land to use the other micro services running in Docker. Open the configuration.toml file in the cmd/res folder of the device-virtual-go project in Go Land. Note that the Registry (located in the [Registry] section of the configuration) and all the \"clients\" (located in the [clients] section of the configuration file) suggest that the \"Host\" of these services is \"localhost\". When everything - including the device-virtual service - are running all together on the same host or in the same Docker environment, the host is assumed to be localhost. The services running in Docker are not running on \"localhost\". Instead, they are running in the Docker Engine. Therefore, the \"Host\" configuration setting needs to be modified in the configuration.toml for the Registry and other client services to be the IP address of the Docker Engine hosting the services. The Docker Engine IP address varies by environment. For Linux based systems, the IP address for the Docker Engine is typically 172.17.0.1. In Windows environments, the IP address of Docker Engine is typically 192.168.99.100. Docker networking configurations vary so check your system for the appropriate Docker network. Find information on Docker container network here: Docker Network . In the configuration.toml file in the cmd/res folder of the device-virtual-go project in Go Land, change the Host property for the Registry Clients.Data, Clients.Metadata, and Clients.Logging from \"localhost\" to the IP address for the Docker Engine of your system. Below, localhost is changed to the 172.17.0.1 IP address for the Docker Engine. [Registry] Type = \"consul\" Host = \"172.17.0.1\" Port = 8500 CheckInterval = \"10s\" FailLimit = 3 FailWaitTime = 10 [Logging] EnableRemote = false File = \"./device-virtual.log\" [Writable] LogLevel = 'INFO' [Clients] [Clients.Data] Name = \"edgex-core-data\" Protocol = \"http\" Host = \"172.17.0.1\" Port = 48080 Timeout = 5000 [Clients.Metadata] Name = \"edgex-core-metadata\" Protocol = \"http\" Host = \"172.17.0.1\" Port = 48081 Timeout = 5000 [Clients.Logging] Name = \"edgex-support-logging\" Protocol = \"http\" Host = \"172.17.0.1\" Port = 48061 Save the configuration.toml file after making these changes. Run device-virtual Now you can run device-virtual-go in Go Land that uses the rest of EdgeX Dockerized services. For example, in the Go Land terminal, make sure you are in the device-virtual-go directory and first build the service with a call to make build. After successfully building the service, run the service by going into the cmd folder and executing ./device-virtual (as shown below). Check the Results At this time, your virtual device micro service running in Go Land should be communicating with the other EdgeX micro services running in their Docker containers. Give the virtual device a few seconds or so to initialize itself and start sending data to Core Data. To check that it is working properly, open a browser and point your browser to Core Data to check that events are being deposited. You can do this by calling on the Core Data API that checks the count of events in Core Data ( http://[host].48080/api/v1/event/count ).","title":"Working in a Hybrid Environment"},{"location":"getting-started/Ch-GettingStartedHybrid/#working-in-a-hybrid-environment","text":"In some cases, as a developer, you want to work on a particular micro service, but you don't want to have to download all the source code for all the micro services and run all the micro services in the development tool(s) like Go Land. In this case, you can download and run the EdgeX Docker containers for all the micro services you need and run your single micro service (the one you are presumably working on) from the developer tool of choice and have it point to the other micro services by appropriate address. Within EdgeX, we call this a \"hybrid\" environment - where part of your EdgeX platform is running from a development environment, while other parts are running from the Dockerized containers. This page outlines how to do hybrid development. As an example of this process, let's say you want to do coding work with/on the Virtual Device service in Go Land. You want the rest of the EdgeX environment up and running via Docker containers. How would you set up this hybrid environment? Let's take a look.","title":"Working in a Hybrid Environment"},{"location":"getting-started/Ch-GettingStartedHybrid/#get-and-run-the-edgex-docker-containers","text":"Per Getting Started Users , get Docker, Docker Compose setup and then pull the EdgeX docker containers. Since you are working with the virtual device, you probably don't need or want to run all the micro services. You just need the few that the Virtual Device will be communicating with or that will be required to run a minimal EdgeX environment. So you will need to run Consul, Mongo, Core Data, Core Metadata, Core Command, Support Logging, and Support Notifications. After pulling the EdgeX containers, start these containers with a docker-compose up -d command or by starting each of the services individually with the following commands in order Docker Command Description Notes docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from docker-compose up -d config-seed Populate the configuration/registry microservice docker-compose up -d mongo Start the NoSQL MongoDB container An embedded initialization script configures the database for EdgeX documents docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices docker-compose up -d metadata Start the Core Metadata microservice docker-compose up -d data Start the Core Data microservice docker-compose up -d command Start the Core Command microservice Run a \"docker-compose ps\" command to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.)","title":"Get and Run the EdgeX Docker Containers"},{"location":"getting-started/Ch-GettingStartedHybrid/#get-and-run-the-code-in-go-land","text":"Per Getting Started Go Developers , get your development environment (GoLand et. al) setup and pull the micro service code you want to work on from GitHub. In this example, we assume you want to get the device-virtual-go project and import it into Go Land. Next, import the device-virtual-go project into Go Land. Follow the instructions in the Import the Project section of Getting Started Go Developers if you need help getting the project into Go Land. Now configure the device-virtual-go project in Go Land to use the other micro services running in Docker. Open the configuration.toml file in the cmd/res folder of the device-virtual-go project in Go Land. Note that the Registry (located in the [Registry] section of the configuration) and all the \"clients\" (located in the [clients] section of the configuration file) suggest that the \"Host\" of these services is \"localhost\". When everything - including the device-virtual service - are running all together on the same host or in the same Docker environment, the host is assumed to be localhost. The services running in Docker are not running on \"localhost\". Instead, they are running in the Docker Engine. Therefore, the \"Host\" configuration setting needs to be modified in the configuration.toml for the Registry and other client services to be the IP address of the Docker Engine hosting the services. The Docker Engine IP address varies by environment. For Linux based systems, the IP address for the Docker Engine is typically 172.17.0.1. In Windows environments, the IP address of Docker Engine is typically 192.168.99.100. Docker networking configurations vary so check your system for the appropriate Docker network. Find information on Docker container network here: Docker Network . In the configuration.toml file in the cmd/res folder of the device-virtual-go project in Go Land, change the Host property for the Registry Clients.Data, Clients.Metadata, and Clients.Logging from \"localhost\" to the IP address for the Docker Engine of your system. Below, localhost is changed to the 172.17.0.1 IP address for the Docker Engine. [Registry] Type = \"consul\" Host = \"172.17.0.1\" Port = 8500 CheckInterval = \"10s\" FailLimit = 3 FailWaitTime = 10 [Logging] EnableRemote = false File = \"./device-virtual.log\" [Writable] LogLevel = 'INFO' [Clients] [Clients.Data] Name = \"edgex-core-data\" Protocol = \"http\" Host = \"172.17.0.1\" Port = 48080 Timeout = 5000 [Clients.Metadata] Name = \"edgex-core-metadata\" Protocol = \"http\" Host = \"172.17.0.1\" Port = 48081 Timeout = 5000 [Clients.Logging] Name = \"edgex-support-logging\" Protocol = \"http\" Host = \"172.17.0.1\" Port = 48061 Save the configuration.toml file after making these changes. Run device-virtual Now you can run device-virtual-go in Go Land that uses the rest of EdgeX Dockerized services. For example, in the Go Land terminal, make sure you are in the device-virtual-go directory and first build the service with a call to make build. After successfully building the service, run the service by going into the cmd folder and executing ./device-virtual (as shown below). Check the Results At this time, your virtual device micro service running in Go Land should be communicating with the other EdgeX micro services running in their Docker containers. Give the virtual device a few seconds or so to initialize itself and start sending data to Core Data. To check that it is working properly, open a browser and point your browser to Core Data to check that events are being deposited. You can do this by calling on the Core Data API that checks the count of events in Core Data ( http://[host].48080/api/v1/event/count ).","title":"Get and Run the Code In Go Land"},{"location":"getting-started/Ch-GettingStartedSDK-C/","text":"C SDK In this guide, you create a simple device service in C that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to. Install dependencies To build a device service using the EdgeX C SDK you'll need the following: - libmicrohttpd - libcurl - libyaml - libcbor You can install these on Ubuntu by running: sudo apt install libcurl4-openssl-dev libmicrohttpd-dev libyaml-dev libcbor-dev Get the EdgeX Device SDK for C The next step is to download and build the EdgeX Device SDK for C. You always want to use the release of the SDK that matches the release of EdgeX you are targeting. As of this writing the fuji release is the current stable release of EdgeX, so we will be using the fuji branch of the C SDK. First, clone the fuji branch of device-sdk-c from Github: git clone -b fuji https://github.com/edgexfoundry/device-sdk-c.git cd ./device-sdk-c Then, build the device-sdk-c: ./scripts/build.sh Starting a new Device Service project For this guide we're going to use the example template provided by the C SDK as a starting point, and will modify it to generate random integer values. Begin by copying the template example source into a new directory named example-device-c : mkdir -p ../example-device-c/res cp ./src/c/examples/template.c ../example-device-c cd ../example-device-c Build your Device Service Now you are ready to build your new device service using the C SDK you compiled in an earlier step. Tell the compiler where to find the C SDK files: export CSDK_DIR=../device-sdk-c/build/release/_CPack_Packages/Linux/TGZ/csdk-1.0.0 Note The exact path to your compiled CSDK_DIR may differ, depending on the tagged version number on the SDK Now you can build your device service executable: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk Customize your Device Service Up to now you've been building the example device service provided by the C SDK. In order to change it to a device service that generates random numbers, you need to modify your template.c method template_get_handler so that it reads as follows: for ( uint32_t i = 0 ; i < nreadings ; i ++ ) { const edgex_nvpairs * current = requests [ i ]. attributes ; while ( current != NULL ) { if ( strcmp ( current -> name , \"type\" ) == 0 ) { /* Set the resulting reading type as Uint64 */ readings [ i ]. type = Uint64 ; if ( strcmp ( current -> value , \"random\" ) == 0 ) { /* Set the reading as a random value between 0 and 100 */ readings [ i ]. value . ui64_result = rand () % 100 ; } } current = current -> next ; } } return true ; Creating your Device Profile A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data that will be collected from the simple random number generating Device Service. Explore the files in the src/c/examples/res folder. Take note of the example Device Profile YAML file that is already there (TemplateProfile.yaml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Commands to be issued to the device are represented by \"coreCommands\". Download this random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} into the ./res folder. You can open random-generator-device.yaml in a text editor. In this Device Profile, you are suggesting that the device you are describing to EdgeX has a single property (or deviceResource) which EdgeX should know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In more real world IoT situations, this deviceResource list could be extensive and could be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service. Configuring your Device Service You will now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the scheduled times of when the data is collected from the Device Service (every 10 seconds), and setting up the initial provisioning of the random number generating device when the service starts. Download this configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the ./res folder. If you will be running EdgeX inside of Docker containers (which you will at the bottom of this guide) you need to tell your new Device Service to listen on the Docker host IP address (172.17.0.1) instead of localhost . To do that, modify the configuration.toml file so that the top section looks like this: [Service] Host = \"172.17.0.1\" Port = 49992 Rebuild your Device Service Now you have your new Device Service, modified to return a random number, a Device Profile that will tell EdgeX how to read that random number, as well as a configuration file that will let your Device Service register itself and it's Device Profile with EdgeX, and begin taking readings every 10 seconds. Rebuild your Device Service to reflect the changes that you have made: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk Run your Device Service Allow your newly created Device Service, which was formed out of the Device Service C SDK, to create sensor mimicking data which it then sends to EdgeX. Follow the Getting Started Users guide to start all of the EdgeX services in Docker. From the folder containing the docker-compose file, start EdgeX with a call to: docker-compose up -d Back in your custom Device Service directory, tell your device service where to find the libcsdk.so : export LD_LIBRARY_PATH=$CSDK_DIR/lib Run your device service: ./device-example-c You should now see your Device Service having it's /Random command called every 10 seconds. You can verify that it is sending data into EdgeX by watching the logs of the edgex-core-data service: docker logs -f edgex-core-data Which would print an Event record every time your Device Service is called. You can manually generate an event using curl to query the device service directly: curl 0:49992/api/v1/device/name/RandNum-Device01/Random Note that the value of the \"randomnumber\" reading is an integer between 0 and 100: { \"device\" : \"RandNum-Device01\" , \"origin\" : 1559317102457 , \"readings\" :[{ \"name\" : \"randomnumber\" , \"value\" : \"63\" }]}","title":"C SDK"},{"location":"getting-started/Ch-GettingStartedSDK-C/#c-sdk","text":"In this guide, you create a simple device service in C that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to.","title":"C SDK"},{"location":"getting-started/Ch-GettingStartedSDK-C/#install-dependencies","text":"To build a device service using the EdgeX C SDK you'll need the following: - libmicrohttpd - libcurl - libyaml - libcbor You can install these on Ubuntu by running: sudo apt install libcurl4-openssl-dev libmicrohttpd-dev libyaml-dev libcbor-dev","title":"Install dependencies"},{"location":"getting-started/Ch-GettingStartedSDK-C/#get-the-edgex-device-sdk-for-c","text":"The next step is to download and build the EdgeX Device SDK for C. You always want to use the release of the SDK that matches the release of EdgeX you are targeting. As of this writing the fuji release is the current stable release of EdgeX, so we will be using the fuji branch of the C SDK. First, clone the fuji branch of device-sdk-c from Github: git clone -b fuji https://github.com/edgexfoundry/device-sdk-c.git cd ./device-sdk-c Then, build the device-sdk-c: ./scripts/build.sh","title":"Get the EdgeX Device SDK for C"},{"location":"getting-started/Ch-GettingStartedSDK-C/#starting-a-new-device-service-project","text":"For this guide we're going to use the example template provided by the C SDK as a starting point, and will modify it to generate random integer values. Begin by copying the template example source into a new directory named example-device-c : mkdir -p ../example-device-c/res cp ./src/c/examples/template.c ../example-device-c cd ../example-device-c","title":"Starting a new Device Service project"},{"location":"getting-started/Ch-GettingStartedSDK-C/#build-your-device-service","text":"Now you are ready to build your new device service using the C SDK you compiled in an earlier step. Tell the compiler where to find the C SDK files: export CSDK_DIR=../device-sdk-c/build/release/_CPack_Packages/Linux/TGZ/csdk-1.0.0 Note The exact path to your compiled CSDK_DIR may differ, depending on the tagged version number on the SDK Now you can build your device service executable: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk","title":"Build your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#customize-your-device-service","text":"Up to now you've been building the example device service provided by the C SDK. In order to change it to a device service that generates random numbers, you need to modify your template.c method template_get_handler so that it reads as follows: for ( uint32_t i = 0 ; i < nreadings ; i ++ ) { const edgex_nvpairs * current = requests [ i ]. attributes ; while ( current != NULL ) { if ( strcmp ( current -> name , \"type\" ) == 0 ) { /* Set the resulting reading type as Uint64 */ readings [ i ]. type = Uint64 ; if ( strcmp ( current -> value , \"random\" ) == 0 ) { /* Set the reading as a random value between 0 and 100 */ readings [ i ]. value . ui64_result = rand () % 100 ; } } current = current -> next ; } } return true ;","title":"Customize your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#creating-your-device-profile","text":"A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data that will be collected from the simple random number generating Device Service. Explore the files in the src/c/examples/res folder. Take note of the example Device Profile YAML file that is already there (TemplateProfile.yaml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Commands to be issued to the device are represented by \"coreCommands\". Download this random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} into the ./res folder. You can open random-generator-device.yaml in a text editor. In this Device Profile, you are suggesting that the device you are describing to EdgeX has a single property (or deviceResource) which EdgeX should know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In more real world IoT situations, this deviceResource list could be extensive and could be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service.","title":"Creating your Device Profile"},{"location":"getting-started/Ch-GettingStartedSDK-C/#configuring-your-device-service","text":"You will now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the scheduled times of when the data is collected from the Device Service (every 10 seconds), and setting up the initial provisioning of the random number generating device when the service starts. Download this configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the ./res folder. If you will be running EdgeX inside of Docker containers (which you will at the bottom of this guide) you need to tell your new Device Service to listen on the Docker host IP address (172.17.0.1) instead of localhost . To do that, modify the configuration.toml file so that the top section looks like this: [Service] Host = \"172.17.0.1\" Port = 49992","title":"Configuring your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#rebuild-your-device-service","text":"Now you have your new Device Service, modified to return a random number, a Device Profile that will tell EdgeX how to read that random number, as well as a configuration file that will let your Device Service register itself and it's Device Profile with EdgeX, and begin taking readings every 10 seconds. Rebuild your Device Service to reflect the changes that you have made: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk","title":"Rebuild your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#run-your-device-service","text":"Allow your newly created Device Service, which was formed out of the Device Service C SDK, to create sensor mimicking data which it then sends to EdgeX. Follow the Getting Started Users guide to start all of the EdgeX services in Docker. From the folder containing the docker-compose file, start EdgeX with a call to: docker-compose up -d Back in your custom Device Service directory, tell your device service where to find the libcsdk.so : export LD_LIBRARY_PATH=$CSDK_DIR/lib Run your device service: ./device-example-c You should now see your Device Service having it's /Random command called every 10 seconds. You can verify that it is sending data into EdgeX by watching the logs of the edgex-core-data service: docker logs -f edgex-core-data Which would print an Event record every time your Device Service is called. You can manually generate an event using curl to query the device service directly: curl 0:49992/api/v1/device/name/RandNum-Device01/Random Note that the value of the \"randomnumber\" reading is an integer between 0 and 100: { \"device\" : \"RandNum-Device01\" , \"origin\" : 1559317102457 , \"readings\" :[{ \"name\" : \"randomnumber\" , \"value\" : \"63\" }]}","title":"Run your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/","text":"Golang SDK The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX by providing the common framework that each Device Service needs. The framework provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the framework provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In this guide, you create a simple Device Service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the framework and work necessary to complete a Device Service without actually having a device to talk to. Install dependencies Creating a Device Service requires a little programming in Go. Go Lang (version 1.11 or better) must be installed on your system to complete this lab. Follow the instructions in the link below to install Go if it is not already installed on your platform: https://golang.org/doc/install You need a Git tool to pull the Device Service Go SDK code from the EdgeX Foundry Git repository. Follow the instructions in the link below to install Git for your platform: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git You will also need a \"make\" program. On Ubuntu Linux environments, this can be accomplished with the following command: sudo apt install build-essential Finally, you need a simple text editor (or Go Lang IDE). Get the EdgeX Device SDK for Go Complete the following steps to create a folder on your file system, download the Device SDK , then you pull the SDK to your system, and finally create the new EdgeX Device Service from the SDK templating code. Create a collection of nested folders, ~/go/src/github.com/edgexfoundry on your file system. This folder will eventually hold your new Device Service. In Linux, this can be done with a single mkdir (with -p switch) command: mkdir -p ~/go/src/github.com/edgexfoundry In a terminal window, change directories to the folder you created: cd ~/go/src/github.com/edgexfoundry Enter the following command to pull down the EdgeX Device Service SDK in Go (there is also a Device Service SDK in C): git clone https://github.com/edgexfoundry/device-sdk-go.git Create a folder for the Device Service that we are going to develop. In this step, you are naming the folder to the name you want to give your new Device Service. Standard practice in EdgeX is to prefix the name of a Device Service with device- : mkdir device-simple Copy the example code from device-sdk-go to device-simple : cp -rf ./device-sdk-go/example/* ./device-simple/ Copy Makefile to device-simple: cp ./device-sdk-go/Makefile ./device-simple Copy VERSION to device-simple: cp ./device-sdk-go/VERSION ./device-simple/ Copy version.go to device-simple: cp ./device-sdk-go/version.go ./device-simple/ Starting a new Device Service project The device-sdk-go comes with example code to create a new Device Service. Complete the following steps to modify the copy of the example code to use in your new service. Edit the main.go file in the cmd/device-simple folder. Modify the import statements to replace \"device-sdk-go/example/driver\" to \"device-simple/driver\" from the paths in the import statements. Save the file when you have finished editing. Open Makefile in your favorite text editor and make the following changes Replace MICROSERVICES : MICROSERVICES=example/cmd/device-simple/device-simple with : MICROSERVICES=cmd/device-simple/device-simple Modify GOFLAGS : GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-sdk-go.Version=$(VERSION)\" line to refer to the new service with: GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-simple.Version=$(VERSION)\" Modify build : example/cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./example/cmd/device-simple to: cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./cmd/device-simple Save the file. Enter the following command to create the initial module definition and write it to the go.mod file: GO111MODULE=on go mod init Build your Device Service To ensure that the code you have moved and updated still works, build the current Device Service. In a terminal window, change directories to the device-simple folder (the folder containing the Makefile): device-simple \u251c\u2500\u2500 cmd \u2502 \u2514\u2500\u2500 device-simple \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u251c\u2500\u2500 main.go \u2502 \u2514\u2500\u2500 res \u2502 \u251c\u2500\u2500 Simple-Driver.yaml \u2502 \u251c\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 docker \u2502 \u2502 \u2514\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 off.jpg \u2502 \u2514\u2500\u2500 on.png \u251c\u2500\u2500 driver \u2502 \u2514\u2500\u2500 simpledriver.go \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 Version \u251c\u2500\u2500 version.go \u251c\u2500\u2500 go.mod \u2514\u2500\u2500 go.sum Build the service by issuing the following command: make build If there are no errors, your service is ready for you to add customizations to generate data values as if there was a sensor attached. If there are errors, retrace your steps to correct the error and try to build again. Ask you instructor for help in finding the issue if you are unable to locate it given the error messages you receive from the build process. Customize your Device Service The Device Service you are creating isn't going to talk to a real device. Instead, it is going to generate a random number where the service would make a call to get sensor data from the actual device. By so doing, you see where the EdgeX Device Service would make a call to a local device (using its protocol and device drivers under the covers) to provide EdgeX with its sensor readings: Locate the simpledriver.go file in the /driver folder and open it with your favorite editor. In the import() area at the top of the file, add \"math/rand\" under \"time\". Locate the HandleReadCommands() function in this file. Notice the following line of code in this file: cv, _ := dsModels.NewBoolValue(reqs[0].DeviceResourceName, now, s.switchButton) Replace the two lines of code with the following: if reqs[0].DeviceResourceName == \"randomnumber\" { cv, _ := dsModels.NewInt32Value(reqs[0].DeviceResourceName, now, int32(rand.Intn(100))) The first line of code to confirmed request is for the customized resource \"randomnumber\". Also, the second line of code generates an integer (between 0 and 100) and uses that as the value the Device Service sends to EdgeX -- mimicking the collection of data from a real device. It is here that the Device Service would normally capture some sensor reading from a device and send the data to EdgeX. The line of code you just added is where you'd need to do some customization work to talk to the sensor, get the sensor's latest sensor values and send them into EdgeX. Save the simpledriver.go file Creating your Device Profile A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data to collect from the simple random number generating Device Service. Do the following: Explore the files in the cmd/device-simple/res folder. Take note of the example Device Profile YAML file that is already there (Simple-Driver.yml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Command to be issued to the device are represented by \"deviceCommands\". Download random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder. Open the random-generator-device.yaml file in a text editor. In this Device Profile, you define that the device you are describing to EdgeX has a single property (or deviceResource) that EdgeX needs to know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In real world IoT situations, this deviceResource list could be extensive and be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service. Configuring your Device Service Now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the auto event frequency of when the data is collected from the Device Service (every 10 seconds in this example), and setting up the initial provisioning of the random number generating device when the service starts. Download configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder (this will overwrite an existing file -- that's ok). Rebuild your Device Service Just as you did before, you are ready to build the device-simple service -- creating the executable program that is your Device Service: In a terminal window, change directories to the base device-simple folder (containing the Makefile). Build the Device Service by issuing the following command: make build If there are no errors, your service has now been created and is available in the cmd/device-simple folder (look for the device-simple file). Run your Device Service Allow your newly created Device Service, which was formed out of the Device Service Go SDK, to create sensor-mimicking data that it then sends to EdgeX: As described in the ./Ch-GettingStartedUsers {.interpreted-text role=\"doc\"} guide, use Docker Compose to start all of EdgeX. From the folder containing the docker-compose file, start EdgeX with the following call: docker-compose up -d In a terminal window, change directories to the device-simple's cmd/device-simple folder. The executable device-simple is located there. Execute the Device Service with the ./device-simple command, as shown below: This starts the service and immediately displays log entries in the terminal. Using a browser, enter the following URL to see the Event/Reading data that the service is generating and sending to EdgeX: http://localhost:48080/api/v1/event/device/RandNum-Device-01/100 This request asks for the last 100 Events/Readings from Core Data associated to the RandNum-Device-01. Note : If you are running the other EdgeX services somewhere other than localhost, use that hostname in the above URL.","title":"Golang SDK"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#golang-sdk","text":"The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX by providing the common framework that each Device Service needs. The framework provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the framework provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In this guide, you create a simple Device Service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the framework and work necessary to complete a Device Service without actually having a device to talk to.","title":"Golang SDK"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#install-dependencies","text":"Creating a Device Service requires a little programming in Go. Go Lang (version 1.11 or better) must be installed on your system to complete this lab. Follow the instructions in the link below to install Go if it is not already installed on your platform: https://golang.org/doc/install You need a Git tool to pull the Device Service Go SDK code from the EdgeX Foundry Git repository. Follow the instructions in the link below to install Git for your platform: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git You will also need a \"make\" program. On Ubuntu Linux environments, this can be accomplished with the following command: sudo apt install build-essential Finally, you need a simple text editor (or Go Lang IDE).","title":"Install dependencies"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#get-the-edgex-device-sdk-for-go","text":"Complete the following steps to create a folder on your file system, download the Device SDK , then you pull the SDK to your system, and finally create the new EdgeX Device Service from the SDK templating code. Create a collection of nested folders, ~/go/src/github.com/edgexfoundry on your file system. This folder will eventually hold your new Device Service. In Linux, this can be done with a single mkdir (with -p switch) command: mkdir -p ~/go/src/github.com/edgexfoundry In a terminal window, change directories to the folder you created: cd ~/go/src/github.com/edgexfoundry Enter the following command to pull down the EdgeX Device Service SDK in Go (there is also a Device Service SDK in C): git clone https://github.com/edgexfoundry/device-sdk-go.git Create a folder for the Device Service that we are going to develop. In this step, you are naming the folder to the name you want to give your new Device Service. Standard practice in EdgeX is to prefix the name of a Device Service with device- : mkdir device-simple Copy the example code from device-sdk-go to device-simple : cp -rf ./device-sdk-go/example/* ./device-simple/ Copy Makefile to device-simple: cp ./device-sdk-go/Makefile ./device-simple Copy VERSION to device-simple: cp ./device-sdk-go/VERSION ./device-simple/ Copy version.go to device-simple: cp ./device-sdk-go/version.go ./device-simple/","title":"Get the EdgeX Device SDK for Go"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#starting-a-new-device-service-project","text":"The device-sdk-go comes with example code to create a new Device Service. Complete the following steps to modify the copy of the example code to use in your new service. Edit the main.go file in the cmd/device-simple folder. Modify the import statements to replace \"device-sdk-go/example/driver\" to \"device-simple/driver\" from the paths in the import statements. Save the file when you have finished editing. Open Makefile in your favorite text editor and make the following changes Replace MICROSERVICES : MICROSERVICES=example/cmd/device-simple/device-simple with : MICROSERVICES=cmd/device-simple/device-simple Modify GOFLAGS : GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-sdk-go.Version=$(VERSION)\" line to refer to the new service with: GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-simple.Version=$(VERSION)\" Modify build : example/cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./example/cmd/device-simple to: cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./cmd/device-simple Save the file. Enter the following command to create the initial module definition and write it to the go.mod file: GO111MODULE=on go mod init","title":"Starting a new Device Service project"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#build-your-device-service","text":"To ensure that the code you have moved and updated still works, build the current Device Service. In a terminal window, change directories to the device-simple folder (the folder containing the Makefile): device-simple \u251c\u2500\u2500 cmd \u2502 \u2514\u2500\u2500 device-simple \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u251c\u2500\u2500 main.go \u2502 \u2514\u2500\u2500 res \u2502 \u251c\u2500\u2500 Simple-Driver.yaml \u2502 \u251c\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 docker \u2502 \u2502 \u2514\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 off.jpg \u2502 \u2514\u2500\u2500 on.png \u251c\u2500\u2500 driver \u2502 \u2514\u2500\u2500 simpledriver.go \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 Version \u251c\u2500\u2500 version.go \u251c\u2500\u2500 go.mod \u2514\u2500\u2500 go.sum Build the service by issuing the following command: make build If there are no errors, your service is ready for you to add customizations to generate data values as if there was a sensor attached. If there are errors, retrace your steps to correct the error and try to build again. Ask you instructor for help in finding the issue if you are unable to locate it given the error messages you receive from the build process.","title":"Build your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#customize-your-device-service","text":"The Device Service you are creating isn't going to talk to a real device. Instead, it is going to generate a random number where the service would make a call to get sensor data from the actual device. By so doing, you see where the EdgeX Device Service would make a call to a local device (using its protocol and device drivers under the covers) to provide EdgeX with its sensor readings: Locate the simpledriver.go file in the /driver folder and open it with your favorite editor. In the import() area at the top of the file, add \"math/rand\" under \"time\". Locate the HandleReadCommands() function in this file. Notice the following line of code in this file: cv, _ := dsModels.NewBoolValue(reqs[0].DeviceResourceName, now, s.switchButton) Replace the two lines of code with the following: if reqs[0].DeviceResourceName == \"randomnumber\" { cv, _ := dsModels.NewInt32Value(reqs[0].DeviceResourceName, now, int32(rand.Intn(100))) The first line of code to confirmed request is for the customized resource \"randomnumber\". Also, the second line of code generates an integer (between 0 and 100) and uses that as the value the Device Service sends to EdgeX -- mimicking the collection of data from a real device. It is here that the Device Service would normally capture some sensor reading from a device and send the data to EdgeX. The line of code you just added is where you'd need to do some customization work to talk to the sensor, get the sensor's latest sensor values and send them into EdgeX. Save the simpledriver.go file","title":"Customize your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#creating-your-device-profile","text":"A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data to collect from the simple random number generating Device Service. Do the following: Explore the files in the cmd/device-simple/res folder. Take note of the example Device Profile YAML file that is already there (Simple-Driver.yml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Command to be issued to the device are represented by \"deviceCommands\". Download random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder. Open the random-generator-device.yaml file in a text editor. In this Device Profile, you define that the device you are describing to EdgeX has a single property (or deviceResource) that EdgeX needs to know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In real world IoT situations, this deviceResource list could be extensive and be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service.","title":"Creating your Device Profile"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#configuring-your-device-service","text":"Now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the auto event frequency of when the data is collected from the Device Service (every 10 seconds in this example), and setting up the initial provisioning of the random number generating device when the service starts. Download configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder (this will overwrite an existing file -- that's ok).","title":"Configuring your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#rebuild-your-device-service","text":"Just as you did before, you are ready to build the device-simple service -- creating the executable program that is your Device Service: In a terminal window, change directories to the base device-simple folder (containing the Makefile). Build the Device Service by issuing the following command: make build If there are no errors, your service has now been created and is available in the cmd/device-simple folder (look for the device-simple file).","title":"Rebuild your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#run-your-device-service","text":"Allow your newly created Device Service, which was formed out of the Device Service Go SDK, to create sensor-mimicking data that it then sends to EdgeX: As described in the ./Ch-GettingStartedUsers {.interpreted-text role=\"doc\"} guide, use Docker Compose to start all of EdgeX. From the folder containing the docker-compose file, start EdgeX with the following call: docker-compose up -d In a terminal window, change directories to the device-simple's cmd/device-simple folder. The executable device-simple is located there. Execute the Device Service with the ./device-simple command, as shown below: This starts the service and immediately displays log entries in the terminal. Using a browser, enter the following URL to see the Event/Reading data that the service is generating and sending to EdgeX: http://localhost:48080/api/v1/event/device/RandNum-Device-01/100 This request asks for the last 100 Events/Readings from Core Data associated to the RandNum-Device-01. Note : If you are running the other EdgeX services somewhere other than localhost, use that hostname in the above URL.","title":"Run your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK/","text":"Device Service SDK The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX because it provides the common scaffolding that each Device Service needs to have. The scaffolding provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the scaffolding provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In these guides, you will create a simple device service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to.","title":"Device Service SDK"},{"location":"getting-started/Ch-GettingStartedSDK/#device-service-sdk","text":"The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX because it provides the common scaffolding that each Device Service needs to have. The scaffolding provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the scaffolding provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In these guides, you will create a simple device service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to.","title":"Device Service SDK"},{"location":"getting-started/Ch-GettingStartedUsers/","text":"Getting Started - Users Introduction These instructions are for Users to obtain and run EdgeX Foundry. (Developers should read: Getting Started Developers ) EdgeX Foundry is a collection of more than a dozen microservices that are deployed to provide minimal edge platform capability. EdgeX Foundry microservice source code can be downloaded and built into deployment artifacts, but if you are not a Developer, or if you do not have a specific need to run EdgeX Foundry \"natively,\" you do not need to download source code. Users have the easier option to use Docker and run EdgeX Foundry in microservice Docker containers. EdgeX Foundry microservices are automatically built and containerized as new code is checked into the source repository. Therefore, \"Dockerized\" EdgeX Foundry is not only easier to obtain and deploy to your environment, but can also have the most up-to-date EdgeX Foundry microservices (depending on which container registry is used to get the microservices). To obtain and run EdgeX Foundry, perform the following steps: Platform Requirements EdgeX Foundry is an operating system (OS)-agnostic and hardware (HW)-agnostic IoT edge platform. While minimum platform requirements are yet being established, at this time the following options are recommended: Memory: minimum of 1 GB Hard drive space: minimum of 3 GB of space to run the EdgeX Foundry containers, but you may want more depending on how long sensor and device data is to be retained OS: EdgeX Foundry has been run successfully on many systems, including, but not limited to the following systems Windows (ver 7 - 10) Ubuntu Desktop (ver 14-16) Ubuntu Server (ver 14) Ubuntu Core (ver 16) Mac OS X 10 Get & Run EdgeX Foundry Install Docker & Docker Compose To run Dockerized EdgeX Foundry, you need to install Docker. See https://docs.docker.com/install/ to learn how to obtain and install Docker. If you are new to using Docker, the same web site provides you additional information. The following short video has is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Docker Compose is used to orchestrate the fetch (or pull), installation, and the start and stop of the EdgeX Foundry microservice containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. Docker Compose is automatically installed with Docker for Mac and Windows users. See: https://docs.docker.com/compose/install/ to determine if your Docker installation already contains Docker Compose, and how to install Compose if it does not. You do not need to be an expert with Docker to obtain and run EdgeX Foundry. The instructions in this guide provide you with the steps to get EdgeX Foundry running in your environment. Some basic knowledge of these two technologies of Docker and Docker Compose, are nice to have, but not required. Basic Docker and Docker Compose commands, enable you to run, update, and diagnose issues within EdgeX Foundry. Download the EdgeX Foundry Compose File After installing Docker and Docker Compose, you need the Docker Compose file that is a manifest of all the EdgeX Foundry microservices. EdgeX Foundry has over 12 microservices, each deployed in their own Docker container, in a typical EdgeX Foundry deployment. If you know Docker and understand the architecture of EdgeX Foundry and its microservices, you can manually issue Docker commands to download and run each of the EdgeX Foundry containers yourself. Situations exist, particularly in development situations, when you want to have this manual control even though manually issuing commands can be a bit tedious. More instructions are provided in this documentation set if you need to have more control of downloading and running EdgeX Foundry microservices. Getting and running EdgeX Foundry microservices can also be accomplished more easily provided you have the Docker Compose file that specifies to Docker/Docker Compose which containers you want, and how you want to run those containers. The EdgeX Foundry development team provides you with Docker Compose files for each release through the EdgeX Foundry GitHub repository. To obtain and run EdgeX Foundry, visit the project GitHub and download (or copy the contents) of the EdgeX Foundry Docker Compose file suitable to the version you wish to use - to a local directory. The collection of the EdgeX Foundry Docker compose files are found here: https://github.com/edgexfoundry/developer-scripts/tree/master/releases Note that most of the Docker Compose files carry a specific version identifier (like california-0.6.0) in the file name. These Compose files help obtain the specific version of EdgeX. The docker-compose.yml file will pull the latest tagged EdgeX microservices from Docker Hub. The docker-compose-nexus.yml will pull the latest microservice images from the developer's Nexus registry which contains the latest built artifacts. These are typically work-in-progress microservice artifacts and should not be used by most end users. It is recommended that you use the latest version of EdgeX Foundry. As of this writing, the latest version is: Delhi (version 0.7.1) A Docker Compose file is a manifest file, which lists: The Docker containers (or more precisely the Docker container images) that should be downloaded, The order in which the containers should be started The parameters under which the containers should be run Run EdgeX Foundry Now that you have the EdgeX Foundry Docker Compose file, you are ready to run EdgeX Foundry. Follow these steps to get the container images and start EdgeX Foundry! First, unless you downloaded docker-compose.yml, rename the Docker Compose file you downloaded to docker-compose.yml . By default, Docker Compose looks for a file by this name when running all Docker Compose commands. You could use the original file name, but it would require adding more arguments to the Docker Compose commands and creates more circumstances for errors. Next, open a command terminal to the Docker Compose file location - that is where you download the EdgeX Foundry docker-compose.yml file above. On some operating systems, there is a special Docker Terminal. On other platforms, Docker and Docker Compose can be run from a normal terminal window. See the Docker documentation for more help running Docker and Docker Compose commands on your platform. Now run the following command in the terminal to pull (but don't start) all the EdgeX Docker images down to your system: docker-compose pull After the Docker images are pulled, start EdgeX with this command in the terminal: docker-compose up -d The -d option indicates you want the Docker Compose to run the EdgeX containers in detached mode - that is to run the containers in the background. Without -d, the containers will all start in the terminal and to use the terminal further you have to stop the containers. In some situations, you may want to bring up the containers one at a time using Docker Compose. If you are a developer or if you don't want to bring up all of EdgeX (perhaps because you are just working with a few of the services), you can issue Docker Compose commands to pull and start each EdgeX container separately. Because some containers are dependent on others, you should try to start them in a specific order. The table below provides you the commands to start each of the EdgeX microservices in the order they should be started (based on their dependencies with one another). Essentially, the single Docker Compose up command (docker-compose up -d) uses the manifest to run all of the individual up command listed here in order. Docker Command Description Notes docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from docker-compose up -d config-seed Populate the configuration/registry microservice docker-compose up -d mongo Start the NoSQL MongoDB container An embedded initialization script configures the database for EdgeX documents docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices docker-compose up -d metadata Start the Core Metadata microservice docker-compose up -d data Start the Core Data microservice docker-compose up -d command Start the Core Command microservice docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices docker-compose up -d export-client Start the Export Client registration microservice docker-compose up -d export-distro Start the Export Distribution microservice docker-compose up -d rulesengine Start the Rules Engine microservice This service is still implemented in Java and takes more time to start docker-compose up -d device-virtual Start the virtual device service Run a docker-compose ps command to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.) Stop and Remove EdgeX Foundry To stop and deconstruct (remove) all the EdgeX Foundry containers, call on docker-compose down . Docker shows the containers being stopped and then removed. Note, you may wish to stop (versus stop and remove) all the EdgeX Containers. See more details in the Advanced EdgeX Foundry User Command below. After this operation, calling docker-compose ps shows no running or available containers. Advanced EdgeX Foundry User Commands After you have mastered obtaining and running EdgeX Foundry, you may want to take more control of EdgeX Foundry microservices. These commands provide you the ability to do that. Pull Images Use docker-compose pull to download all the container images listed in the Compose file. The docker-compose pull [compose-container-name] to download a specific container image name from the Compose file. Here, the Export Client image is being pulled. To get a list of the Docker Compose names of the containers (as they are in the docker-compose.yml file), run docker-compose config --services as shown below. Start Containers Use docker-compose start to re-start all the containers (after a stop) ... or docker-compose start [compose-container-name] to start an individual container (after that container has been stopped). Here, the volume container is started. If you have stopped a specific container and updated its image (with docker-compose pull above), this command allows you to recreate/start the image without affect to other containers. Stop Containers To stop an individual container, you can use docker-compose stop [compose-container-name] . Below the rulesengine container is stopped. Stopped containers can be started again (using docker-compose start above) versus docker-compose down with stops all the containers and then destroys/removes all the containers. To stop (but not remove) all containers, issue docker-compose stop . The docker-compose down command stops and then removes all the containers, whereby docker-compose stop just stops the container(s) but does not remove the container image. For example, if your run docker ps -a after a docker-compose stop , you would still see the container images in an \"exited\" state. Checking the Status of EdgeX Foundry Docker Container Status Check As shown above, from the terminal, use docker-compose ps to get a list of the containers that exist and are running. In additional, the standard docker command ( docker ps -a ) can also provide the list of running containers. The standard docker command ( docker ps -a ) command also indicates when the container was started, how long it has been running, and many other details. You can use a --format option to retain only the pertinent information in your list. See here for more details on formatting the list of containers. The status above was created using docker ps -a --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\\t{{.RunningFor}}\" EdgeX Foundry Container Logs To view the log of any container, use the command docker-compose logs -f [compose-container-name] . The example below shows the log for the data container. When you are done reviewing the content of the log, select Control-c to stop the output to your terminal. Microservice Ping Check Each EdgeX Foundry microservice has been built to respond to a \"ping\" HTTP request. A ping request or ping utility is used in networking environments to check the reach-ability of a network resource (see here). EdgeX Foundry uses the same concept to check the availability or reach-ability of a microservice resource. After the EdgeX Foundry microservice containers are running, you can \"ping\" any one of the microservices to check that it is running. Open a browser or HTTP REST client tool and use the service's ping address ( http://host :[port]/api/v1/ping) to see if it is available. Below, the Core Data microservice is \"pinged.\" Below is a list of the EdgeX Foundry microservices, their ports, and \"ping\" URLs. EdgeX Foundry Microservice Docker Compose Container Container Name Port Ping URL Core Command command edgex-core-command 48082 http://[host]:48082/api/v1/ping Core Data data edgex-core-data 48080 http://[host]:48080/api/v1/ping Core Metadata metadata edgex-core-metadata 48081 http://[host]:48081/api/v1/ping Export Client export-client edgex-export-client 48071 http://[host]:48071/api/v1/ping Export Distribution export-distro edgex-export-distro 48070 http://[host]:48070/api/v1/ping Rules Engine rulesengine edgex-support-rulesengine 48075 http://[host]:48075/api/v1/ping Support Logging logging edgex-support-logging 48061 http://[host]:48061/api/v1/ping Support Notifications notifications edgex-support-notifications 48060 http://[host]:48060/api/v1/ping Support Scheduler scheduler edgex-support-scheduler 48085 http://[host]:48085/api/v1/ping Virtual Device Service device-virtual edgex-device-virtual 49990 http://[host]:49990/api/v1/ping The \"host\" address for the URLs above is determined by the Docker Engine. The default Docker Engine IP address varies by operating system (this can be configured on your system-see the Docker documentation for details). EdgeX Foundry Consul Registry EdgeX Foundry uses the open source Consul project as its registry service. All EdgeX Foundry microservices are expected to register with the Consul registry as they come up. Going to Consul's dashboard UI enables you to see which services are up. Find the Consul UI at http://[host]:8500/ui .","title":"Getting Started - Users"},{"location":"getting-started/Ch-GettingStartedUsers/#getting-started-users","text":"","title":"Getting Started - Users"},{"location":"getting-started/Ch-GettingStartedUsers/#introduction","text":"These instructions are for Users to obtain and run EdgeX Foundry. (Developers should read: Getting Started Developers ) EdgeX Foundry is a collection of more than a dozen microservices that are deployed to provide minimal edge platform capability. EdgeX Foundry microservice source code can be downloaded and built into deployment artifacts, but if you are not a Developer, or if you do not have a specific need to run EdgeX Foundry \"natively,\" you do not need to download source code. Users have the easier option to use Docker and run EdgeX Foundry in microservice Docker containers. EdgeX Foundry microservices are automatically built and containerized as new code is checked into the source repository. Therefore, \"Dockerized\" EdgeX Foundry is not only easier to obtain and deploy to your environment, but can also have the most up-to-date EdgeX Foundry microservices (depending on which container registry is used to get the microservices). To obtain and run EdgeX Foundry, perform the following steps:","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedUsers/#platform-requirements","text":"EdgeX Foundry is an operating system (OS)-agnostic and hardware (HW)-agnostic IoT edge platform. While minimum platform requirements are yet being established, at this time the following options are recommended: Memory: minimum of 1 GB Hard drive space: minimum of 3 GB of space to run the EdgeX Foundry containers, but you may want more depending on how long sensor and device data is to be retained OS: EdgeX Foundry has been run successfully on many systems, including, but not limited to the following systems Windows (ver 7 - 10) Ubuntu Desktop (ver 14-16) Ubuntu Server (ver 14) Ubuntu Core (ver 16) Mac OS X 10","title":"Platform Requirements"},{"location":"getting-started/Ch-GettingStartedUsers/#get-run-edgex-foundry","text":"","title":"Get &amp; Run EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#install-docker-docker-compose","text":"To run Dockerized EdgeX Foundry, you need to install Docker. See https://docs.docker.com/install/ to learn how to obtain and install Docker. If you are new to using Docker, the same web site provides you additional information. The following short video has is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Docker Compose is used to orchestrate the fetch (or pull), installation, and the start and stop of the EdgeX Foundry microservice containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. Docker Compose is automatically installed with Docker for Mac and Windows users. See: https://docs.docker.com/compose/install/ to determine if your Docker installation already contains Docker Compose, and how to install Compose if it does not. You do not need to be an expert with Docker to obtain and run EdgeX Foundry. The instructions in this guide provide you with the steps to get EdgeX Foundry running in your environment. Some basic knowledge of these two technologies of Docker and Docker Compose, are nice to have, but not required. Basic Docker and Docker Compose commands, enable you to run, update, and diagnose issues within EdgeX Foundry.","title":"Install Docker &amp; Docker Compose"},{"location":"getting-started/Ch-GettingStartedUsers/#download-the-edgex-foundry-compose-file","text":"After installing Docker and Docker Compose, you need the Docker Compose file that is a manifest of all the EdgeX Foundry microservices. EdgeX Foundry has over 12 microservices, each deployed in their own Docker container, in a typical EdgeX Foundry deployment. If you know Docker and understand the architecture of EdgeX Foundry and its microservices, you can manually issue Docker commands to download and run each of the EdgeX Foundry containers yourself. Situations exist, particularly in development situations, when you want to have this manual control even though manually issuing commands can be a bit tedious. More instructions are provided in this documentation set if you need to have more control of downloading and running EdgeX Foundry microservices. Getting and running EdgeX Foundry microservices can also be accomplished more easily provided you have the Docker Compose file that specifies to Docker/Docker Compose which containers you want, and how you want to run those containers. The EdgeX Foundry development team provides you with Docker Compose files for each release through the EdgeX Foundry GitHub repository. To obtain and run EdgeX Foundry, visit the project GitHub and download (or copy the contents) of the EdgeX Foundry Docker Compose file suitable to the version you wish to use - to a local directory. The collection of the EdgeX Foundry Docker compose files are found here: https://github.com/edgexfoundry/developer-scripts/tree/master/releases Note that most of the Docker Compose files carry a specific version identifier (like california-0.6.0) in the file name. These Compose files help obtain the specific version of EdgeX. The docker-compose.yml file will pull the latest tagged EdgeX microservices from Docker Hub. The docker-compose-nexus.yml will pull the latest microservice images from the developer's Nexus registry which contains the latest built artifacts. These are typically work-in-progress microservice artifacts and should not be used by most end users. It is recommended that you use the latest version of EdgeX Foundry. As of this writing, the latest version is: Delhi (version 0.7.1) A Docker Compose file is a manifest file, which lists: The Docker containers (or more precisely the Docker container images) that should be downloaded, The order in which the containers should be started The parameters under which the containers should be run","title":"Download the EdgeX Foundry Compose File"},{"location":"getting-started/Ch-GettingStartedUsers/#run-edgex-foundry","text":"Now that you have the EdgeX Foundry Docker Compose file, you are ready to run EdgeX Foundry. Follow these steps to get the container images and start EdgeX Foundry! First, unless you downloaded docker-compose.yml, rename the Docker Compose file you downloaded to docker-compose.yml . By default, Docker Compose looks for a file by this name when running all Docker Compose commands. You could use the original file name, but it would require adding more arguments to the Docker Compose commands and creates more circumstances for errors. Next, open a command terminal to the Docker Compose file location - that is where you download the EdgeX Foundry docker-compose.yml file above. On some operating systems, there is a special Docker Terminal. On other platforms, Docker and Docker Compose can be run from a normal terminal window. See the Docker documentation for more help running Docker and Docker Compose commands on your platform. Now run the following command in the terminal to pull (but don't start) all the EdgeX Docker images down to your system: docker-compose pull After the Docker images are pulled, start EdgeX with this command in the terminal: docker-compose up -d The -d option indicates you want the Docker Compose to run the EdgeX containers in detached mode - that is to run the containers in the background. Without -d, the containers will all start in the terminal and to use the terminal further you have to stop the containers. In some situations, you may want to bring up the containers one at a time using Docker Compose. If you are a developer or if you don't want to bring up all of EdgeX (perhaps because you are just working with a few of the services), you can issue Docker Compose commands to pull and start each EdgeX container separately. Because some containers are dependent on others, you should try to start them in a specific order. The table below provides you the commands to start each of the EdgeX microservices in the order they should be started (based on their dependencies with one another). Essentially, the single Docker Compose up command (docker-compose up -d) uses the manifest to run all of the individual up command listed here in order. Docker Command Description Notes docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from docker-compose up -d config-seed Populate the configuration/registry microservice docker-compose up -d mongo Start the NoSQL MongoDB container An embedded initialization script configures the database for EdgeX documents docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices docker-compose up -d metadata Start the Core Metadata microservice docker-compose up -d data Start the Core Data microservice docker-compose up -d command Start the Core Command microservice docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices docker-compose up -d export-client Start the Export Client registration microservice docker-compose up -d export-distro Start the Export Distribution microservice docker-compose up -d rulesengine Start the Rules Engine microservice This service is still implemented in Java and takes more time to start docker-compose up -d device-virtual Start the virtual device service Run a docker-compose ps command to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.)","title":"Run EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#stop-and-remove-edgex-foundry","text":"To stop and deconstruct (remove) all the EdgeX Foundry containers, call on docker-compose down . Docker shows the containers being stopped and then removed. Note, you may wish to stop (versus stop and remove) all the EdgeX Containers. See more details in the Advanced EdgeX Foundry User Command below. After this operation, calling docker-compose ps shows no running or available containers.","title":"Stop and Remove EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#advanced-edgex-foundry-user-commands","text":"After you have mastered obtaining and running EdgeX Foundry, you may want to take more control of EdgeX Foundry microservices. These commands provide you the ability to do that. Pull Images Use docker-compose pull to download all the container images listed in the Compose file. The docker-compose pull [compose-container-name] to download a specific container image name from the Compose file. Here, the Export Client image is being pulled. To get a list of the Docker Compose names of the containers (as they are in the docker-compose.yml file), run docker-compose config --services as shown below. Start Containers Use docker-compose start to re-start all the containers (after a stop) ... or docker-compose start [compose-container-name] to start an individual container (after that container has been stopped). Here, the volume container is started. If you have stopped a specific container and updated its image (with docker-compose pull above), this command allows you to recreate/start the image without affect to other containers. Stop Containers To stop an individual container, you can use docker-compose stop [compose-container-name] . Below the rulesengine container is stopped. Stopped containers can be started again (using docker-compose start above) versus docker-compose down with stops all the containers and then destroys/removes all the containers. To stop (but not remove) all containers, issue docker-compose stop . The docker-compose down command stops and then removes all the containers, whereby docker-compose stop just stops the container(s) but does not remove the container image. For example, if your run docker ps -a after a docker-compose stop , you would still see the container images in an \"exited\" state.","title":"Advanced EdgeX Foundry User Commands"},{"location":"getting-started/Ch-GettingStartedUsers/#checking-the-status-of-edgex-foundry","text":"","title":"Checking the Status of EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#docker-container-status-check","text":"As shown above, from the terminal, use docker-compose ps to get a list of the containers that exist and are running. In additional, the standard docker command ( docker ps -a ) can also provide the list of running containers. The standard docker command ( docker ps -a ) command also indicates when the container was started, how long it has been running, and many other details. You can use a --format option to retain only the pertinent information in your list. See here for more details on formatting the list of containers. The status above was created using docker ps -a --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\\t{{.RunningFor}}\" EdgeX Foundry Container Logs To view the log of any container, use the command docker-compose logs -f [compose-container-name] . The example below shows the log for the data container. When you are done reviewing the content of the log, select Control-c to stop the output to your terminal. Microservice Ping Check Each EdgeX Foundry microservice has been built to respond to a \"ping\" HTTP request. A ping request or ping utility is used in networking environments to check the reach-ability of a network resource (see here). EdgeX Foundry uses the same concept to check the availability or reach-ability of a microservice resource. After the EdgeX Foundry microservice containers are running, you can \"ping\" any one of the microservices to check that it is running. Open a browser or HTTP REST client tool and use the service's ping address ( http://host :[port]/api/v1/ping) to see if it is available. Below, the Core Data microservice is \"pinged.\" Below is a list of the EdgeX Foundry microservices, their ports, and \"ping\" URLs. EdgeX Foundry Microservice Docker Compose Container Container Name Port Ping URL Core Command command edgex-core-command 48082 http://[host]:48082/api/v1/ping Core Data data edgex-core-data 48080 http://[host]:48080/api/v1/ping Core Metadata metadata edgex-core-metadata 48081 http://[host]:48081/api/v1/ping Export Client export-client edgex-export-client 48071 http://[host]:48071/api/v1/ping Export Distribution export-distro edgex-export-distro 48070 http://[host]:48070/api/v1/ping Rules Engine rulesengine edgex-support-rulesengine 48075 http://[host]:48075/api/v1/ping Support Logging logging edgex-support-logging 48061 http://[host]:48061/api/v1/ping Support Notifications notifications edgex-support-notifications 48060 http://[host]:48060/api/v1/ping Support Scheduler scheduler edgex-support-scheduler 48085 http://[host]:48085/api/v1/ping Virtual Device Service device-virtual edgex-device-virtual 49990 http://[host]:49990/api/v1/ping The \"host\" address for the URLs above is determined by the Docker Engine. The default Docker Engine IP address varies by operating system (this can be configured on your system-see the Docker documentation for details).","title":"Docker Container Status Check"},{"location":"getting-started/Ch-GettingStartedUsers/#edgex-foundry-consul-registry","text":"EdgeX Foundry uses the open source Consul project as its registry service. All EdgeX Foundry microservices are expected to register with the Consul registry as they come up. Going to Consul's dashboard UI enables you to see which services are up. Find the Consul UI at http://[host]:8500/ui .","title":"EdgeX Foundry Consul Registry"},{"location":"getting-started/Ch-GettingStartedUsersNexus/","text":"Getting Docker Images from EdgeX Nexus Repository In some cases, it may be necessary to get your EdgeX container images from the Nexus Repository (managed by the Linux Foundation) as opposed to the Docker Hub repository. Nexus is a repository that contains all the staging and development containers and images for the EdgeX Foundry project. Containers destined for Docker Hub actually move through the Nexus Repository on their way to Docker Hub. Some reasons why you might want to use container images from the Nexus Repos might include when: a) the container is not available from Docker Hub (or Docker Hub is down temporarily) b) you need the latest development build container. c) you are working in a Windows or non-Linux environment and you are unable to build a container without some issues (Docker shell scripts may not work in Docker For Windows due to CR-LF on Git in Windows). In order to get containers from the Nexus Repository, follow these steps: Pull the container(s) Pull the container(s) from Nexus into your local environment. In the example below, the docker-core-config-seed container image is pulled from Nexus. Note the host name (nexus3.edgexfoundry.org) and port (10004) for pulling \\$ docker pull nexus3.edgexfoundry.org:10004/docker-core-config-seed Replace the image(s) in docker-compose.yml A Docker Compose file that pulls the latest EdgeX container images from Nexus is available here: https://github.com/edgexfoundry/developer-scripts/blob/master/releases/nightly-build/compose-files/docker-compose-nexus.yml . If you are creating your own Docker Compose file or want to use and existing EdgeX Docker Compose file but selectively use Nexus images, replace the name/location of the Docker image in your docker-compose.yml file for the containers you want to get from Nexus versus Docker Hub. For example, the config-seed item in docker-compose.yml might ordinarily look like this: config - seed : ** image : edgexfoundry / docker - core - config - seed ** ports : \\ - \"8400:8400\" \\ \\ - \"8500:8500\" \\ \\ - \"8600:8600\" \\ container_name : edgex - config - seed hostname : edgex - core - config - seed networks : edgex - network : aliases : \\ - edgex - core - consul \\ volumes_from : \\ - volume \\ depends_on : \\ - volume \\ Change the \"image\" field to point to the Nexus Repos config - seed : ** image : nexus3 . edgexfoundry . org : 10004 / docker - core - config - seed ** ports : \\ - \"8400:8400\" \\ \\ - \"8500:8500\" \\ \\ - \"8600:8600\" \\ container_name : edgex - config - seed hostname : edgex - core - config - seed networks : edgex - network : aliases : \\ - edgex - core - consul \\ volumes_from : \\ - volume \\ depends_on : \\ - volume \\ Save the Docker Compose YAML file after you make any changes. Use the image(s) Now start your container(s) as you would normally with Docker Compose \\$ docker-compose up -d [container_name]","title":"Getting Docker Images from EdgeX Nexus Repository"},{"location":"getting-started/Ch-GettingStartedUsersNexus/#getting-docker-images-from-edgex-nexus-repository","text":"In some cases, it may be necessary to get your EdgeX container images from the Nexus Repository (managed by the Linux Foundation) as opposed to the Docker Hub repository. Nexus is a repository that contains all the staging and development containers and images for the EdgeX Foundry project. Containers destined for Docker Hub actually move through the Nexus Repository on their way to Docker Hub. Some reasons why you might want to use container images from the Nexus Repos might include when: a) the container is not available from Docker Hub (or Docker Hub is down temporarily) b) you need the latest development build container. c) you are working in a Windows or non-Linux environment and you are unable to build a container without some issues (Docker shell scripts may not work in Docker For Windows due to CR-LF on Git in Windows). In order to get containers from the Nexus Repository, follow these steps: Pull the container(s) Pull the container(s) from Nexus into your local environment. In the example below, the docker-core-config-seed container image is pulled from Nexus. Note the host name (nexus3.edgexfoundry.org) and port (10004) for pulling \\$ docker pull nexus3.edgexfoundry.org:10004/docker-core-config-seed Replace the image(s) in docker-compose.yml A Docker Compose file that pulls the latest EdgeX container images from Nexus is available here: https://github.com/edgexfoundry/developer-scripts/blob/master/releases/nightly-build/compose-files/docker-compose-nexus.yml . If you are creating your own Docker Compose file or want to use and existing EdgeX Docker Compose file but selectively use Nexus images, replace the name/location of the Docker image in your docker-compose.yml file for the containers you want to get from Nexus versus Docker Hub. For example, the config-seed item in docker-compose.yml might ordinarily look like this: config - seed : ** image : edgexfoundry / docker - core - config - seed ** ports : \\ - \"8400:8400\" \\ \\ - \"8500:8500\" \\ \\ - \"8600:8600\" \\ container_name : edgex - config - seed hostname : edgex - core - config - seed networks : edgex - network : aliases : \\ - edgex - core - consul \\ volumes_from : \\ - volume \\ depends_on : \\ - volume \\ Change the \"image\" field to point to the Nexus Repos config - seed : ** image : nexus3 . edgexfoundry . org : 10004 / docker - core - config - seed ** ports : \\ - \"8400:8400\" \\ \\ - \"8500:8500\" \\ \\ - \"8600:8600\" \\ container_name : edgex - config - seed hostname : edgex - core - config - seed networks : edgex - network : aliases : \\ - edgex - core - consul \\ volumes_from : \\ - volume \\ depends_on : \\ - volume \\ Save the Docker Compose YAML file after you make any changes. Use the image(s) Now start your container(s) as you would normally with Docker Compose \\$ docker-compose up -d [container_name]","title":"Getting Docker Images from EdgeX Nexus Repository"},{"location":"getting-started/quick-start/","text":"This guide will get EdgeX up and running on your development machine in as little as 5 minutes. We will will skip over lengthy descriptions for now, you can read up on those later. The goal here is to get you a working IoT Edge stack, from device to cloud, as simply as possible. Setup The fastest way to start running EdgeX is by using our pre-built Docker images. To use them you'll need to install the following: Docker https://docs.docker.com/install/ Docker Compose https://docs.docker.com/compose/install/ Running EdgeX Once you have Docker and Docker Compose installed, you need the configuration file for downloading and running the EdgeX Foundry docker containers. Download the latest docker-compose file here https://github.com/edgexfoundry/developer-scripts/raw/master/releases/fuji/compose-files/docker-compose-fuji.yml and save this as docker-compose.yml in your local directory. This file contains everything you need to deploy EdgeX with docker. First, use this command to download the EdgeX Foundry Docker images from Docker Hub: docker-compose pull Then start up all of the EdgeX Foundry microservices: docker-compose up -d Finally, verify that the EdgeX containers have all been started: docker-compose ps Note Initialization or seed containers, like config-seed, will have exited as their job is just to initialize the associated service and then exit. The following table captured the Default Service Ports. This table provides the default ports used by each of the EdgeX micro services (per its default configuration and the EdgeX provided docker-compose files). These default ports are also used in the EdgeX provided service routes defined in the Kong API Gateway for access control. Services Name Port Definition consul 8400 8500 8600 vault 8200 kong-db 5432 kong 8000 8001 8443 8444 mongo 27017 redis 6379 system 48090 core-data 48080 5563 core-metadata 48001 core-command 48082 support-notifications 48060 support-logging 48061 support-scheduler 48085 device-virtual 49990 device-random 49988 device-mqtt 49982 device-modbus 49991 device-snmp 49993 UI 4000 Connecting a Device EdgeX Foundry provides a Random Number Device Service which is useful to testing, it returns a random number within a configurable range. Configuration for running this service was included in the docker-compose.yml file you downloaded at the start of this guide, but it is disabled by default. To enable it, uncomment the following lines in your docker-compose.yml : device-random : image : edgexfoundry/docker-device-random-go:1.1.0 ports : - \"49988:49988\" container_name : edgex-device-random hostname : edgex-device-random networks : - edgex-network volumes : - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data depends_on : - data - command Then you can start the Random Device Service with: docker-compose up -d device-random The Device Service will automatically register a device named Random-Integer-Generator01 , which will start sending its random number readings into EdgeX. You can verify that those readings are being sent by querying the EdgeX Logging service: curl http://localhost:48080/api/v1/event/device/Random-Integer-Generator01/10 Connecting an Application EdgeX provides exporters for a variety of cloud services and application. To keep this guide simple, we're going to use a public MQTT broker hosted by HiveMQ, then watch for our EdgeX readings to be pushed to it automatically. You can connect to this broker with any MQTT client to watch the data being sent. HiveMQ provides a web-based client that you can use, simply subscribe to the \"EdgeXQuickStartGuide\" topic and you will begin seeing your random number readings. You can also use the Mosquitto CLI <https://mosquitto.org/download/> _ tool to verify that readings are being sent by running: mosquitto_sub -h broker.hivemq.com -p 1883 -t EdgeXQuickStartGuide Controlling the Device Reading data from devices is only part of what EdgeX is capable of, you can also use it to control your devices. When a device is registered with the EdgeX services, it provides a Device Profile that describes both the data readings available from that device, and also the commands that can be called to control it. When our Random Number Device Service registered the device Random-Integer-Generator01 , it used a profile which defines commands for changing the minimum and maximum values for the random numbers it will generate. You won't call commands on devices directly, instead you use the EdgeX Foundry Command Service to do that. The first step is to check what commands are available to be called by asking the Command service about your device: curl http://localhost:48082/api/v1/device/name/Random-Integer-Generator01 This will return a lot of JSON, because there are a number of commands you can call on this device, but the one we're going to try in this guide in will look something like this: { \"created\" : 1544456741615 , \"modified\" : 0 , \"origin\" : 0 , \"id\" : \"5c0e8a259f8fc20001a5d22b\" , \"name\" : \"GenerateRandomValue_Int8\" , \"get\" :{ \"path\" : \"/api/v1/device/{deviceId}/GenerateRandomValue_Int8\" , \"responses\" :[ { \"code\" : \"200\" , \"description\" : null , \"expectedValues\" :[ \"RandomValue_Int8\" ] }, { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" :[ ] } ], \"url\" : \"http://edgex-core-command:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b\" }, \"put\" :{ \"path\" : \"/api/v1/device/{deviceId}/GenerateRandomValue_Int8\" , \"responses\" :[ { \"code\" : \"200\" , \"description\" : null , \"expectedValues\" :[] }, { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" :[] } ], \"parameterNames\" :[ \"Min_Int8\" , \"Max_Int8\" ], \"url\" : \"http://edgex-core-command:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b\" } } Note The URLs won't be exactly the same for you, as the generated unique IDs for both the Device and the Command will be different. So be sure to use your values for the following steps. You'll notice that this one command has both a get and a put option. The get call will return a random number, and is what is being called automatically to send data into the rest of EdgeX. You can also call it manually using the get URL provided: curl http://localhost:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b Notice that I replaced edgex-core-command with localhost here. That's because the EdgeX Foundry services are running in docker, which recognizes the internal hostname edgex-core-command , but I'm calling it from outside of docker, so I have to use localhost to reach it. This command will return a JSON result that looks like this: { \"id\" : \"\" , \"pushed\" : 0 , \"device\" : \"Random-Integer-Generator01\" , \"created\" : 0 , \"modified\" : 0 , \"origin\" : 1544457033233 , \"schedule\" : null , \"event\" : null , \"readings\" :[ { \"id\" : \"\" , \"pushed\" : 0 , \"created\" : 0 , \"origin\" : 1544457033233 , \"modified\" : 0 , \"device\" : \"Random-Integer-Generator01\" , \"name\" : \"RandomValue_Int8\" , \"value\" : \"-92\" } ] } The default range for this reading is -128 to 127. We can limit that to only positive values between 0 and 100 by calling the command as a put method with new minimum and maximum values: curl -X PUT -d '[ {\"Min_Int8\": \"0\", \"Max_Int8\": \"100\"} ]' http://localhost:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b Note You can make multiple requests to a Command with a single call, so your parameters need to be in an array (surrounded by [ ] ) as shown above. The parameter names were given in the PUT section of the Command definition we queried at the start of this section.* Now every time we call GET on this command, the returned value will be between 0 and 100. Next Steps Congratulations! You now have a full EdgeX deployment reading data from a (virtual) device and publishing it to the cloud, and you were able to control your device through commands into EdgeX. It's time to continue your journey by reading the Introduction to EdgeX Foundry, what it is and how it's built. From there you can take the Walkthrough to learn how the microservices work together to control devices and read data from them as you just did.","title":"Quick Start"},{"location":"getting-started/quick-start/#setup","text":"The fastest way to start running EdgeX is by using our pre-built Docker images. To use them you'll need to install the following: Docker https://docs.docker.com/install/ Docker Compose https://docs.docker.com/compose/install/","title":"Setup"},{"location":"getting-started/quick-start/#running-edgex","text":"Once you have Docker and Docker Compose installed, you need the configuration file for downloading and running the EdgeX Foundry docker containers. Download the latest docker-compose file here https://github.com/edgexfoundry/developer-scripts/raw/master/releases/fuji/compose-files/docker-compose-fuji.yml and save this as docker-compose.yml in your local directory. This file contains everything you need to deploy EdgeX with docker. First, use this command to download the EdgeX Foundry Docker images from Docker Hub: docker-compose pull Then start up all of the EdgeX Foundry microservices: docker-compose up -d Finally, verify that the EdgeX containers have all been started: docker-compose ps Note Initialization or seed containers, like config-seed, will have exited as their job is just to initialize the associated service and then exit. The following table captured the Default Service Ports. This table provides the default ports used by each of the EdgeX micro services (per its default configuration and the EdgeX provided docker-compose files). These default ports are also used in the EdgeX provided service routes defined in the Kong API Gateway for access control. Services Name Port Definition consul 8400 8500 8600 vault 8200 kong-db 5432 kong 8000 8001 8443 8444 mongo 27017 redis 6379 system 48090 core-data 48080 5563 core-metadata 48001 core-command 48082 support-notifications 48060 support-logging 48061 support-scheduler 48085 device-virtual 49990 device-random 49988 device-mqtt 49982 device-modbus 49991 device-snmp 49993 UI 4000","title":"Running EdgeX"},{"location":"getting-started/quick-start/#connecting-a-device","text":"EdgeX Foundry provides a Random Number Device Service which is useful to testing, it returns a random number within a configurable range. Configuration for running this service was included in the docker-compose.yml file you downloaded at the start of this guide, but it is disabled by default. To enable it, uncomment the following lines in your docker-compose.yml : device-random : image : edgexfoundry/docker-device-random-go:1.1.0 ports : - \"49988:49988\" container_name : edgex-device-random hostname : edgex-device-random networks : - edgex-network volumes : - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data depends_on : - data - command Then you can start the Random Device Service with: docker-compose up -d device-random The Device Service will automatically register a device named Random-Integer-Generator01 , which will start sending its random number readings into EdgeX. You can verify that those readings are being sent by querying the EdgeX Logging service: curl http://localhost:48080/api/v1/event/device/Random-Integer-Generator01/10","title":"Connecting a Device"},{"location":"getting-started/quick-start/#connecting-an-application","text":"EdgeX provides exporters for a variety of cloud services and application. To keep this guide simple, we're going to use a public MQTT broker hosted by HiveMQ, then watch for our EdgeX readings to be pushed to it automatically. You can connect to this broker with any MQTT client to watch the data being sent. HiveMQ provides a web-based client that you can use, simply subscribe to the \"EdgeXQuickStartGuide\" topic and you will begin seeing your random number readings. You can also use the Mosquitto CLI <https://mosquitto.org/download/> _ tool to verify that readings are being sent by running: mosquitto_sub -h broker.hivemq.com -p 1883 -t EdgeXQuickStartGuide","title":"Connecting an Application"},{"location":"getting-started/quick-start/#controlling-the-device","text":"Reading data from devices is only part of what EdgeX is capable of, you can also use it to control your devices. When a device is registered with the EdgeX services, it provides a Device Profile that describes both the data readings available from that device, and also the commands that can be called to control it. When our Random Number Device Service registered the device Random-Integer-Generator01 , it used a profile which defines commands for changing the minimum and maximum values for the random numbers it will generate. You won't call commands on devices directly, instead you use the EdgeX Foundry Command Service to do that. The first step is to check what commands are available to be called by asking the Command service about your device: curl http://localhost:48082/api/v1/device/name/Random-Integer-Generator01 This will return a lot of JSON, because there are a number of commands you can call on this device, but the one we're going to try in this guide in will look something like this: { \"created\" : 1544456741615 , \"modified\" : 0 , \"origin\" : 0 , \"id\" : \"5c0e8a259f8fc20001a5d22b\" , \"name\" : \"GenerateRandomValue_Int8\" , \"get\" :{ \"path\" : \"/api/v1/device/{deviceId}/GenerateRandomValue_Int8\" , \"responses\" :[ { \"code\" : \"200\" , \"description\" : null , \"expectedValues\" :[ \"RandomValue_Int8\" ] }, { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" :[ ] } ], \"url\" : \"http://edgex-core-command:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b\" }, \"put\" :{ \"path\" : \"/api/v1/device/{deviceId}/GenerateRandomValue_Int8\" , \"responses\" :[ { \"code\" : \"200\" , \"description\" : null , \"expectedValues\" :[] }, { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" :[] } ], \"parameterNames\" :[ \"Min_Int8\" , \"Max_Int8\" ], \"url\" : \"http://edgex-core-command:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b\" } } Note The URLs won't be exactly the same for you, as the generated unique IDs for both the Device and the Command will be different. So be sure to use your values for the following steps. You'll notice that this one command has both a get and a put option. The get call will return a random number, and is what is being called automatically to send data into the rest of EdgeX. You can also call it manually using the get URL provided: curl http://localhost:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b Notice that I replaced edgex-core-command with localhost here. That's because the EdgeX Foundry services are running in docker, which recognizes the internal hostname edgex-core-command , but I'm calling it from outside of docker, so I have to use localhost to reach it. This command will return a JSON result that looks like this: { \"id\" : \"\" , \"pushed\" : 0 , \"device\" : \"Random-Integer-Generator01\" , \"created\" : 0 , \"modified\" : 0 , \"origin\" : 1544457033233 , \"schedule\" : null , \"event\" : null , \"readings\" :[ { \"id\" : \"\" , \"pushed\" : 0 , \"created\" : 0 , \"origin\" : 1544457033233 , \"modified\" : 0 , \"device\" : \"Random-Integer-Generator01\" , \"name\" : \"RandomValue_Int8\" , \"value\" : \"-92\" } ] } The default range for this reading is -128 to 127. We can limit that to only positive values between 0 and 100 by calling the command as a put method with new minimum and maximum values: curl -X PUT -d '[ {\"Min_Int8\": \"0\", \"Max_Int8\": \"100\"} ]' http://localhost:48082/api/v1/device/5c0e8a259f8fc20001a5d230/command/5c0e8a259f8fc20001a5d22b Note You can make multiple requests to a Command with a single call, so your parameters need to be in an array (surrounded by [ ] ) as shown above. The parameter names were given in the PUT section of the Command definition we queried at the start of this section.* Now every time we call GET on this command, the returned value will be between 0 and 100.","title":"Controlling the Device"},{"location":"getting-started/quick-start/#next-steps","text":"Congratulations! You now have a full EdgeX deployment reading data from a (virtual) device and publishing it to the cloud, and you were able to control your device through commands into EdgeX. It's time to continue your journey by reading the Introduction to EdgeX Foundry, what it is and how it's built. From there you can take the Walkthrough to learn how the microservices work together to control devices and read data from them as you just did.","title":"Next Steps"},{"location":"microservices/application/Ch-AppServiceConfigurable/","text":"App Service Configurable App-Service-Configurable is provided as an easy way to get started with processing data flowing through EdgeX. This service leverages the App Functions SDK and provides a way for developers to use configuration instead of having to compile standalone services to utilize built in functions in the SDK. For a full list of supported/built-in functions view the README located in the App Functions SDK repository. Getting Started Deploying Multiple Instances Input Data Not An EdgeX Event Environment Variable Overrides For Docker Getting Started To get started with the configurable app service, you'll want to start by determining which functions are required in your pipeline. Using a simple example. let's assume you wish to use the following functions from the SDK: 1) FilterByDeviceName - to filter events for a specific device. 2) TransformToXML - to transform the data to XML 3) HTTPPost - to send the data to an HTTP endpoint that takes our XML data 4) MarkAsPushed - to call Core Data API to mark the event as having been pushed Once the functions have been identified, we'll go ahead and build out the configuration in the configuration.toml file under the [Writable.Pipeline] section: [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] ExecutionOrder = \"FilterByDeviceName,TransformToXML,HTTPPost,MarkAsPushed\" [Writable.Pipeline.Functions.FilterByDeviceName] [Writable.Pipeline.Functions.FilterByDeviceName.Parameters] FilterValues = \"Random-Float-Device,Random-Integer-Device\" [Writable.Pipeline.Functions.TransformToXML] [Writable.Pipeline.Functions.MarkAsPushed] [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" mimeType = \"\" #OPTIONAL - default application/json The first line of note is ExecutionOrder = \"FilterByDeviceName,TransformToXML,HTTPPost,MarkAsPushed\" . This specifies the order in which to execute your functions. Each function specified here must also be placed in the [Writeable.Pipeline.Functions] section. Next, each function and its required information is listed. Each function typically has associated Parameters that must be configured to properly execute the function as designated by [Writable.Pipeline.Functions.{FunctionName}.Parameters] . Knowing which parameters are required for each function, can be referenced by taking a look at the documentation located here . In a few cases, such as TransformToXML , TransformToJSON , or SetOutputData , there are no parameters required. Note: By default, the configuration provided is set to use MessageBus as a trigger from CoreData. This means you must have EdgeX Running with devices sending data in order to trigger the pipeline. You can also change the trigger to be HTTP. For more on triggers, view the documentation here . That's it! Now we can run/deploy this service and the functions pipeline will process the data with functions we've defined. Deploying Multiple Instances What if I want to deploy multiple pipelines using this service? It not uncommon to have different sets of function pipelines that need to be deployed. This is where --profiles comes in handy. You can create different \"profile\" folders inside the /res directory with different configurations. This is typically used in other EdgeX services for having a separate configuration for Docker based deployments and native deployments. Since the pipeline is specified in the configuration.toml file, we can use this as a way to run the app-service-configurable application with different profiles by specifying the \"--profile=http-export\" and \"--confdir=/res\" as command line options when deploying. If running with Registry enabled, the service key used will be AppService-[profile name] , e.g AppService-http-export when using --profile option or just AppService if not using the --profile option. Input Data Not An EdgeX Event What if my input data isn't an EdgeX Event ? The default TargetType for data flowing into the functions pipeline is an EdgeX event. There are cases when this incoming data might not be an EdgeX event. In these cases the Pipeline can be configured using UseTargetTypeOfByteArray=true to set the TargetType to be a byte array, i.e. byte[] . The first function in the pipeline must then be one that can handle the byte[] data. The compression , encryption and export functions are examples of pipeline functions that will take input data that is byte[] . Here is an example of how to configure the functions pipeline to compress , encrypt and then export the byte[] data via HTTP. [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] UseTargetTypeOfByteArray = true ExecutionOrder = \"CompressWithGZIP, EncryptWithAES, HTTPPost\" [Writable.Pipeline.Functions.CompressWithGZIP] [Writable.Pipeline.Functions.EncryptWithAES] [Writable.Pipeline.Functions.EncryptWithAES.Parameters] Key = \"aquqweoruqwpeoruqwpoeruqwpoierupqoweiurpoqwiuerpqowieurqpowieurpoqiweuroipwqure\" InitVector = \"123456789012345678901234567890\" [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" If along with this pipeline configuration, you also configured the Binding to be http trigger, you could then send any data to the app-service-configurable' s /api/v1/trigger endpoint and have it compressed, encrypted and sent to your configured URL above. [Binding] Type = \"http\" Environment Variable Overrides For Docker App Service Configurable no longer has docker specific profiles. It now relies on environment variable overrides in the docker compose files for the docker specific differences. The following environment settings are required in the compose files when using App Service Configurable. edgex_registry : consul : //edgex-core-consul:8500 edgex_profile : [ target profile ] edgex_service : http : //[service name]:[port] Service_Host : [ service name ] Clients_CoreData_Host : edgex - core - data Clients_Logging_Host : edgex - support - logging Logging_EnableRemote : \"true\" Database_Host : edgex - mongo Database_Username : appservice Database_Password : password The following is an example docker compose entry for App Service Configurable : app-service-configurable-rules : image : edgexfoundry/docker-app-service-configurable:1.1.0 environment : edgex_registry : consul://edgex-core-consul:8500 edgex_service : http://edgex-app-service-configurable-rules:48096 edgex_profile : rules-engine Service_Host : edgex-app-service-configurable-rules Clients_CoreData_Host : edgex-core-data Clients_Logging_Host : edgex-support-logging Logging_EnableRemote : \"true\" MessageBus_SubscribeHost_Host : edgex-core-data ports : - \"48096:48096\" container_name : edgex-app-service-configurable-rules hostname : edgex-app-service-configurable-rules networks : edgex-network : aliases : - edgex-app-service-configurable-rules depends_on : - data - command","title":"App Service Configurable"},{"location":"microservices/application/Ch-AppServiceConfigurable/#app-service-configurable","text":"App-Service-Configurable is provided as an easy way to get started with processing data flowing through EdgeX. This service leverages the App Functions SDK and provides a way for developers to use configuration instead of having to compile standalone services to utilize built in functions in the SDK. For a full list of supported/built-in functions view the README located in the App Functions SDK repository. Getting Started Deploying Multiple Instances Input Data Not An EdgeX Event Environment Variable Overrides For Docker","title":"App Service Configurable"},{"location":"microservices/application/Ch-AppServiceConfigurable/#getting-started","text":"To get started with the configurable app service, you'll want to start by determining which functions are required in your pipeline. Using a simple example. let's assume you wish to use the following functions from the SDK: 1) FilterByDeviceName - to filter events for a specific device. 2) TransformToXML - to transform the data to XML 3) HTTPPost - to send the data to an HTTP endpoint that takes our XML data 4) MarkAsPushed - to call Core Data API to mark the event as having been pushed Once the functions have been identified, we'll go ahead and build out the configuration in the configuration.toml file under the [Writable.Pipeline] section: [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] ExecutionOrder = \"FilterByDeviceName,TransformToXML,HTTPPost,MarkAsPushed\" [Writable.Pipeline.Functions.FilterByDeviceName] [Writable.Pipeline.Functions.FilterByDeviceName.Parameters] FilterValues = \"Random-Float-Device,Random-Integer-Device\" [Writable.Pipeline.Functions.TransformToXML] [Writable.Pipeline.Functions.MarkAsPushed] [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" mimeType = \"\" #OPTIONAL - default application/json The first line of note is ExecutionOrder = \"FilterByDeviceName,TransformToXML,HTTPPost,MarkAsPushed\" . This specifies the order in which to execute your functions. Each function specified here must also be placed in the [Writeable.Pipeline.Functions] section. Next, each function and its required information is listed. Each function typically has associated Parameters that must be configured to properly execute the function as designated by [Writable.Pipeline.Functions.{FunctionName}.Parameters] . Knowing which parameters are required for each function, can be referenced by taking a look at the documentation located here . In a few cases, such as TransformToXML , TransformToJSON , or SetOutputData , there are no parameters required. Note: By default, the configuration provided is set to use MessageBus as a trigger from CoreData. This means you must have EdgeX Running with devices sending data in order to trigger the pipeline. You can also change the trigger to be HTTP. For more on triggers, view the documentation here . That's it! Now we can run/deploy this service and the functions pipeline will process the data with functions we've defined.","title":"Getting Started"},{"location":"microservices/application/Ch-AppServiceConfigurable/#deploying-multiple-instances","text":"What if I want to deploy multiple pipelines using this service? It not uncommon to have different sets of function pipelines that need to be deployed. This is where --profiles comes in handy. You can create different \"profile\" folders inside the /res directory with different configurations. This is typically used in other EdgeX services for having a separate configuration for Docker based deployments and native deployments. Since the pipeline is specified in the configuration.toml file, we can use this as a way to run the app-service-configurable application with different profiles by specifying the \"--profile=http-export\" and \"--confdir=/res\" as command line options when deploying. If running with Registry enabled, the service key used will be AppService-[profile name] , e.g AppService-http-export when using --profile option or just AppService if not using the --profile option.","title":"Deploying Multiple Instances"},{"location":"microservices/application/Ch-AppServiceConfigurable/#input-data-not-an-edgex-event","text":"What if my input data isn't an EdgeX Event ? The default TargetType for data flowing into the functions pipeline is an EdgeX event. There are cases when this incoming data might not be an EdgeX event. In these cases the Pipeline can be configured using UseTargetTypeOfByteArray=true to set the TargetType to be a byte array, i.e. byte[] . The first function in the pipeline must then be one that can handle the byte[] data. The compression , encryption and export functions are examples of pipeline functions that will take input data that is byte[] . Here is an example of how to configure the functions pipeline to compress , encrypt and then export the byte[] data via HTTP. [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] UseTargetTypeOfByteArray = true ExecutionOrder = \"CompressWithGZIP, EncryptWithAES, HTTPPost\" [Writable.Pipeline.Functions.CompressWithGZIP] [Writable.Pipeline.Functions.EncryptWithAES] [Writable.Pipeline.Functions.EncryptWithAES.Parameters] Key = \"aquqweoruqwpeoruqwpoeruqwpoierupqoweiurpoqwiuerpqowieurqpowieurpoqiweuroipwqure\" InitVector = \"123456789012345678901234567890\" [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" If along with this pipeline configuration, you also configured the Binding to be http trigger, you could then send any data to the app-service-configurable' s /api/v1/trigger endpoint and have it compressed, encrypted and sent to your configured URL above. [Binding] Type = \"http\"","title":"Input Data Not An EdgeX Event"},{"location":"microservices/application/Ch-AppServiceConfigurable/#environment-variable-overrides-for-docker","text":"App Service Configurable no longer has docker specific profiles. It now relies on environment variable overrides in the docker compose files for the docker specific differences. The following environment settings are required in the compose files when using App Service Configurable. edgex_registry : consul : //edgex-core-consul:8500 edgex_profile : [ target profile ] edgex_service : http : //[service name]:[port] Service_Host : [ service name ] Clients_CoreData_Host : edgex - core - data Clients_Logging_Host : edgex - support - logging Logging_EnableRemote : \"true\" Database_Host : edgex - mongo Database_Username : appservice Database_Password : password The following is an example docker compose entry for App Service Configurable : app-service-configurable-rules : image : edgexfoundry/docker-app-service-configurable:1.1.0 environment : edgex_registry : consul://edgex-core-consul:8500 edgex_service : http://edgex-app-service-configurable-rules:48096 edgex_profile : rules-engine Service_Host : edgex-app-service-configurable-rules Clients_CoreData_Host : edgex-core-data Clients_Logging_Host : edgex-support-logging Logging_EnableRemote : \"true\" MessageBus_SubscribeHost_Host : edgex-core-data ports : - \"48096:48096\" container_name : edgex-app-service-configurable-rules hostname : edgex-app-service-configurable-rules networks : edgex-network : aliases : - edgex-app-service-configurable-rules depends_on : - data - command","title":"Environment Variable Overrides For Docker"},{"location":"microservices/application/Ch-ApplServices/","text":"Application Services Microservices Application Services are the means to extract, process/transform and send event/reading data from EdgeX to an endpoint or process of your choice. Application Services are based on the idea of a \"Functions Pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event/reading messages) in the order that you've specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger is something like a message landing in a watched message queue. An SDK is provided (the Application Functions SDK) to help build Application Services by assembling triggers, pre-existing functions and custom functions of your making into a pipeline. Note Application Services will replace Export Services in a future EdgeX release.","title":"Application Services Microservices"},{"location":"microservices/application/Ch-ApplServices/#application-services-microservices","text":"Application Services are the means to extract, process/transform and send event/reading data from EdgeX to an endpoint or process of your choice. Application Services are based on the idea of a \"Functions Pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event/reading messages) in the order that you've specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger is something like a message landing in a watched message queue. An SDK is provided (the Application Functions SDK) to help build Application Services by assembling triggers, pre-existing functions and custom functions of your making into a pipeline. Note Application Services will replace Export Services in a future EdgeX release.","title":"Application Services Microservices"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/","text":"App Functions SDK (Golang) - Beta Welcome the App Functions SDK for EdgeX. This sdk is meant to provide all the plumbing necessary for developers to get started in processing/transforming/exporting data out of EdgeX. Table of contents Getting Started Triggers Context API Built-In Functions Filtering Encryption Conversion Compressions Core Data Export Functions Configuration Error Handling Advanced Topics Target Type Command Line Options Environment Variable Overrides Store and Forward Getting Started Build Prerequisites Please see the edgex-go README . The SDK The SDK is built around the idea of a \"Functions Pipeline\". A functions pipeline is a collection of various functions that process the data in the order that you've specified. The functions pipeline is executed by the specified trigger in the configuration.toml . The first function in the pipeline is called with the event that triggered the pipeline (ex. events.Model ). Each successive call in the pipeline is called with the return result of the previous function. Let's take a look at a simple example that creates a pipeline to filter particular device ids and subsequently transform the data to XML: package main import ( \"fmt\" \"github.com/edgexfoundry/app-functions-sdk-go/appsdk\" \"github.com/edgexfoundry/app-functions-sdk-go/pkg/transforms\" \"os\" ) func main () { // 1 ) First thing to do is to create an instance of the EdgeX SDK , giving it a service key edgexSdk : = & appsdk . AppFunctionsSDK { ServiceKey : \"SimpleFilterXMLApp\" , // Key used by Registry ( Aka Consul ) } // 2 ) Next , we need to initialize the SDK if err : = edgexSdk . Initialize (); err != nil { edgexSdk . LoggingClient . Error ( fmt . Sprintf ( \"SDK initialization failed: %v \\n \" , err )) os . Exit ( - 1 ) } // 3 ) Since our FilterByDeviceName Function requires the list of Device Names we would // like to search for , we 'll go ahead and define that now. deviceNames : = [] string { \"Random-Float-Device\" } // 4 ) This is our pipeline configuration , the collection of functions to // execute every time an event is triggered . if err : = edgexSdk . SetFunctionsPipeline ( transforms . NewFilter ( deviceNames ) . FilterByDeviceName , transforms . NewConversion () . TransformToXML , ); err != nil { edgexSdk . LoggingClient . Error ( fmt . Sprintf ( \"SDK SetPipeline failed: %v \\n \" , err )) os . Exit ( - 1 ) } // 5 ) shows how to access the application 's specific configuration settings. appSettings : = edgexSdk . ApplicationSettings () if appSettings != nil { appName , ok : = appSettings [ \"ApplicationName\" ] if ok { edgexSdk . LoggingClient . Info ( fmt . Sprintf ( \" %s now running...\" , appName )) } else { edgexSdk . LoggingClient . Error ( \"ApplicationName application setting not found\" ) os . Exit ( - 1 ) } } else { edgexSdk . LoggingClient . Error ( \"No application settings found\" ) os . Exit ( - 1 ) } // 6 ) Lastly , we 'll go ahead and tell the SDK to \"start\" and begin listening for events to trigger the pipeline. edgexSdk . MakeItRun () } The above example is meant to merely demonstrate the structure of your application. Notice that the output of the last function is not available anywhere inside this application. You must provide a function in order to work with the data from the previous function. Let's go ahead and add the following function that prints the output to the console. func printXMLToConsole(edgexcontext *appcontext.Context, params ...interface{}) (bool,interface{}) { if len(params) < 1 { // We didn't receive a result return false, errors.New(\"No Data Received\") } println(params[0].(string)) return true, nil } After placing the above function in your code, the next step is to modify the pipeline to call this function: edgexSdk.SetFunctionsPipeline( transforms.NewFilter(deviceNames).FilterByDeviceName, transforms.NewConversion().TransformToXML, printXMLToConsole //notice this is not a function call, but simply a function pointer. ) After making the above modifications, you should now see data printing out to the console in XML when an event is triggered. > You can find this example in the /examples directory located in this repository. You can also use the provided EdgeX Applications Function SDK.postman\\_collection.json file to load into postman to trigger the sample pipeline. Up until this point, the pipeline has been triggered by an event over HTTP and the data at the end of that pipeline lands in the last function specified. In the example, data ends up printed to the console. Perhaps we'd like to send the data back to where it came from. In the case of an HTTP trigger, this would be the HTTP response. In the case of a message bus, this could be a new topic to send the data back to for other applications that wish to receive it. To do this, simply call edgexcontext.Complete([]byte outputData) passing in the data you wish to \"respond\" with. In the above printXMLToConsole(...) function, replace println(params[0].(string)) with edgexcontext.Complete([]byte(params[0].(string))) . You should now see the response in your postman window when testing the pipeline. Examples The App Service Examples repo contains a variety of simple to advanced example Application Services built upon the App Functions SDK . Examples that once were in the examples folder of the SDK have been moved to this Examples repo. Triggers Triggers determine how the app functions pipeline begins execution. In the simple example provided above, an HTTP trigger is used. The trigger is determine by the configuration.toml file located in the /res directory under a section called [Binding] . Check out the Configuration Section for more information about the toml file. Message Bus Trigger A message bus trigger will execute the pipeline every time data is received off of the configured topic. Type and Topic configuration Here's an example: Type = \"messagebus\" SubscribeTopic = \"events\" PublishTopic = \"\" The Type= is set to \"messagebus\". EdgeX Core Data is publishing data to the events topic. So to receive data from core data, you can set your SubscribeTopic= either to \"\" or \"events\" . You may also designate a PublishTopic= if you wish to publish data back to the message bus. edgexcontext.Complete([]byte outputData) - Will send data back to back to the message bus with the topic specified in the PublishTopic= property #### Message bus connection configuration The other piece of configuration required are the connection settings: [MessageBus] Type = 'zero' #specifies of message bus (i.e zero for ZMQ) [MessageBus.PublishHost] Host = '*' Port = 5564 Protocol = 'tcp' [MessageBus.SubscribeHost] Host = 'localhost' Port = 5563 Protocol = 'tcp' By default, EdgeX Core Data publishes data to the events topic on port 5563. The publish host is used if publishing data back to the message bus. > Important Note: Publish Host MUST be different for every topic you wish to publish to since the SDK will bind to the specific port. 5563 for example cannot be used to publish since EdgeX Core Data has bound to that port. Similarly, you cannot have two separate instances of the app functions SDK running publishing to the same port. HTTP Trigger Designating an HTTP trigger will allow the pipeline to be triggered by a RESTful POST call to http://[host]:[port]/trigger/ . The body of the POST must be an EdgeX event. edgexcontext.Complete([]byte outputData) - Will send the specified data as the response to the request that originally triggered the HTTP Request. Context API The context parameter passed to each function/transform provides operations and data associated with each execution of the pipeline. Let's take a look at a few of the properties that are available: type Context struct { // ID of the EdgeX Event -- will be filled for a received JSON Event EventID string // Checksum of the EdgeX Event -- will be filled for a received CBOR Event EventChecksum string // This is the ID used to track the EdgeX event through entire EdgeX framework . CorrelationID string // OutputData is used for specifying the data that is to be outputted . Leverage the . Complete () function to set . OutputData [] byte // This holds the configuration for your service . This is the preferred way to access your custom application settings that have been set in the configuration . Configuration common . ConfigurationStruct // LoggingClient is exposed to allow logging following the preferred logging strategy within EdgeX . LoggingClient logger . LoggingClient // EventClient exposes Core Data 's EventClient API EventClient coredata.EventClient // ValueDescriptorClient exposes Core Data' s ValueDescriptor API ValueDescriptorClient coredata . ValueDescriptorClient // CommandClient exposes Core Commands 's Command API CommandClient command.CommandClient // NotificationsClient exposes Support Notification' s Notifications API NotificationsClient notifications . NotificationsClient // RetryData holds the data to be stored for later retry when the pipeline function returns an error RetryData [] byte } LoggingClient The LoggingClient exposed on the context is available to leverage logging libraries/service utilized throughout the EdgeX framework. The SDK has initialized everything so it can be used to log Trace , Debug , Warn , Info , and Error messages as appropriate. See examples/simple-filter-xml/main.go for an example of how to use the LoggingClient . EventClient The EventClient exposed on the context is available to leverage Core Data's Event API. See interface definition for more details. This client is useful for querying events and is used by the MarkAsPushed convenience API described below. ValueDescriptorClient The ValueDescriptorClient exposed on the context is available to leverage Core Data's ValueDescriptor API. See interface definition for more details. Useful for looking up the value descriptor for a reading received. CommandClient The CommandClient exposed on the context is available to leverage Core Command's Command API. See interface definition for more details. Useful for sending commands to devices. NotificationsClient The CommandClient exposed on the context is available to leverage Support Notifications' Notifications API. See README for more details. Useful for sending notifications. Note about Clients Each of the clients above is only initialized if the Clients section of the configuration contains an entry for the service associated with the Client API. If it isn't in the configuration the client will be nil . Your code must check for nil to avoid panic in case it is missing from the configuration. Only add the clients to your configuration that your Application Service will actually be using. All application services need the Logging and many will need Core-Data . The following is an example Clients section of a configuration.toml with all supported clients specified: [Clients] [Clients.Logging] Protocol = \"http\" Host = \"localhost\" Port = 48061 [Clients.CoreData] Protocol = 'http' Host = 'localhost' Port = 48080 [Clients.Command] Protocol = 'http' Host = 'localhost' Port = 48082 [Clients.Notifications] Protocol = 'http' Host = 'localhost' Port = 48060 .MarkAsPushed() .MarkAsPushed() is used to indicate to EdgeX Core Data that an event has been \"pushed\" and is no longer required to be stored. The scheduler service will purge all events that have been marked as pushed based on the configured schedule. By default, it is once daily at midnight. If you leverage the built in export functions (i.e. HTTP Export, or MQTT Export), then the event will automatically be marked as pushed upon a successful export. ### .PushToCore() .PushToCore(string deviceName, string readingName, byte[] value) is used to push data to EdgeX Core Data so that it can be shared with other applications that are subscribed to the message bus that core-data publishes to. deviceName can be set as you like along with the readingName which will be set on the EdgeX event sent to CoreData. This function will return the new EdgeX Event with the ID populated, however the CorrelationId will not be available. NOTE: If validation is turned on in CoreServices then your deviceName and readingName must exist in the CoreMetadata and be properly registered in EdgeX. .SetRetryData() .SetRetryData(payload []byte) can be used to store data for later retry. This is useful when creating a custom export function that needs to retry on failure sending the data. The payload data will be stored for later retry based on Store and Forward configuration. When the retry is triggered, the function pipeline will be re-executed starting with the function that called this API. That function will be passed the stored data, so it is important that all transformations occur in functions prior to the export function. The Context will also be restored to the state when the function called this API. See Store and Forward for more details. NOTE: Store and Forward be must enabled when calling this API. Built-In Transforms/Functions All transforms define a type and a New function which is used to initialize an instance of the type with the required parameters. These instances returned by these New functions give access to their appropriate pipeline function pointers when build the function pipeline. E.G. NewFilter([] {\"Device1\", \"Device2\"}).FilterByDeviceName Filtering There are two basic types of filtering included in the SDK to add to your pipeline. Theses provided Filter functions return a type of events.Model. If filtering results in no remaining data, the pipeline execution for that pass is terminated. If no values are provided for filtering, then data flows through unfiltered. - NewFilter([]string filterValues) - This function returns a Filter instance initialized with the passed in filter values. This Filter instance is used to access the following filter functions that will operate using the specified filter values. - FilterByDeviceName - This function will filter the event data down to the specified device names and return the filtered data to the pipeline. - FilterByValueDescriptor - This function will filter the event data down to the specified device value descriptor and return the filtered data to the pipeline. Encryption There is one encryption transform included in the SDK that can be added to your pipeline. NewEncryption(key string, initializationVector string) - This function returns a Encryption instance initialized with the passed in key and initialization vector. This Encryption instance is used to access the following encryption function that will use the specified key and initialization vector. EncryptWithAES - This function receives a either a string , []byte , or json.Marshaller type and encrypts it using AES encryption and returns a []byte to the pipeline. Conversion There are two conversions included in the SDK that can be added to your pipeline. These transforms return a string . NewConversion() - This function returns a Conversion instance that is used to access the following conversion functions: TransformToXML - This function receives an events.Model type, converts it to XML format and returns the XML string to the pipeline. TransformToJSON - This function receives an events.Model type and converts it to JSON format and returns the JSON string to the pipeline. ### Compressions There are two compression types included in the SDK that can be added to your pipeline. These transforms return a []byte . NewCompression() - This function returns a Compression instance that is used to access the following compression functions: CompressWithGZIP - This function receives either a string , []byte , or json.Marshaler type, GZIP compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. CompressWithZLIB - This function receives either a string , []byte , or json.Marshaler type, ZLIB compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. CoreData Functions These are functions that enable interactions with the CoreData REST API. - NewCoreData() - This function returns a CoreData instance. This CoreData instance is used to access the following function(s). - MarkAsPushed - This function provides the MarkAsPushed function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is passed along unmodifed since all required information is provided on the context (EventId, CorrelationId,etc.. ) - PushToCore - This function provides the PushToCore function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is wrapped in an EdgeX event with the deviceName and readingName that were set upon instantiation and then sent to CoreData to be added as an event. Returns the new EdgeX event with ID populated. > NOTE : If validation is turned on in CoreServices then your `deviceName` and `readingName` must exist in the CoreMetadata and be properly registered in EdgeX . Export Functions There are two export functions included in the SDK that can be added to your pipeline. - NewHTTPSender(url string, mimeType string, persistOnError bool) -This function returns a HTTPSender instance initialized with the passed in url, mime type and persistOnError values. This HTTPSender instance is used to access the following functions that will use the required url and optional mime type and persistOnError: HTTPPost - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and posts it to the configured endpoint. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. Currently, only unauthenticated endpoints are supported. Authenticated endpoints will be supported in the future. If the post fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details NewMQTTSender(logging logger.LoggingClient, addr models.Addressable, keyCertPair *KeyCertPair, mqttConfig MqttConfig, persistOnError bool) - This function returns a MQTTSender instance initialized with the passed in MQTT configuration . This MQTTSender instance is used to access the following function that will use the specified MQTT configuration KeyCertPair - This structure holds the Key and Certificate information for when using secure TLS connection to the broker. Can be nil if not using secure TLS connection. MqttConfig - This structure holds addition MQTT configuration settings. Qos byte Retain bool AutoReconnect bool SkipCertVerify bool User string Password string The GO complier will default these to 0 , false and \"\" , so you only need to set the fields that your usage requires that differ from the default. MQTTSend - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sends it to the specified MQTT broker. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. If the send fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details Output Functions There is one output function included in the SDK that can be added to your pipeline. NewOutput() - This function returns a Output instance that is used to access the following output function: SetOutput - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sets it as the output data for the pipeline to return to the configured trigger. If configured to use message bus, the data will be published to the message bus as determined by the MessageBus and Binding configuration. If configured to use HTTP trigger the data is returned as the HTTP response. Note that calling Complete() from the Context API in a custom function can be used in place of adding this function to your pipeline Configuration Similar to other EdgeX services, configuration is first determined by the configuration.toml file in the /res folder. If -r is passed to the application on startup, the SDK will leverage the provided registry (i.e Consul) to push configuration from the file into the registry and monitor configuration from there. You will find the configuration under the edgex/appservices/1.0/ key. There are two primary sections in the configuration.toml file that will need to be set that are specific to the AppFunctionsSDK. 1) [Binding] - This specifies the trigger type and associated data required to configure a trigger. [Binding] Type = \"\" SubscribeTopic = \"\" PublishTopic = \"\" 2) [ApplicationSettings] - Is used for custom application settings and is accessed via the ApplicationSettings() API. The ApplicationSettings API returns a map[string] string containing the contents on the ApplicationSetting section of the configuration.toml file. [ApplicationSettings] ApplicationName = \"My Application Service\" Error Handling Each transform returns a true or false as part of the return signature. This is called the continuePipeline flag and indicates whether the SDK should continue calling successive transforms in the pipeline. return false, nil will stop the pipeline and stop processing the event. This is useful for example when filtering on values and nothing matches the criteria you've filtered on. return false, error , will stop the pipeline as well and the SDK will log the errorString you have returned. Returning true tells the SDK to continue, and will call the next function in the pipeline with your result. The SDK will return control back to main when receiving a SIGTERM/SIGINT event to allow for custom clean up. Advanced Topics The following items discuss topics that are a bit beyond the basic use cases of the Application Functions SDK when interacting with EdgeX. Configurable Functions Pipeline This SDK provides the capability to define the functions pipeline via configuration rather than code using the app-service-configurable application service. See app-service-configurable README for more details. Using The Webserver It is not uncommon to require your own API endpoints when building an app service. Rather than spin up your own webserver inside of your app (alongside the already existing running webserver), we've exposed a method that allows you add your own routes to the existing webserver. A few routes are reserved and cannot be used: - /api/version -/api/v1/ping - /api/v1/metrics - /api/v1/config - /api/v1/trigger To add your own route, use the AddRoute(route string, handler func(nethttp.ResponseWriter, *nethttp.Request), methods ...string) function provided on the sdk. Here's an example: edgexSdk.AddRoute(\"/myroute\", func(writer http.ResponseWriter, req *http.Request) { context := req.Context().Value(appsdk.SDKKey).(*appsdk.AppFunctionsSDK) context.LoggingClient.Info(\"TEST\") // alternative to edgexSdk.LoggingClient.Info(\"TEST\") writer.Header().Set(\"Content-Type\", \"text/plain\") writer.Write([]byte(\"hello\")) writer.WriteHeader(200) }, \"GET\") Under the hood, this simply adds the provided route, handler, and method to the gorilla mux.Router we use in the SDK. For more information you can check out the github repo here . You can access the resources such as the logging client by accessing the context as shown above -- this is useful for when your routes might not be defined in your main.go where you have access to the edgexSdk instance. Target Type The target type is the object type of the incoming data that is sent to the first function in the function pipeline. By default this is an EdgeX Event since typical usage is receiving events from Core Data via Message Bus. For other usages where the data is not events coming from Core Data, the TargetType of the accepted incoming data can be set when the SDK instance is created. There are scenarios where the incoming data is not an EdgeX Event . One example scenario is 2 application services are chained via the Message Bus. The output of the first service back to the Messages Bus is inference data from analyzing the original input Event data. The second service needs to be able to let the SDK know the target type of the input data it is expecting. For usages where the incoming data is not events , the TargetType of the excepted incoming data can the set when the SDK instance is created. Example: type Person struct { FirstName string ` json : \"first_name\" ` LastName string ` json : \"last_name\" ` } edgexSdk : = & appsdk . AppFunctionsSDK { ServiceKey : serviceKey , TargetType : & Person {} , } Note that TargetType must be set to a pointer to an instance of your target type such as &Person{} . The first function in your function pipeline will be passed an instance of your target type, not a pointer to it. In the example above the first function in the pipeline would start something like: func MyPersonFunction ( edgexcontext * appcontext . Context , params ... interface {} ) ( bool , interface {} ) { edgexcontext . LoggingClient . Debug ( \"MyPersonFunction\" ) if len ( params ) < 1 { // We didn ' t receive a result return false , nil } person , ok : = params [ 0 ].( Person ) if ! ok { return false , errors . New ( \"type received is not a Person\" ) } .... The SDK supports unmarshaling JSON or CBOR encoded data into an instance of the target type. If your incoming data is not JSON or CBOR encoded, you then need to set the TargetType to &[]byte . If the target type is set to &[]byte the incoming data will not be unmarshaled. The content type, if set, will be passed as the second parameter to the first function in your pipeline. Your first function will be responsible for decoding the data or not. Command Line Options The following command line options are available -c=<path> --confdir=<path> Specify an alternate configuration directory. -p=<profile> --profile=<profile> Specify a profile other than default. -r --registry Indicates the service should use the registry. -o -overwrite Overwrite configuration in the Registry with local values. -s -skipVersionCheck Indicates the service should skip the Core Service's version compatibility check. Examples: simple-filter-xml -r -c=./res -p=docker or simple-filter-xml --registry --confdir=./res --profile=docker Environment Variable Overrides All the configuration settings from the configuration.toml file can be overridden by environment variables. Except for two special cases listed below, the overrides only occur when the configuration values are first pushed into the Registry. Once the values are in the Registry, the Registry values are always used. The environment variable names have the following format: <TOML Key> <TOML Section>_<TOML Key> <TOML Section>_<TOML Sub-Section>_<TOML Key> Examples: TOML : FailLimit = 30 ENVVAR : FailLimit = 100 TOML : [ Logging ] EnableRemote = false ENVVAR : Logging . EnableRemote = true TOML : [ Clients ] [ Clients.CoreData ] Host = 'localhost' ENVVAR : Clients_CoreData_Host = edgex - core - data edgex_registry This environment variable overrides the Registry connection information and occurs every time the application service starts. The value is in the format of a URL. edgex_registry = consul : // edgex - core - consul : 8500 This sets the Registry information fields as follows : Type : consul Host : edgex - core - consul Port : 8500 edgex_service This environment variable overrides the Service connection information and occurs every time the application service starts. The value is in the format of a URL. edgex_service = http : // 192 . 168 . 1 . 2 : 4903 This sets the Service information fields as follows : Protocol : http Host : 192 . 168 . 1 . 2 Port : 4903 edgex_profile This environment variable overrides the command line profile argument. It will replace the current value passed via the -p or --profile , if one exists. If not specified it will add the --profile argument. This is useful when running the service via docker-compose. Using docker-compose: app-service-configurable-rules: image: edgexfoundry/docker-app-service-configurable:1.1.0 environment: - edgex_profile : \"rules-engine\" ports: - \"48095:48095\" container_name: edgex-app-service-configurable hostname: edgex-app-service-configurable networks: edgex-network: aliases: - edgex-app-service-configurable depends_on: - data - command This sets the --profile=docker-rules-engine command line argument so that the application service uses the docker-rules-engine configuration profile which resides at /res/docker-rules-engine/configuration.toml Note that Application Services no longer use docker profiles. They use Environment Overrides in the docker compose file to make the necessary changes to the configuration for running in Docker. See the Environment Variable Overrides For Docker section in App Service Configurable's README for more details and an example. Store and Forward The Store and Forward capability allows for export functions to persist data on failure and for the export of the data to be retried at a later time. Note: The order the data exported via this retry mechanism is not guaranteed to be the same order in which the data was initial received from Core Data Configuration Two sections of configuration have been added for Store and Forward. Writable.StoreAndForward allows enabling, setting the interval between retries and the max number of retries. If running with Registry, these setting can be changed on the fly without having to restart the service. [Writable.StoreAndForward] Enabled = false RetryInterval = '5m' MaxRetryCount = 10 Database describes which database type to use, mongodb or redisdb , and the information required to connect to the database. This section is required if Store and Forward is enabled, otherwise it is currently optional. [Database] Type = \"mongodb\" Host = \"localhost\" Port = 27017 Timeout = '5s' Username = \"\" Password = \"\" How it works When an export function encounters an error sending data it can call SetRetryData(payload []byte) on the Context. This will store the data for later retry. If the application service is stop and then restarted while stored data hasn't been successfully exported, the export retry will resume once the service is up and running again. Note: It is important that export functions return an error and stop pipeline execution after the call to SetRetryData . See HTTPPost function in SDK as an example When the RetryInterval expires, the function pipeline will be re-executed starting with the export function that saved the data. The saved data will be passed to the export function which can then attempt to resend the data. NOTE: The export function will receive the data as it was stored, so it is important that any transformation of the data occur in functions prior to the export function. The export function should only export the data that it receives. One of three out comes can occur after the export retried has completed. Export retry was successful In this case the stored data is removed from the database and the execution of the pipeline functions after the export function, if any, continues. Export retry fails and retry count has not been exceeded In this case the store data is updated in the database with the incremented retry count Export retry fails and retry count has been exceeded In this case the store data is removed from the database and never retried again.","title":"App Functions SDK (Golang) - Beta"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#app-functions-sdk-golang-beta","text":"Welcome the App Functions SDK for EdgeX. This sdk is meant to provide all the plumbing necessary for developers to get started in processing/transforming/exporting data out of EdgeX.","title":"App Functions SDK (Golang) - Beta"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#table-of-contents","text":"Getting Started Triggers Context API Built-In Functions Filtering Encryption Conversion Compressions Core Data Export Functions Configuration Error Handling Advanced Topics Target Type Command Line Options Environment Variable Overrides Store and Forward","title":"Table of contents"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#getting-started","text":"","title":"Getting Started"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#build-prerequisites","text":"Please see the edgex-go README .","title":"Build Prerequisites"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#the-sdk","text":"The SDK is built around the idea of a \"Functions Pipeline\". A functions pipeline is a collection of various functions that process the data in the order that you've specified. The functions pipeline is executed by the specified trigger in the configuration.toml . The first function in the pipeline is called with the event that triggered the pipeline (ex. events.Model ). Each successive call in the pipeline is called with the return result of the previous function. Let's take a look at a simple example that creates a pipeline to filter particular device ids and subsequently transform the data to XML: package main import ( \"fmt\" \"github.com/edgexfoundry/app-functions-sdk-go/appsdk\" \"github.com/edgexfoundry/app-functions-sdk-go/pkg/transforms\" \"os\" ) func main () { // 1 ) First thing to do is to create an instance of the EdgeX SDK , giving it a service key edgexSdk : = & appsdk . AppFunctionsSDK { ServiceKey : \"SimpleFilterXMLApp\" , // Key used by Registry ( Aka Consul ) } // 2 ) Next , we need to initialize the SDK if err : = edgexSdk . Initialize (); err != nil { edgexSdk . LoggingClient . Error ( fmt . Sprintf ( \"SDK initialization failed: %v \\n \" , err )) os . Exit ( - 1 ) } // 3 ) Since our FilterByDeviceName Function requires the list of Device Names we would // like to search for , we 'll go ahead and define that now. deviceNames : = [] string { \"Random-Float-Device\" } // 4 ) This is our pipeline configuration , the collection of functions to // execute every time an event is triggered . if err : = edgexSdk . SetFunctionsPipeline ( transforms . NewFilter ( deviceNames ) . FilterByDeviceName , transforms . NewConversion () . TransformToXML , ); err != nil { edgexSdk . LoggingClient . Error ( fmt . Sprintf ( \"SDK SetPipeline failed: %v \\n \" , err )) os . Exit ( - 1 ) } // 5 ) shows how to access the application 's specific configuration settings. appSettings : = edgexSdk . ApplicationSettings () if appSettings != nil { appName , ok : = appSettings [ \"ApplicationName\" ] if ok { edgexSdk . LoggingClient . Info ( fmt . Sprintf ( \" %s now running...\" , appName )) } else { edgexSdk . LoggingClient . Error ( \"ApplicationName application setting not found\" ) os . Exit ( - 1 ) } } else { edgexSdk . LoggingClient . Error ( \"No application settings found\" ) os . Exit ( - 1 ) } // 6 ) Lastly , we 'll go ahead and tell the SDK to \"start\" and begin listening for events to trigger the pipeline. edgexSdk . MakeItRun () } The above example is meant to merely demonstrate the structure of your application. Notice that the output of the last function is not available anywhere inside this application. You must provide a function in order to work with the data from the previous function. Let's go ahead and add the following function that prints the output to the console. func printXMLToConsole(edgexcontext *appcontext.Context, params ...interface{}) (bool,interface{}) { if len(params) < 1 { // We didn't receive a result return false, errors.New(\"No Data Received\") } println(params[0].(string)) return true, nil } After placing the above function in your code, the next step is to modify the pipeline to call this function: edgexSdk.SetFunctionsPipeline( transforms.NewFilter(deviceNames).FilterByDeviceName, transforms.NewConversion().TransformToXML, printXMLToConsole //notice this is not a function call, but simply a function pointer. ) After making the above modifications, you should now see data printing out to the console in XML when an event is triggered. > You can find this example in the /examples directory located in this repository. You can also use the provided EdgeX Applications Function SDK.postman\\_collection.json file to load into postman to trigger the sample pipeline. Up until this point, the pipeline has been triggered by an event over HTTP and the data at the end of that pipeline lands in the last function specified. In the example, data ends up printed to the console. Perhaps we'd like to send the data back to where it came from. In the case of an HTTP trigger, this would be the HTTP response. In the case of a message bus, this could be a new topic to send the data back to for other applications that wish to receive it. To do this, simply call edgexcontext.Complete([]byte outputData) passing in the data you wish to \"respond\" with. In the above printXMLToConsole(...) function, replace println(params[0].(string)) with edgexcontext.Complete([]byte(params[0].(string))) . You should now see the response in your postman window when testing the pipeline.","title":"The SDK"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#examples","text":"The App Service Examples repo contains a variety of simple to advanced example Application Services built upon the App Functions SDK . Examples that once were in the examples folder of the SDK have been moved to this Examples repo.","title":"Examples"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#triggers","text":"Triggers determine how the app functions pipeline begins execution. In the simple example provided above, an HTTP trigger is used. The trigger is determine by the configuration.toml file located in the /res directory under a section called [Binding] . Check out the Configuration Section for more information about the toml file.","title":"Triggers"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#message-bus-trigger","text":"A message bus trigger will execute the pipeline every time data is received off of the configured topic.","title":"Message Bus Trigger"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#type-and-topic-configuration","text":"Here's an example: Type = \"messagebus\" SubscribeTopic = \"events\" PublishTopic = \"\" The Type= is set to \"messagebus\". EdgeX Core Data is publishing data to the events topic. So to receive data from core data, you can set your SubscribeTopic= either to \"\" or \"events\" . You may also designate a PublishTopic= if you wish to publish data back to the message bus. edgexcontext.Complete([]byte outputData) - Will send data back to back to the message bus with the topic specified in the PublishTopic= property #### Message bus connection configuration The other piece of configuration required are the connection settings: [MessageBus] Type = 'zero' #specifies of message bus (i.e zero for ZMQ) [MessageBus.PublishHost] Host = '*' Port = 5564 Protocol = 'tcp' [MessageBus.SubscribeHost] Host = 'localhost' Port = 5563 Protocol = 'tcp' By default, EdgeX Core Data publishes data to the events topic on port 5563. The publish host is used if publishing data back to the message bus. > Important Note: Publish Host MUST be different for every topic you wish to publish to since the SDK will bind to the specific port. 5563 for example cannot be used to publish since EdgeX Core Data has bound to that port. Similarly, you cannot have two separate instances of the app functions SDK running publishing to the same port.","title":"Type and Topic configuration"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#http-trigger","text":"Designating an HTTP trigger will allow the pipeline to be triggered by a RESTful POST call to http://[host]:[port]/trigger/ . The body of the POST must be an EdgeX event. edgexcontext.Complete([]byte outputData) - Will send the specified data as the response to the request that originally triggered the HTTP Request.","title":"HTTP Trigger"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#context-api","text":"The context parameter passed to each function/transform provides operations and data associated with each execution of the pipeline. Let's take a look at a few of the properties that are available: type Context struct { // ID of the EdgeX Event -- will be filled for a received JSON Event EventID string // Checksum of the EdgeX Event -- will be filled for a received CBOR Event EventChecksum string // This is the ID used to track the EdgeX event through entire EdgeX framework . CorrelationID string // OutputData is used for specifying the data that is to be outputted . Leverage the . Complete () function to set . OutputData [] byte // This holds the configuration for your service . This is the preferred way to access your custom application settings that have been set in the configuration . Configuration common . ConfigurationStruct // LoggingClient is exposed to allow logging following the preferred logging strategy within EdgeX . LoggingClient logger . LoggingClient // EventClient exposes Core Data 's EventClient API EventClient coredata.EventClient // ValueDescriptorClient exposes Core Data' s ValueDescriptor API ValueDescriptorClient coredata . ValueDescriptorClient // CommandClient exposes Core Commands 's Command API CommandClient command.CommandClient // NotificationsClient exposes Support Notification' s Notifications API NotificationsClient notifications . NotificationsClient // RetryData holds the data to be stored for later retry when the pipeline function returns an error RetryData [] byte }","title":"Context API"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#loggingclient","text":"The LoggingClient exposed on the context is available to leverage logging libraries/service utilized throughout the EdgeX framework. The SDK has initialized everything so it can be used to log Trace , Debug , Warn , Info , and Error messages as appropriate. See examples/simple-filter-xml/main.go for an example of how to use the LoggingClient .","title":"LoggingClient"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#eventclient","text":"The EventClient exposed on the context is available to leverage Core Data's Event API. See interface definition for more details. This client is useful for querying events and is used by the MarkAsPushed convenience API described below.","title":"EventClient"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#valuedescriptorclient","text":"The ValueDescriptorClient exposed on the context is available to leverage Core Data's ValueDescriptor API. See interface definition for more details. Useful for looking up the value descriptor for a reading received.","title":"ValueDescriptorClient"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#commandclient","text":"The CommandClient exposed on the context is available to leverage Core Command's Command API. See interface definition for more details. Useful for sending commands to devices.","title":"CommandClient"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#notificationsclient","text":"The CommandClient exposed on the context is available to leverage Support Notifications' Notifications API. See README for more details. Useful for sending notifications.","title":"NotificationsClient"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#note-about-clients","text":"Each of the clients above is only initialized if the Clients section of the configuration contains an entry for the service associated with the Client API. If it isn't in the configuration the client will be nil . Your code must check for nil to avoid panic in case it is missing from the configuration. Only add the clients to your configuration that your Application Service will actually be using. All application services need the Logging and many will need Core-Data . The following is an example Clients section of a configuration.toml with all supported clients specified: [Clients] [Clients.Logging] Protocol = \"http\" Host = \"localhost\" Port = 48061 [Clients.CoreData] Protocol = 'http' Host = 'localhost' Port = 48080 [Clients.Command] Protocol = 'http' Host = 'localhost' Port = 48082 [Clients.Notifications] Protocol = 'http' Host = 'localhost' Port = 48060","title":"Note about Clients"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#markaspushed","text":".MarkAsPushed() is used to indicate to EdgeX Core Data that an event has been \"pushed\" and is no longer required to be stored. The scheduler service will purge all events that have been marked as pushed based on the configured schedule. By default, it is once daily at midnight. If you leverage the built in export functions (i.e. HTTP Export, or MQTT Export), then the event will automatically be marked as pushed upon a successful export. ### .PushToCore() .PushToCore(string deviceName, string readingName, byte[] value) is used to push data to EdgeX Core Data so that it can be shared with other applications that are subscribed to the message bus that core-data publishes to. deviceName can be set as you like along with the readingName which will be set on the EdgeX event sent to CoreData. This function will return the new EdgeX Event with the ID populated, however the CorrelationId will not be available. NOTE: If validation is turned on in CoreServices then your deviceName and readingName must exist in the CoreMetadata and be properly registered in EdgeX.","title":".MarkAsPushed()"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#setretrydata","text":".SetRetryData(payload []byte) can be used to store data for later retry. This is useful when creating a custom export function that needs to retry on failure sending the data. The payload data will be stored for later retry based on Store and Forward configuration. When the retry is triggered, the function pipeline will be re-executed starting with the function that called this API. That function will be passed the stored data, so it is important that all transformations occur in functions prior to the export function. The Context will also be restored to the state when the function called this API. See Store and Forward for more details. NOTE: Store and Forward be must enabled when calling this API.","title":".SetRetryData()"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#built-in-transformsfunctions","text":"All transforms define a type and a New function which is used to initialize an instance of the type with the required parameters. These instances returned by these New functions give access to their appropriate pipeline function pointers when build the function pipeline. E.G. NewFilter([] {\"Device1\", \"Device2\"}).FilterByDeviceName","title":"Built-In Transforms/Functions"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#filtering","text":"There are two basic types of filtering included in the SDK to add to your pipeline. Theses provided Filter functions return a type of events.Model. If filtering results in no remaining data, the pipeline execution for that pass is terminated. If no values are provided for filtering, then data flows through unfiltered. - NewFilter([]string filterValues) - This function returns a Filter instance initialized with the passed in filter values. This Filter instance is used to access the following filter functions that will operate using the specified filter values. - FilterByDeviceName - This function will filter the event data down to the specified device names and return the filtered data to the pipeline. - FilterByValueDescriptor - This function will filter the event data down to the specified device value descriptor and return the filtered data to the pipeline.","title":"Filtering"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#encryption","text":"There is one encryption transform included in the SDK that can be added to your pipeline. NewEncryption(key string, initializationVector string) - This function returns a Encryption instance initialized with the passed in key and initialization vector. This Encryption instance is used to access the following encryption function that will use the specified key and initialization vector. EncryptWithAES - This function receives a either a string , []byte , or json.Marshaller type and encrypts it using AES encryption and returns a []byte to the pipeline.","title":"Encryption"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#conversion","text":"There are two conversions included in the SDK that can be added to your pipeline. These transforms return a string . NewConversion() - This function returns a Conversion instance that is used to access the following conversion functions: TransformToXML - This function receives an events.Model type, converts it to XML format and returns the XML string to the pipeline. TransformToJSON - This function receives an events.Model type and converts it to JSON format and returns the JSON string to the pipeline. ### Compressions There are two compression types included in the SDK that can be added to your pipeline. These transforms return a []byte . NewCompression() - This function returns a Compression instance that is used to access the following compression functions: CompressWithGZIP - This function receives either a string , []byte , or json.Marshaler type, GZIP compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. CompressWithZLIB - This function receives either a string , []byte , or json.Marshaler type, ZLIB compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline.","title":"Conversion"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#coredata-functions","text":"These are functions that enable interactions with the CoreData REST API. - NewCoreData() - This function returns a CoreData instance. This CoreData instance is used to access the following function(s). - MarkAsPushed - This function provides the MarkAsPushed function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is passed along unmodifed since all required information is provided on the context (EventId, CorrelationId,etc.. ) - PushToCore - This function provides the PushToCore function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is wrapped in an EdgeX event with the deviceName and readingName that were set upon instantiation and then sent to CoreData to be added as an event. Returns the new EdgeX event with ID populated. > NOTE : If validation is turned on in CoreServices then your `deviceName` and `readingName` must exist in the CoreMetadata and be properly registered in EdgeX .","title":"CoreData Functions"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#export-functions","text":"There are two export functions included in the SDK that can be added to your pipeline. - NewHTTPSender(url string, mimeType string, persistOnError bool) -This function returns a HTTPSender instance initialized with the passed in url, mime type and persistOnError values. This HTTPSender instance is used to access the following functions that will use the required url and optional mime type and persistOnError: HTTPPost - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and posts it to the configured endpoint. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. Currently, only unauthenticated endpoints are supported. Authenticated endpoints will be supported in the future. If the post fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details NewMQTTSender(logging logger.LoggingClient, addr models.Addressable, keyCertPair *KeyCertPair, mqttConfig MqttConfig, persistOnError bool) - This function returns a MQTTSender instance initialized with the passed in MQTT configuration . This MQTTSender instance is used to access the following function that will use the specified MQTT configuration KeyCertPair - This structure holds the Key and Certificate information for when using secure TLS connection to the broker. Can be nil if not using secure TLS connection. MqttConfig - This structure holds addition MQTT configuration settings. Qos byte Retain bool AutoReconnect bool SkipCertVerify bool User string Password string The GO complier will default these to 0 , false and \"\" , so you only need to set the fields that your usage requires that differ from the default. MQTTSend - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sends it to the specified MQTT broker. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. If the send fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details","title":"Export Functions"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#output-functions","text":"There is one output function included in the SDK that can be added to your pipeline. NewOutput() - This function returns a Output instance that is used to access the following output function: SetOutput - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sets it as the output data for the pipeline to return to the configured trigger. If configured to use message bus, the data will be published to the message bus as determined by the MessageBus and Binding configuration. If configured to use HTTP trigger the data is returned as the HTTP response. Note that calling Complete() from the Context API in a custom function can be used in place of adding this function to your pipeline","title":"Output Functions"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#configuration","text":"Similar to other EdgeX services, configuration is first determined by the configuration.toml file in the /res folder. If -r is passed to the application on startup, the SDK will leverage the provided registry (i.e Consul) to push configuration from the file into the registry and monitor configuration from there. You will find the configuration under the edgex/appservices/1.0/ key. There are two primary sections in the configuration.toml file that will need to be set that are specific to the AppFunctionsSDK. 1) [Binding] - This specifies the trigger type and associated data required to configure a trigger. [Binding] Type = \"\" SubscribeTopic = \"\" PublishTopic = \"\" 2) [ApplicationSettings] - Is used for custom application settings and is accessed via the ApplicationSettings() API. The ApplicationSettings API returns a map[string] string containing the contents on the ApplicationSetting section of the configuration.toml file. [ApplicationSettings] ApplicationName = \"My Application Service\"","title":"Configuration"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#error-handling","text":"Each transform returns a true or false as part of the return signature. This is called the continuePipeline flag and indicates whether the SDK should continue calling successive transforms in the pipeline. return false, nil will stop the pipeline and stop processing the event. This is useful for example when filtering on values and nothing matches the criteria you've filtered on. return false, error , will stop the pipeline as well and the SDK will log the errorString you have returned. Returning true tells the SDK to continue, and will call the next function in the pipeline with your result. The SDK will return control back to main when receiving a SIGTERM/SIGINT event to allow for custom clean up.","title":"Error Handling"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#advanced-topics","text":"The following items discuss topics that are a bit beyond the basic use cases of the Application Functions SDK when interacting with EdgeX.","title":"Advanced Topics"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#configurable-functions-pipeline","text":"This SDK provides the capability to define the functions pipeline via configuration rather than code using the app-service-configurable application service. See app-service-configurable README for more details.","title":"Configurable Functions Pipeline"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#using-the-webserver","text":"It is not uncommon to require your own API endpoints when building an app service. Rather than spin up your own webserver inside of your app (alongside the already existing running webserver), we've exposed a method that allows you add your own routes to the existing webserver. A few routes are reserved and cannot be used: - /api/version -/api/v1/ping - /api/v1/metrics - /api/v1/config - /api/v1/trigger To add your own route, use the AddRoute(route string, handler func(nethttp.ResponseWriter, *nethttp.Request), methods ...string) function provided on the sdk. Here's an example: edgexSdk.AddRoute(\"/myroute\", func(writer http.ResponseWriter, req *http.Request) { context := req.Context().Value(appsdk.SDKKey).(*appsdk.AppFunctionsSDK) context.LoggingClient.Info(\"TEST\") // alternative to edgexSdk.LoggingClient.Info(\"TEST\") writer.Header().Set(\"Content-Type\", \"text/plain\") writer.Write([]byte(\"hello\")) writer.WriteHeader(200) }, \"GET\") Under the hood, this simply adds the provided route, handler, and method to the gorilla mux.Router we use in the SDK. For more information you can check out the github repo here . You can access the resources such as the logging client by accessing the context as shown above -- this is useful for when your routes might not be defined in your main.go where you have access to the edgexSdk instance.","title":"Using The Webserver"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#target-type","text":"The target type is the object type of the incoming data that is sent to the first function in the function pipeline. By default this is an EdgeX Event since typical usage is receiving events from Core Data via Message Bus. For other usages where the data is not events coming from Core Data, the TargetType of the accepted incoming data can be set when the SDK instance is created. There are scenarios where the incoming data is not an EdgeX Event . One example scenario is 2 application services are chained via the Message Bus. The output of the first service back to the Messages Bus is inference data from analyzing the original input Event data. The second service needs to be able to let the SDK know the target type of the input data it is expecting. For usages where the incoming data is not events , the TargetType of the excepted incoming data can the set when the SDK instance is created. Example: type Person struct { FirstName string ` json : \"first_name\" ` LastName string ` json : \"last_name\" ` } edgexSdk : = & appsdk . AppFunctionsSDK { ServiceKey : serviceKey , TargetType : & Person {} , } Note that TargetType must be set to a pointer to an instance of your target type such as &Person{} . The first function in your function pipeline will be passed an instance of your target type, not a pointer to it. In the example above the first function in the pipeline would start something like: func MyPersonFunction ( edgexcontext * appcontext . Context , params ... interface {} ) ( bool , interface {} ) { edgexcontext . LoggingClient . Debug ( \"MyPersonFunction\" ) if len ( params ) < 1 { // We didn ' t receive a result return false , nil } person , ok : = params [ 0 ].( Person ) if ! ok { return false , errors . New ( \"type received is not a Person\" ) } .... The SDK supports unmarshaling JSON or CBOR encoded data into an instance of the target type. If your incoming data is not JSON or CBOR encoded, you then need to set the TargetType to &[]byte . If the target type is set to &[]byte the incoming data will not be unmarshaled. The content type, if set, will be passed as the second parameter to the first function in your pipeline. Your first function will be responsible for decoding the data or not.","title":"Target Type"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#command-line-options","text":"The following command line options are available -c=<path> --confdir=<path> Specify an alternate configuration directory. -p=<profile> --profile=<profile> Specify a profile other than default. -r --registry Indicates the service should use the registry. -o -overwrite Overwrite configuration in the Registry with local values. -s -skipVersionCheck Indicates the service should skip the Core Service's version compatibility check. Examples: simple-filter-xml -r -c=./res -p=docker or simple-filter-xml --registry --confdir=./res --profile=docker","title":"Command Line Options"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#environment-variable-overrides","text":"All the configuration settings from the configuration.toml file can be overridden by environment variables. Except for two special cases listed below, the overrides only occur when the configuration values are first pushed into the Registry. Once the values are in the Registry, the Registry values are always used. The environment variable names have the following format: <TOML Key> <TOML Section>_<TOML Key> <TOML Section>_<TOML Sub-Section>_<TOML Key> Examples: TOML : FailLimit = 30 ENVVAR : FailLimit = 100 TOML : [ Logging ] EnableRemote = false ENVVAR : Logging . EnableRemote = true TOML : [ Clients ] [ Clients.CoreData ] Host = 'localhost' ENVVAR : Clients_CoreData_Host = edgex - core - data","title":"Environment Variable Overrides"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#edgex_registry","text":"This environment variable overrides the Registry connection information and occurs every time the application service starts. The value is in the format of a URL. edgex_registry = consul : // edgex - core - consul : 8500 This sets the Registry information fields as follows : Type : consul Host : edgex - core - consul Port : 8500","title":"edgex_registry"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#edgex_service","text":"This environment variable overrides the Service connection information and occurs every time the application service starts. The value is in the format of a URL. edgex_service = http : // 192 . 168 . 1 . 2 : 4903 This sets the Service information fields as follows : Protocol : http Host : 192 . 168 . 1 . 2 Port : 4903","title":"edgex_service"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#edgex_profile","text":"This environment variable overrides the command line profile argument. It will replace the current value passed via the -p or --profile , if one exists. If not specified it will add the --profile argument. This is useful when running the service via docker-compose. Using docker-compose: app-service-configurable-rules: image: edgexfoundry/docker-app-service-configurable:1.1.0 environment: - edgex_profile : \"rules-engine\" ports: - \"48095:48095\" container_name: edgex-app-service-configurable hostname: edgex-app-service-configurable networks: edgex-network: aliases: - edgex-app-service-configurable depends_on: - data - command This sets the --profile=docker-rules-engine command line argument so that the application service uses the docker-rules-engine configuration profile which resides at /res/docker-rules-engine/configuration.toml Note that Application Services no longer use docker profiles. They use Environment Overrides in the docker compose file to make the necessary changes to the configuration for running in Docker. See the Environment Variable Overrides For Docker section in App Service Configurable's README for more details and an example.","title":"edgex_profile"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#store-and-forward","text":"The Store and Forward capability allows for export functions to persist data on failure and for the export of the data to be retried at a later time. Note: The order the data exported via this retry mechanism is not guaranteed to be the same order in which the data was initial received from Core Data","title":"Store and Forward"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#configuration_1","text":"Two sections of configuration have been added for Store and Forward. Writable.StoreAndForward allows enabling, setting the interval between retries and the max number of retries. If running with Registry, these setting can be changed on the fly without having to restart the service. [Writable.StoreAndForward] Enabled = false RetryInterval = '5m' MaxRetryCount = 10 Database describes which database type to use, mongodb or redisdb , and the information required to connect to the database. This section is required if Store and Forward is enabled, otherwise it is currently optional. [Database] Type = \"mongodb\" Host = \"localhost\" Port = 27017 Timeout = '5s' Username = \"\" Password = \"\"","title":"Configuration"},{"location":"microservices/application/Ch-ApplicationFunctionsSDK/#how-it-works","text":"When an export function encounters an error sending data it can call SetRetryData(payload []byte) on the Context. This will store the data for later retry. If the application service is stop and then restarted while stored data hasn't been successfully exported, the export retry will resume once the service is up and running again. Note: It is important that export functions return an error and stop pipeline execution after the call to SetRetryData . See HTTPPost function in SDK as an example When the RetryInterval expires, the function pipeline will be re-executed starting with the export function that saved the data. The saved data will be passed to the export function which can then attempt to resend the data. NOTE: The export function will receive the data as it was stored, so it is important that any transformation of the data occur in functions prior to the export function. The export function should only export the data that it receives. One of three out comes can occur after the export retried has completed. Export retry was successful In this case the stored data is removed from the database and the execution of the pipeline functions after the export function, if any, continues. Export retry fails and retry count has not been exceeded In this case the store data is updated in the database with the incremented retry count Export retry fails and retry count has been exceeded In this case the store data is removed from the database and never retried again.","title":"How it works"},{"location":"microservices/application/Ch-ApplicationServices/","text":"Application Services Application Services are a means to get data from EdgeX Foundry to external systems and process (be it analytics package, enterprise or on-prem application, cloud systems like Azure IoT, AWS IoT, or Google IoT Core, etc.). Application Services provide the means for data to be prepared (transformed, enriched, filtered, etc.) and groomed (formatted, compressed, encrypted, etc.) before being sent to an endpoint of choice. Endpoints supported out of the box today include HTTP and MQTT endpoints, but will include additional offerings in the future and could include a custom endpoints. Application Services are based on the idea of a \"Functions Pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event/reading messages) in the order that you've specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger is something like a message landing in a watched message queue. An Applications Functions Software Development Kit (or App Functions SDK) is a available to help create Application Services. Currently the only SDK supported language is Golang, with the intention that community developed and supported SDKs will come in the future for other languages. It is currently available as a Golang module to remain operating system (OS) agnostic and to comply with the latest EdgeX guidelines on dependency management. Application Service Improvements Providing an SDK that connects directly to a message bus by which Core Data events are published eliminates performance issues as well as allow the developers extra control on what happens with that data as soon as it is available. Furthermore, it emphasizes configuration over registration for consuming the data. The application services can be customized to a client's needs and thereby also removing the need for client registration. Standard Functions As mentioned, an Application Service is a function pipeline. The SDK provides some standard functions that can be used in a functions pipeline. In the future, additional functions will be provided \"standard\" or in other words provided with the SDK. Additinally, developers can implement their own custom functions and add those to the Application Service functions pipeline. One of the most common use cases for working with data that come from Core Data is to filter data down to what is relevant for a given application and to format it. To help facilitate this, four primary functions ported over from the existing services today are included in the SDK. The first is the DeviceNameFilter function which will remove events that do not match the specified IDs and will cease execution of the pipeline if no event matches. The second is the ValueDescriptorFilter which exhibits the same behavior as DeviceNameFilter except filtering on Value Descriptor instead of DeviceID. The third and fourth provided functions in the SDK transform the data received to either XML or JSON by calling XMLTransform or JSONTransform . Typically, after filtering and transforming the data as needed, exporting is the last step in a pipeline to ship the data where it needs to go. There are two primary functions included in the SDK to help facilitate this. The first is HTTPPost(string url) function that will POST the provided data to a specified endpoint, and the second is an MQTTPublish() function that will publish the provided data to an MQTT Broker as specified in the configuration. There are two primary triggers that have been included in the SDK that initiate the start of the function pipeline. First is via a POST HTTP Endpoint /trigger with the EdgeX event data as the body. Second is the MessageBus subscription with connection details as specified in the configuration. Finally, data may be sent back to the message bus or HTTP response by calling .complete() on the context. If the trigger is HTTP, then it will be an HTTP Response. If the trigger is MessageBus, then it will be published to the configured host and topic. Examples There are three example Application Services provided in the app-functions-sdk-go repository in the /examples directory that attempt to show basic structure of building an application with the app functions sdk. They also focus on how to leverage various built in provided functions as mentioned above as well as how to write your own in the case that the SDK does not provide what is needed. Simple Filter XML Demonstrates Filter of data by device ID and transforming data to XML Simple Filter XML Post Same example as #1, but result published to HTTP Endpoint Simple Filter XML MQTT Same example as #1, but result published to MQTT Broker The features in the initial implementation of the App Functions SDK should be sufficient to provide the foundation for basic filtering and export needs. There are some functions in the existing export services that are not yet available in application functions and are intended to be included in a later release. This includes the Encryption Transformer, the Compressor Transformer, and Valid Event Check. See Unsupported existing export service functions . The primary purpose for leaving this out was to address core pieces of functionality that would set up the ease of adding additional functions in the future. Unsupported existing export service functions Valid Event Check --The first component in the pipe and filter, before the copier (described in the previous section) is a filter that can be optionally turned on or off by configuration. This filter is a general purpose data checking filter which assesses the device- or sensor-provided Event, with associated Readings, and ensures the data conforms to the ValueDescriptor associated with the Readings. For example, if the data from a sensor is described by its metadata profile as adhering to a \"Temperature\" value descriptor of floating number type, with the value between -100\u00b0 F and 200\u00b0 F, but the data seen in the Event and Readings is not a floating point number, for example if the data in the reading is a word such as \"cold,\" instead of a number, then the Event is rejected (no client receives the data) and no further processing is accomplished on the Event by the Export Distro service.","title":"Application Services"},{"location":"microservices/application/Ch-ApplicationServices/#application-services","text":"Application Services are a means to get data from EdgeX Foundry to external systems and process (be it analytics package, enterprise or on-prem application, cloud systems like Azure IoT, AWS IoT, or Google IoT Core, etc.). Application Services provide the means for data to be prepared (transformed, enriched, filtered, etc.) and groomed (formatted, compressed, encrypted, etc.) before being sent to an endpoint of choice. Endpoints supported out of the box today include HTTP and MQTT endpoints, but will include additional offerings in the future and could include a custom endpoints. Application Services are based on the idea of a \"Functions Pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event/reading messages) in the order that you've specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger is something like a message landing in a watched message queue. An Applications Functions Software Development Kit (or App Functions SDK) is a available to help create Application Services. Currently the only SDK supported language is Golang, with the intention that community developed and supported SDKs will come in the future for other languages. It is currently available as a Golang module to remain operating system (OS) agnostic and to comply with the latest EdgeX guidelines on dependency management.","title":"Application Services"},{"location":"microservices/application/Ch-ApplicationServices/#application-service-improvements","text":"Providing an SDK that connects directly to a message bus by which Core Data events are published eliminates performance issues as well as allow the developers extra control on what happens with that data as soon as it is available. Furthermore, it emphasizes configuration over registration for consuming the data. The application services can be customized to a client's needs and thereby also removing the need for client registration.","title":"Application Service Improvements"},{"location":"microservices/application/Ch-ApplicationServices/#standard-functions","text":"As mentioned, an Application Service is a function pipeline. The SDK provides some standard functions that can be used in a functions pipeline. In the future, additional functions will be provided \"standard\" or in other words provided with the SDK. Additinally, developers can implement their own custom functions and add those to the Application Service functions pipeline. One of the most common use cases for working with data that come from Core Data is to filter data down to what is relevant for a given application and to format it. To help facilitate this, four primary functions ported over from the existing services today are included in the SDK. The first is the DeviceNameFilter function which will remove events that do not match the specified IDs and will cease execution of the pipeline if no event matches. The second is the ValueDescriptorFilter which exhibits the same behavior as DeviceNameFilter except filtering on Value Descriptor instead of DeviceID. The third and fourth provided functions in the SDK transform the data received to either XML or JSON by calling XMLTransform or JSONTransform . Typically, after filtering and transforming the data as needed, exporting is the last step in a pipeline to ship the data where it needs to go. There are two primary functions included in the SDK to help facilitate this. The first is HTTPPost(string url) function that will POST the provided data to a specified endpoint, and the second is an MQTTPublish() function that will publish the provided data to an MQTT Broker as specified in the configuration. There are two primary triggers that have been included in the SDK that initiate the start of the function pipeline. First is via a POST HTTP Endpoint /trigger with the EdgeX event data as the body. Second is the MessageBus subscription with connection details as specified in the configuration. Finally, data may be sent back to the message bus or HTTP response by calling .complete() on the context. If the trigger is HTTP, then it will be an HTTP Response. If the trigger is MessageBus, then it will be published to the configured host and topic.","title":"Standard Functions"},{"location":"microservices/application/Ch-ApplicationServices/#examples","text":"There are three example Application Services provided in the app-functions-sdk-go repository in the /examples directory that attempt to show basic structure of building an application with the app functions sdk. They also focus on how to leverage various built in provided functions as mentioned above as well as how to write your own in the case that the SDK does not provide what is needed. Simple Filter XML Demonstrates Filter of data by device ID and transforming data to XML Simple Filter XML Post Same example as #1, but result published to HTTP Endpoint Simple Filter XML MQTT Same example as #1, but result published to MQTT Broker The features in the initial implementation of the App Functions SDK should be sufficient to provide the foundation for basic filtering and export needs. There are some functions in the existing export services that are not yet available in application functions and are intended to be included in a later release. This includes the Encryption Transformer, the Compressor Transformer, and Valid Event Check. See Unsupported existing export service functions . The primary purpose for leaving this out was to address core pieces of functionality that would set up the ease of adding additional functions in the future.","title":"Examples"},{"location":"microservices/application/Ch-ApplicationServices/#unsupported-existing-export-service-functions","text":"Valid Event Check --The first component in the pipe and filter, before the copier (described in the previous section) is a filter that can be optionally turned on or off by configuration. This filter is a general purpose data checking filter which assesses the device- or sensor-provided Event, with associated Readings, and ensures the data conforms to the ValueDescriptor associated with the Readings. For example, if the data from a sensor is described by its metadata profile as adhering to a \"Temperature\" value descriptor of floating number type, with the value between -100\u00b0 F and 200\u00b0 F, but the data seen in the Event and Readings is not a floating point number, for example if the data in the reading is a word such as \"cold,\" instead of a number, then the Event is rejected (no client receives the data) and no further processing is accomplished on the Event by the Export Distro service.","title":"Unsupported existing export service functions"},{"location":"microservices/application/Ch-HTTPTrigger/","text":"HTTP Trigger Designating an HTTP trigger will allow the pipeline to be triggered by a RESTful POST call to http://[host] :[port]/trigger/. The body of the POST must be an EdgeX event. edgexcontext.Complete([]byte outputData) - Will send the specified data as the response to the request that originally triggered the HTTP Request. In the main() function, note the call to HTTPPostXML or HTTPPostJSON at the end of the pipeline to return the response. from Simple Filter XML Post edgexSdk.SetFunctionsPipeline( edgexSdk.DeviceNameFilter(deviceNames), edgexSdk.XMLTransform(), edgexSdk.HTTPPostXML(\"<Your endpoint goes here>\"), )","title":"HTTP Trigger"},{"location":"microservices/application/Ch-HTTPTrigger/#http-trigger","text":"Designating an HTTP trigger will allow the pipeline to be triggered by a RESTful POST call to http://[host] :[port]/trigger/. The body of the POST must be an EdgeX event. edgexcontext.Complete([]byte outputData) - Will send the specified data as the response to the request that originally triggered the HTTP Request. In the main() function, note the call to HTTPPostXML or HTTPPostJSON at the end of the pipeline to return the response. from Simple Filter XML Post edgexSdk.SetFunctionsPipeline( edgexSdk.DeviceNameFilter(deviceNames), edgexSdk.XMLTransform(), edgexSdk.HTTPPostXML(\"<Your endpoint goes here>\"), )","title":"HTTP Trigger"},{"location":"microservices/application/Ch-MessageBusTrigger/","text":"Message Bus Trigger A message bus trigger will execute the pipeline every time data is received off of the configured topic. Type and Topic Configuration Here's an example: Type=\"messagebus\" SubscribeTopic=\"events\" PublishTopic=\"\" The Type= is set to \"messagebus\". EdgeX Core Data is publishing data to the events topic. So to receive data from core data, you can set your SubscribeTopic= either to \"\" or \"events\". You may also designate a PublishTopic= if you wish to publish data back to the message bus. edgexcontext.Complete([]byte outputData) - Will send data back to back to the message bus with the topic specified in the PublishTopic= property Message bus connection configuration The other piece of configuration required are the connection settings: [MessageBus] Type = 'zero' #specifies of message bus (i.e zero for ZMQ) [MessageBus.PublishHost] Host = '*' Port = 5564 Protocol = 'tcp' [MessageBus.SubscribeHost] Host = 'localhost' Port = 5563 Protocol = 'tcp' By default, EdgeX Core Data publishes data to the events topic on port 5563. The publish host is used if publishing data back to the message bus. Important Note: Publish Host MUST be different for every topic you wish to publish to since the SDK will bind to the specific port. 5563 for example cannot be used to publish since EdgeX Core Data has bound to that port. Similarly, you cannot have two separate instances of the app functions SDK running publishing to the same port. In the main() function, note the call to MQTTSend at the end of the pipeline to return the response. from Simple Filter XML MQTT edgexSdk.SetFunctionsPipeline( edgexSdk.DeviceNameFilter(deviceNames), edgexSdk.XMLTransform(), printXMLToConsole, edgexSdk.MQTTSend(addressable, \"\", \"\", 0, false, false), )","title":"Message Bus Trigger"},{"location":"microservices/application/Ch-MessageBusTrigger/#message-bus-trigger","text":"A message bus trigger will execute the pipeline every time data is received off of the configured topic.","title":"Message Bus Trigger"},{"location":"microservices/application/Ch-MessageBusTrigger/#type-and-topic-configuration","text":"Here's an example: Type=\"messagebus\" SubscribeTopic=\"events\" PublishTopic=\"\" The Type= is set to \"messagebus\". EdgeX Core Data is publishing data to the events topic. So to receive data from core data, you can set your SubscribeTopic= either to \"\" or \"events\". You may also designate a PublishTopic= if you wish to publish data back to the message bus. edgexcontext.Complete([]byte outputData) - Will send data back to back to the message bus with the topic specified in the PublishTopic= property","title":"Type and Topic Configuration"},{"location":"microservices/application/Ch-MessageBusTrigger/#message-bus-connection-configuration","text":"The other piece of configuration required are the connection settings: [MessageBus] Type = 'zero' #specifies of message bus (i.e zero for ZMQ) [MessageBus.PublishHost] Host = '*' Port = 5564 Protocol = 'tcp' [MessageBus.SubscribeHost] Host = 'localhost' Port = 5563 Protocol = 'tcp' By default, EdgeX Core Data publishes data to the events topic on port 5563. The publish host is used if publishing data back to the message bus. Important Note: Publish Host MUST be different for every topic you wish to publish to since the SDK will bind to the specific port. 5563 for example cannot be used to publish since EdgeX Core Data has bound to that port. Similarly, you cannot have two separate instances of the app functions SDK running publishing to the same port. In the main() function, note the call to MQTTSend at the end of the pipeline to return the response. from Simple Filter XML MQTT edgexSdk.SetFunctionsPipeline( edgexSdk.DeviceNameFilter(deviceNames), edgexSdk.XMLTransform(), printXMLToConsole, edgexSdk.MQTTSend(addressable, \"\", \"\", 0, false, false), )","title":"Message bus connection configuration"},{"location":"microservices/configuration/Ch-Configuration/","text":"Configuration and Registry Introduction The purpose of this section is to describe the configuration and service registration capabilities of the EdgeX Foundry platform. In all cases unless otherwise specified, the examples provided are based on the reference architecture built using the Go programming language . Configuration Local Configuration Because EdgeX Foundry may be deployed and run in several different ways, it is important to understand how configuration is loaded and from where it is sourced. Referring to the cmd directory within the edgex-go repository , each service has its own folder. Inside each service folder there is a res directory (short for \"resource\"). There you will find the configuration files in TOML format that defines each service's configuration. A service may support several different configuration profiles, such as a \"docker\" profile. In this case, the configuration file located directly in the res directory should be considered the default configuration profile. Sub-directories will contain configurations appropriate to the respective profile. With the exception of the config-seed service, which will be discussed in a moment, a service's configuration profile can be indicated using one of the following command line flags: --profile / -p Taking the core-data service as an example: ./core-data starts the service using the default profile found locally ./core-data --profile=docker starts the service using the docker profile found locally Seeding Configuration When utilizing the registry to provide centralized configuration management for the EdgeX Foundry microservices, it is necessary to seed the required configuration before starting the services. This is the responsibility of the config-seed service. The config-seed service will assume that a service registry is being used and that the necessary endpoint is included in the local configuration file. The use of profiles is supported by the config-seed service in the same manner described above. So for example, if we wanted to seed the registry with docker-related configuration information, we could execute the following after starting the registry: ./config-seed --profile=docker Assuming a successful run, the config-seed will populate the necessary values and then exit. In order for a service to now load the configuration from the registry, we must use one of the following flags: --registry / -r Again, taking the core-data service as an example: ./core-data --registry --profile=docker will start the service using configuration values found in the registry Note that when utilizing the registry, it is optional to also specify the --profile / -p flag if you are using a profile other than the default. This is because the location of the registry must still be obtained from the local config file. At this time, use of multiple profiles at once is not supported. Configuration Structure Configuration information is organized into a hierarchical structure allowing for a logical grouping of services, as well as versioning, beneath an \"edgex\" namespace at root level of the configuration tree. The root namespace separates EdgeX Foundry-related configuration information from other applications that may be using the same registry. Below the root, sub-nodes facilitate grouping of device services, EdgeX core services, security services, etc. As an example, the top-level nodes shown when one views the configuration registry might be as follows: edgex (root namespace) : - core (edgex core services) - devices (device services) - security (security services) Versioning Incorporating versioning into the configuration hierarchy looks like this. edgex (root namespace) : - core * ( edgex core services ) * : - 1.0 : - edgex - core - command - edgex - core - data - edgex - core - metadata - 2.0 - devices * ( device services ) * : - 1.0 : - mqtt - c - mqtt - go - modbus - go - 2.0 The versions shown correspond to major versions of the given services. These are not necessarily equated with long term support (LTS) releases. For all minor/patch versions associated with a major version, the respective service keys live under the major version in configuration (such as 1.0). Changes to the configuration structure that may be required during the associated minor version development cycles can only be additive. That is, key names will not be removed or changed once set in a major version, nor will sections of the configuration tree be moved from one place to another. In this way backward compatibility for the lifetime of the major version is maintained. An advantage of grouping all minor/patch versions under a major version involves end-user configuration changes that need to be persisted during an upgrade. The config-seed will not overwrite existing keys when it runs unless explicitly told to do so. Therefore if a user leaves their configuration registry running during an EdgeX Foundry upgrade, only the new keys required to support the point release will be added to their configuration, leaving any customizations in place. Readable vs Writable Settings Within a given service's configuration, there are keys whose values can be edited and change the behavior of the service while it is running versus those that are effectively read-only. These writable settings are grouped under a given service key. For example, the top-level groupings for edgex-core-data are: /edgex/core/1.0/edgex-core-data/Clients /edgex/core/1.0/edgex-core-data/Databases /edgex/core /1.0/edgex-core-data/Logging /edgex/core/1.0/edgex-core-data/MessageQueue /edgex/core/1.0/edgex-core-data/Registry /edgex/core/1.0/edgex-core-data/Service /edgex/core/1.0/edgex-core-data/Writable Any configuration settings found in the Writable section shown above may be changed and affect a service's behavior without a restart. Any modifications to the other settings would require a restart. Registry The registry refers to any platform you may use for service discovery and centralized configuration management. For the EdgeX Foundry reference implementation, the default provider for both of these responsibilities is HashiCorp's Consul . Integration with the registry is handled through the go-mod-registry module referenced by all services. Introduction to Registry The objective of the registry is to enable microservices to find and to communicate with each other. When each microservice starts up, it registers itself with the registry, and the registry continues checking its availability periodically via a specified health check endpoint. When one microservice needs to connect to another one, it connects to the registry to retrieve the available host name and port number of the target microservice and then invokes the target microservice. The following figure shows the basic flow. Consul is the default registry implementation and provides native features for service registration, service discovery, and health checking. Please refer to the Consul official web site for more information: https://www.consul.io Physically, the \"registry\" and \"configuration\" management services are combined and running on the same Consul server node. Web User Interface A web user interface is also provided by Consul natively. Users can view the available service list and their health status through the web user interface. The web user interface is available at the /ui path on the same port as the HTTP API. By default this is http://localhost:8500/ui . For more detail, please see: https://www.consul.io/intro/getting-started/ui.html Running on Docker For ease of use to install and update, the microservices of EdgeX Foundry are also published as Docker images onto Docker Hub, including Registry: https://hub.docker.com/r/edgexfoundry/docker-core-consul/ After the Docker engine is ready, users can download the latest Consul image by the docker pull command: docker pull edgexfoundry/docker-core-consul Then, startup Consul using Docker container by the Docker run command: docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul -d edgexfoundry/docker-core-consul These are the command steps to start up Consul and import the default configuration data: login to Docker Hub: \\$ docker login A Docker network is needed to enable one Docker container to communicate with another. This is preferred over use of --links that establishes a client-server relationship: \\$ docker network create edgex-network Create a Docker volume container for EdgeX Foundry: \\$ docker run -it --name edgex-files --net=edgex-network -v /data/db -v /edgex/logs -v /consul/config -v /consul/data -d edgexfoundry/docker-edgex-volume Create the Consul container: \\$ docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul --net=edgex-network --volumes-from edgex-files -d edgexfoundry/docker-core-consul Verify the result: http://localhost:8500/ui Running on Local Machine To run Consul on the local machine, requires the following steps: Download the binary from Consul official website: https://www.consul.io/downloads.html . Please choose the correct binary file according to the operation system. Set up the environment variable. Please refer to https://www.consul.io/intro/getting-started/install.html . Execute the following command: \\$ consul agent -data-dir \\${DATA_FOLDER} -ui -advertise 127.0.0.1 -server -bootstrap-expect 1 \\${DATA_FOLDER} could be any folder to put the data files of Consul, and it needs the read/write permission. Verify the result: http://localhost:8500/ui","title":"Configuration and Registry"},{"location":"microservices/configuration/Ch-Configuration/#configuration-and-registry","text":"","title":"Configuration and Registry"},{"location":"microservices/configuration/Ch-Configuration/#introduction","text":"The purpose of this section is to describe the configuration and service registration capabilities of the EdgeX Foundry platform. In all cases unless otherwise specified, the examples provided are based on the reference architecture built using the Go programming language .","title":"Introduction"},{"location":"microservices/configuration/Ch-Configuration/#configuration","text":"","title":"Configuration"},{"location":"microservices/configuration/Ch-Configuration/#local-configuration","text":"Because EdgeX Foundry may be deployed and run in several different ways, it is important to understand how configuration is loaded and from where it is sourced. Referring to the cmd directory within the edgex-go repository , each service has its own folder. Inside each service folder there is a res directory (short for \"resource\"). There you will find the configuration files in TOML format that defines each service's configuration. A service may support several different configuration profiles, such as a \"docker\" profile. In this case, the configuration file located directly in the res directory should be considered the default configuration profile. Sub-directories will contain configurations appropriate to the respective profile. With the exception of the config-seed service, which will be discussed in a moment, a service's configuration profile can be indicated using one of the following command line flags: --profile / -p Taking the core-data service as an example: ./core-data starts the service using the default profile found locally ./core-data --profile=docker starts the service using the docker profile found locally","title":"Local Configuration"},{"location":"microservices/configuration/Ch-Configuration/#seeding-configuration","text":"When utilizing the registry to provide centralized configuration management for the EdgeX Foundry microservices, it is necessary to seed the required configuration before starting the services. This is the responsibility of the config-seed service. The config-seed service will assume that a service registry is being used and that the necessary endpoint is included in the local configuration file. The use of profiles is supported by the config-seed service in the same manner described above. So for example, if we wanted to seed the registry with docker-related configuration information, we could execute the following after starting the registry: ./config-seed --profile=docker Assuming a successful run, the config-seed will populate the necessary values and then exit. In order for a service to now load the configuration from the registry, we must use one of the following flags: --registry / -r Again, taking the core-data service as an example: ./core-data --registry --profile=docker will start the service using configuration values found in the registry Note that when utilizing the registry, it is optional to also specify the --profile / -p flag if you are using a profile other than the default. This is because the location of the registry must still be obtained from the local config file. At this time, use of multiple profiles at once is not supported.","title":"Seeding Configuration"},{"location":"microservices/configuration/Ch-Configuration/#configuration-structure","text":"Configuration information is organized into a hierarchical structure allowing for a logical grouping of services, as well as versioning, beneath an \"edgex\" namespace at root level of the configuration tree. The root namespace separates EdgeX Foundry-related configuration information from other applications that may be using the same registry. Below the root, sub-nodes facilitate grouping of device services, EdgeX core services, security services, etc. As an example, the top-level nodes shown when one views the configuration registry might be as follows: edgex (root namespace) : - core (edgex core services) - devices (device services) - security (security services)","title":"Configuration Structure"},{"location":"microservices/configuration/Ch-Configuration/#versioning","text":"Incorporating versioning into the configuration hierarchy looks like this. edgex (root namespace) : - core * ( edgex core services ) * : - 1.0 : - edgex - core - command - edgex - core - data - edgex - core - metadata - 2.0 - devices * ( device services ) * : - 1.0 : - mqtt - c - mqtt - go - modbus - go - 2.0 The versions shown correspond to major versions of the given services. These are not necessarily equated with long term support (LTS) releases. For all minor/patch versions associated with a major version, the respective service keys live under the major version in configuration (such as 1.0). Changes to the configuration structure that may be required during the associated minor version development cycles can only be additive. That is, key names will not be removed or changed once set in a major version, nor will sections of the configuration tree be moved from one place to another. In this way backward compatibility for the lifetime of the major version is maintained. An advantage of grouping all minor/patch versions under a major version involves end-user configuration changes that need to be persisted during an upgrade. The config-seed will not overwrite existing keys when it runs unless explicitly told to do so. Therefore if a user leaves their configuration registry running during an EdgeX Foundry upgrade, only the new keys required to support the point release will be added to their configuration, leaving any customizations in place.","title":"Versioning"},{"location":"microservices/configuration/Ch-Configuration/#readable-vs-writable-settings","text":"Within a given service's configuration, there are keys whose values can be edited and change the behavior of the service while it is running versus those that are effectively read-only. These writable settings are grouped under a given service key. For example, the top-level groupings for edgex-core-data are: /edgex/core/1.0/edgex-core-data/Clients /edgex/core/1.0/edgex-core-data/Databases /edgex/core /1.0/edgex-core-data/Logging /edgex/core/1.0/edgex-core-data/MessageQueue /edgex/core/1.0/edgex-core-data/Registry /edgex/core/1.0/edgex-core-data/Service /edgex/core/1.0/edgex-core-data/Writable Any configuration settings found in the Writable section shown above may be changed and affect a service's behavior without a restart. Any modifications to the other settings would require a restart.","title":"Readable vs Writable Settings"},{"location":"microservices/configuration/Ch-Configuration/#registry","text":"The registry refers to any platform you may use for service discovery and centralized configuration management. For the EdgeX Foundry reference implementation, the default provider for both of these responsibilities is HashiCorp's Consul . Integration with the registry is handled through the go-mod-registry module referenced by all services.","title":"Registry"},{"location":"microservices/configuration/Ch-Configuration/#introduction-to-registry","text":"The objective of the registry is to enable microservices to find and to communicate with each other. When each microservice starts up, it registers itself with the registry, and the registry continues checking its availability periodically via a specified health check endpoint. When one microservice needs to connect to another one, it connects to the registry to retrieve the available host name and port number of the target microservice and then invokes the target microservice. The following figure shows the basic flow. Consul is the default registry implementation and provides native features for service registration, service discovery, and health checking. Please refer to the Consul official web site for more information: https://www.consul.io Physically, the \"registry\" and \"configuration\" management services are combined and running on the same Consul server node.","title":"Introduction to Registry"},{"location":"microservices/configuration/Ch-Configuration/#web-user-interface","text":"A web user interface is also provided by Consul natively. Users can view the available service list and their health status through the web user interface. The web user interface is available at the /ui path on the same port as the HTTP API. By default this is http://localhost:8500/ui . For more detail, please see: https://www.consul.io/intro/getting-started/ui.html","title":"Web User Interface"},{"location":"microservices/configuration/Ch-Configuration/#running-on-docker","text":"For ease of use to install and update, the microservices of EdgeX Foundry are also published as Docker images onto Docker Hub, including Registry: https://hub.docker.com/r/edgexfoundry/docker-core-consul/ After the Docker engine is ready, users can download the latest Consul image by the docker pull command: docker pull edgexfoundry/docker-core-consul Then, startup Consul using Docker container by the Docker run command: docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul -d edgexfoundry/docker-core-consul These are the command steps to start up Consul and import the default configuration data: login to Docker Hub: \\$ docker login A Docker network is needed to enable one Docker container to communicate with another. This is preferred over use of --links that establishes a client-server relationship: \\$ docker network create edgex-network Create a Docker volume container for EdgeX Foundry: \\$ docker run -it --name edgex-files --net=edgex-network -v /data/db -v /edgex/logs -v /consul/config -v /consul/data -d edgexfoundry/docker-edgex-volume Create the Consul container: \\$ docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul --net=edgex-network --volumes-from edgex-files -d edgexfoundry/docker-core-consul Verify the result: http://localhost:8500/ui","title":"Running on Docker"},{"location":"microservices/configuration/Ch-Configuration/#running-on-local-machine","text":"To run Consul on the local machine, requires the following steps: Download the binary from Consul official website: https://www.consul.io/downloads.html . Please choose the correct binary file according to the operation system. Set up the environment variable. Please refer to https://www.consul.io/intro/getting-started/install.html . Execute the following command: \\$ consul agent -data-dir \\${DATA_FOLDER} -ui -advertise 127.0.0.1 -server -bootstrap-expect 1 \\${DATA_FOLDER} could be any folder to put the data files of Consul, and it needs the read/write permission. Verify the result: http://localhost:8500/ui","title":"Running on Local Machine"},{"location":"microservices/core/command/Ch-Command/","text":"Command Introduction The Core Services Layer microservice Command (often called the Command and Control microservice) enables the issuance of commands or actions to devices and sensors on behalf of: other microservices within EdgeX Foundry (for example, a local edge analytics or rules engine microservice) other applications that may exist on the same system with EdgeX Foundry (for example, a system management agent that needs to shutoff a sensor) To any external system that needs to command those devices (for example, a cloud-based application that had determined the need to modify the settings on a collection of devices) The Command microservice exposes the commands in a common, normalized way to simplify communications with the devices. Commands to devices are made through the command GET, a request for data from the device or sensor, and the command PUT, a request to take action or receive new settings or data from EdgeX Foundry. The Command microservice gets its knowledge about the devices and sensors from the Metadata service. The Command service always relays commands and actions to the devices and sensors through the Device Service and never communicates directly to a device or sensor. Therefore, the Command microservice is a translator of command or action requests from the north side of EdgeX Foundry, such as the rules engine and export facilities, to the protocol-specific device or sensor and associated Device Service side of EdgeX Foundry and the gateway. The Command service provides a layer of protection around devices and sensors by not allowing unwarranted interaction with the devices and sensors through the Device Service. Data Dictionary Class Name Description CommandResponse Contains the target and parameters and expected responses, that describe a REST call. High Level Interaction Diagrams The two following High Level Diagrams show: EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands","title":"Command"},{"location":"microservices/core/command/Ch-Command/#command","text":"","title":"Command"},{"location":"microservices/core/command/Ch-Command/#introduction","text":"The Core Services Layer microservice Command (often called the Command and Control microservice) enables the issuance of commands or actions to devices and sensors on behalf of: other microservices within EdgeX Foundry (for example, a local edge analytics or rules engine microservice) other applications that may exist on the same system with EdgeX Foundry (for example, a system management agent that needs to shutoff a sensor) To any external system that needs to command those devices (for example, a cloud-based application that had determined the need to modify the settings on a collection of devices) The Command microservice exposes the commands in a common, normalized way to simplify communications with the devices. Commands to devices are made through the command GET, a request for data from the device or sensor, and the command PUT, a request to take action or receive new settings or data from EdgeX Foundry. The Command microservice gets its knowledge about the devices and sensors from the Metadata service. The Command service always relays commands and actions to the devices and sensors through the Device Service and never communicates directly to a device or sensor. Therefore, the Command microservice is a translator of command or action requests from the north side of EdgeX Foundry, such as the rules engine and export facilities, to the protocol-specific device or sensor and associated Device Service side of EdgeX Foundry and the gateway. The Command service provides a layer of protection around devices and sensors by not allowing unwarranted interaction with the devices and sensors through the Device Service.","title":"Introduction"},{"location":"microservices/core/command/Ch-Command/#data-dictionary","text":"Class Name Description CommandResponse Contains the target and parameters and expected responses, that describe a REST call.","title":"Data Dictionary"},{"location":"microservices/core/command/Ch-Command/#high-level-interaction-diagrams","text":"The two following High Level Diagrams show: EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands","title":"High Level Interaction Diagrams"},{"location":"microservices/core/data/Ch-CoreData/","text":"Core Data Introduction The Core Data microservice provides a centralized persistence facility for data readings collected by devices and sensors. Device services for devices and sensors that collect data, call on the Core Data service to store the device and sensor data on the edge system (such as in a gateway) until the data can be moved \"north\" and then exported to Enterprise and cloud systems. Other services, such as a Scheduling services, within EdgeX Foundry and potentially outside of EdgeX Foundry, access the device and sensor data stored on the gateway only through the Core Data service. Core Data provides a degree of security and protection of the data collected by devices and sensors while the data is at the edge. Core Data uses a REST API for moving data into and out of the local storage. In the future, the microservice could be expandable to allow data to be accessed via other protocols such as MQTT, AMQP, and so forth. Core Data moves data to the Export Service layer via ZeroMQ by default. An alternate configuration of the Core Data microservice allows the data to be distributed to the Export Services via MQTT, but would also require the installation of a broker such as ActiveMQ. The Rules Engine microservice receives its data from the Export Distribution microservice by default. Where latency or volume are of concern, an alternate configuration of the Rules Engine microservices allows it to also get its data directly from Core Data via ZeroMQ (it becomes a second subscriber to the same Export Services ZeroMQ distribution channel). Core Data \"Streaming\" By default, Core Data does persist all data collected by devices and sensors sent to it. However, when the data is too sensitive to be stored at the edge, or the need is not present for data at the edge to be used by other services locally (e.g. by an analytics microservice), the data can be \"streamed\" through Core Data without persisting it. A configuration change to Core Data (persist.data=false) has Core Data send data to the Export Service, through message queue, without persisting the data locally. This option has the advantage of reducing latency through this layer and storage needs at the network edge, but the cost is having no historical data to use for operations based on changes over time, and only minimal device actuation decisions, based on single event data, at the edge. Data Model The following diagram shows the Data Model for Core Data. Data Dictionary Class Description Event ID Device Identifier Collection of Readings Event has a one-to-many relationship with Reading. Reading name-value pair Examples: \"temp 62\" \"rpm 3000\" The value is an Integer, Decimal, String, or Boolean. The name a value descriptor reference. The value descriptor defines information about the information the Reading should convey. Value Descriptor This specifies a folder to put the log files. High Level Interaction Diagrams The two following High Level Interaction Diagrams show: EdgeX Foundry Core Data add device or sensor readings EdgeX Foundry Core Data request event reading or data for a device Core Data Add Device or Sensor Readings Core Data Request Event Reading or Data for a Device","title":"Core Data"},{"location":"microservices/core/data/Ch-CoreData/#core-data","text":"","title":"Core Data"},{"location":"microservices/core/data/Ch-CoreData/#introduction","text":"The Core Data microservice provides a centralized persistence facility for data readings collected by devices and sensors. Device services for devices and sensors that collect data, call on the Core Data service to store the device and sensor data on the edge system (such as in a gateway) until the data can be moved \"north\" and then exported to Enterprise and cloud systems. Other services, such as a Scheduling services, within EdgeX Foundry and potentially outside of EdgeX Foundry, access the device and sensor data stored on the gateway only through the Core Data service. Core Data provides a degree of security and protection of the data collected by devices and sensors while the data is at the edge. Core Data uses a REST API for moving data into and out of the local storage. In the future, the microservice could be expandable to allow data to be accessed via other protocols such as MQTT, AMQP, and so forth. Core Data moves data to the Export Service layer via ZeroMQ by default. An alternate configuration of the Core Data microservice allows the data to be distributed to the Export Services via MQTT, but would also require the installation of a broker such as ActiveMQ. The Rules Engine microservice receives its data from the Export Distribution microservice by default. Where latency or volume are of concern, an alternate configuration of the Rules Engine microservices allows it to also get its data directly from Core Data via ZeroMQ (it becomes a second subscriber to the same Export Services ZeroMQ distribution channel).","title":"Introduction"},{"location":"microservices/core/data/Ch-CoreData/#core-data-streaming","text":"By default, Core Data does persist all data collected by devices and sensors sent to it. However, when the data is too sensitive to be stored at the edge, or the need is not present for data at the edge to be used by other services locally (e.g. by an analytics microservice), the data can be \"streamed\" through Core Data without persisting it. A configuration change to Core Data (persist.data=false) has Core Data send data to the Export Service, through message queue, without persisting the data locally. This option has the advantage of reducing latency through this layer and storage needs at the network edge, but the cost is having no historical data to use for operations based on changes over time, and only minimal device actuation decisions, based on single event data, at the edge.","title":"Core Data \"Streaming\""},{"location":"microservices/core/data/Ch-CoreData/#data-model","text":"The following diagram shows the Data Model for Core Data.","title":"Data Model"},{"location":"microservices/core/data/Ch-CoreData/#data-dictionary","text":"Class Description Event ID Device Identifier Collection of Readings Event has a one-to-many relationship with Reading. Reading name-value pair Examples: \"temp 62\" \"rpm 3000\" The value is an Integer, Decimal, String, or Boolean. The name a value descriptor reference. The value descriptor defines information about the information the Reading should convey. Value Descriptor This specifies a folder to put the log files.","title":"Data Dictionary"},{"location":"microservices/core/data/Ch-CoreData/#high-level-interaction-diagrams","text":"The two following High Level Interaction Diagrams show: EdgeX Foundry Core Data add device or sensor readings EdgeX Foundry Core Data request event reading or data for a device Core Data Add Device or Sensor Readings Core Data Request Event Reading or Data for a Device","title":"High Level Interaction Diagrams"},{"location":"microservices/core/metadata/Ch-Metadata/","text":"Metadata Introduction The Metadata microservice has the knowledge about the devices and sensors and how to communicate with them that is used by the other services, such as Core Data, Command, and so forth. Specifically, Metadata has the following abilities: Manages information about the devices and sensors connected to, and operated by, EdgeX Foundry Knows the type, and organization of data reported by the devices and sensors Knows how to command the devices and sensors The Metadata does not do the following activities: Does not do, and is not responsible for actual data collection from devices and sensors, which is performed by Device Services and Core Data Does not do, and is not responsible for issuing commands to the devices and sensors, which is performed by Command and Device Service General characteristics about Devices, the data they provide, and how to command them is shown in Device Profiles in EdgeX Foundry. A Device Profile can be thought of as a template of a type or classification of Device. For example, a device profile for BACnet thermostats provides general characteristics for the types of data a BACnet thermostat sends, such as current temperature, and which types of commands or actions can be sent to the BACnet thermostat, such as cooling set point, or heating set point. Therefore, Device Profiles are the first item that the Metadata service must be able to store or manage in local persistence, and provide to the other services of EdgeX Foundry. Data about actual devices and sensors is another type of information that the Metadata microservice stores and manages. Each specific device and sensor that is managed by EdgeX Foundry must be registered with Metadata and have a unique ID associated to it. Information, such as the device's or sensor's address is stored with that identifier. Each device and sensor is also associated to a device profile. This association enables Metadata to apply generic knowledge provided by the device profile to each device and sensor. For example, a specific device such as the BACNet thermostat located in the CTO Solutions lab in Dell's building, is associated to the BACnet thermostat device profile described above and this connection would imply that this specific BACnet thermostat provides current temperature data and responds to commands to set the cooling and heating points. {.align-center} Metadata stores and manages information about the device services that serve as EdgeX Foundry's interfaces to the actual devices and sensors. Device services are other microservices that communicate directly with the device or sensor in the device or sensor protocol of choice, and normalize information and communications with the device or sensor for the rest of EdgeX Foundry. A single Device Service facilitates the communications between EdgeX Foundry and one or more actual devices or sensors. Typically, a Device Service is built to communicate through a particular protocol with devices and sensors that use that protocol. For example, a Modbus Device Service that facilitates the communications among all types of Modbus devices such as motor controllers, proximity sensors, thermostats, power meters, and so forth. {.align-center} Data Models Metadata Command Model Metadata Device and Device Profile Model Metadata Device Profile Model Metadata Provision Watcher Model Data Dictionary Class Name Description Dependencies Action Contains the target and parameters and expected responses, that describe a REST call. Addressable The metadata required to make a request to an EdgeX Foundry target. For example, the Addressable could be HTTP and URL address details to reach a device service by REST and might include attributes such as HTTP Protocol, URL host of edgex-modbus-device-service, port of 49090. AdminState An object\u2019s current administrative state of \u201cLocked\u201d or\u201dUnlocked.\u201d CallbackAlert The object used by the system to alert regarding a change to a system object. Command The REST description of an interface. Device The object that contains information about the state, position, reachability, and methods of interfacing with a Device Top Level object DeviceManager An object that groups other Devices and groups of Devices. DeviceResource The atomic description of a particular protocol level interface for a class of Devices. DeviceProfile The description of both the protocol level interface, device service interface, and mapping and interpretation logic that describe communication to a class of devices. Top Level object DeviceReport DeviceService The current state and reachability information for a registered Device Services OperatingState An object\u2019s current operating state of \u201cEnabled\u201d or \u201cDisabled.\u201d ProfileProperty The transformation, constraint, and unit properties for a class of Device data. ProfileResource The set of operations that is executed by a Service for a particular Command. PropertyValue The transformation and constraint properties for a class of data. ProvisionWatcher The metadata used by a Service for automatically provisioning matching Devices. ResourceOperation An Operation or set of Operations executed by the Device Service on a Device. Response A description of a possible REST response for a Command. Schedule An object defining a timer or alarm. Top Level object ScheduleEvent The action taken by a Service when the schedule triggers. Top Level object Service The current state and reachability information registered for a Service. Units The unit metadata about a class of Device data. High Level Interaction Diagrams Sequence diagrams for some of the more critical or complex events regarding Metadata. The three following High Level Interaction Diagrams show: EdgeX Foundry Metadata Add a New Device Profile (Step 1 to provisioning a new device) EdgeX Foundry Metadata Add a New Device Profile (Step 2 to provisioning a new device) EdgeX Foundry Metadata Device Service Startup Metadata Add a New Device Profile (Step 1 to provisioning a new device)","title":"Metadata"},{"location":"microservices/core/metadata/Ch-Metadata/#metadata","text":"","title":"Metadata"},{"location":"microservices/core/metadata/Ch-Metadata/#introduction","text":"The Metadata microservice has the knowledge about the devices and sensors and how to communicate with them that is used by the other services, such as Core Data, Command, and so forth. Specifically, Metadata has the following abilities: Manages information about the devices and sensors connected to, and operated by, EdgeX Foundry Knows the type, and organization of data reported by the devices and sensors Knows how to command the devices and sensors The Metadata does not do the following activities: Does not do, and is not responsible for actual data collection from devices and sensors, which is performed by Device Services and Core Data Does not do, and is not responsible for issuing commands to the devices and sensors, which is performed by Command and Device Service General characteristics about Devices, the data they provide, and how to command them is shown in Device Profiles in EdgeX Foundry. A Device Profile can be thought of as a template of a type or classification of Device. For example, a device profile for BACnet thermostats provides general characteristics for the types of data a BACnet thermostat sends, such as current temperature, and which types of commands or actions can be sent to the BACnet thermostat, such as cooling set point, or heating set point. Therefore, Device Profiles are the first item that the Metadata service must be able to store or manage in local persistence, and provide to the other services of EdgeX Foundry. Data about actual devices and sensors is another type of information that the Metadata microservice stores and manages. Each specific device and sensor that is managed by EdgeX Foundry must be registered with Metadata and have a unique ID associated to it. Information, such as the device's or sensor's address is stored with that identifier. Each device and sensor is also associated to a device profile. This association enables Metadata to apply generic knowledge provided by the device profile to each device and sensor. For example, a specific device such as the BACNet thermostat located in the CTO Solutions lab in Dell's building, is associated to the BACnet thermostat device profile described above and this connection would imply that this specific BACnet thermostat provides current temperature data and responds to commands to set the cooling and heating points. {.align-center} Metadata stores and manages information about the device services that serve as EdgeX Foundry's interfaces to the actual devices and sensors. Device services are other microservices that communicate directly with the device or sensor in the device or sensor protocol of choice, and normalize information and communications with the device or sensor for the rest of EdgeX Foundry. A single Device Service facilitates the communications between EdgeX Foundry and one or more actual devices or sensors. Typically, a Device Service is built to communicate through a particular protocol with devices and sensors that use that protocol. For example, a Modbus Device Service that facilitates the communications among all types of Modbus devices such as motor controllers, proximity sensors, thermostats, power meters, and so forth. {.align-center}","title":"Introduction"},{"location":"microservices/core/metadata/Ch-Metadata/#data-models","text":"Metadata Command Model Metadata Device and Device Profile Model Metadata Device Profile Model Metadata Provision Watcher Model","title":"Data Models"},{"location":"microservices/core/metadata/Ch-Metadata/#data-dictionary","text":"Class Name Description Dependencies Action Contains the target and parameters and expected responses, that describe a REST call. Addressable The metadata required to make a request to an EdgeX Foundry target. For example, the Addressable could be HTTP and URL address details to reach a device service by REST and might include attributes such as HTTP Protocol, URL host of edgex-modbus-device-service, port of 49090. AdminState An object\u2019s current administrative state of \u201cLocked\u201d or\u201dUnlocked.\u201d CallbackAlert The object used by the system to alert regarding a change to a system object. Command The REST description of an interface. Device The object that contains information about the state, position, reachability, and methods of interfacing with a Device Top Level object DeviceManager An object that groups other Devices and groups of Devices. DeviceResource The atomic description of a particular protocol level interface for a class of Devices. DeviceProfile The description of both the protocol level interface, device service interface, and mapping and interpretation logic that describe communication to a class of devices. Top Level object DeviceReport DeviceService The current state and reachability information for a registered Device Services OperatingState An object\u2019s current operating state of \u201cEnabled\u201d or \u201cDisabled.\u201d ProfileProperty The transformation, constraint, and unit properties for a class of Device data. ProfileResource The set of operations that is executed by a Service for a particular Command. PropertyValue The transformation and constraint properties for a class of data. ProvisionWatcher The metadata used by a Service for automatically provisioning matching Devices. ResourceOperation An Operation or set of Operations executed by the Device Service on a Device. Response A description of a possible REST response for a Command. Schedule An object defining a timer or alarm. Top Level object ScheduleEvent The action taken by a Service when the schedule triggers. Top Level object Service The current state and reachability information registered for a Service. Units The unit metadata about a class of Device data.","title":"Data Dictionary"},{"location":"microservices/core/metadata/Ch-Metadata/#high-level-interaction-diagrams","text":"Sequence diagrams for some of the more critical or complex events regarding Metadata. The three following High Level Interaction Diagrams show: EdgeX Foundry Metadata Add a New Device Profile (Step 1 to provisioning a new device) EdgeX Foundry Metadata Add a New Device Profile (Step 2 to provisioning a new device) EdgeX Foundry Metadata Device Service Startup Metadata Add a New Device Profile (Step 1 to provisioning a new device)","title":"High Level Interaction Diagrams"},{"location":"microservices/device/Ch-DeviceServices/","text":"Device Services Microservices The Device Services Layer interacts with Device Services. Device Services (DS) are the edge connectors interacting with the devices or IoT objects that include, but are not limited to: appliances in your home, alarm systems, HVAC equipment, lighting, machines in any industry, irrigation systems, drones, traffic signals, automated transportation, and so forth. Device services may service one or a number of devices, including sensors, actuators, and so forth, at one time. A \"device\" that a DS manages, could be something other than a simple single physical device and could be another gateway and all of that gateway's devices, a device manager, or a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The Device Services layer's microservices communicate with the devices, sensors, actuators, and other IoT objects through protocols native to the IoT object. The DS Layer converts the data produced and communicated by the IoT object, into a common EdgeX Foundry data structure, and sends that converted data into the Core Services layer, and to other microservices in other layers of EdgeX Foundry. The EdgeX Foundry Device Services layer at this time, includes the following microservice: APIs--Device Services--Virtual Device Service Device Service Requirements Requirements for the device service are provided below. These requirements are being used to define what functionality needs to be offered via any Device Service SDK to produce the device service scaffolding code. They may also help the reader understand the duties and role of a device service. DS-SDK Requirements Device Service Design Sequence Diagrams that outline the objects and process of each of the requirements. Design for Requirements #1-4","title":"Device Services Microservices"},{"location":"microservices/device/Ch-DeviceServices/#device-services-microservices","text":"The Device Services Layer interacts with Device Services. Device Services (DS) are the edge connectors interacting with the devices or IoT objects that include, but are not limited to: appliances in your home, alarm systems, HVAC equipment, lighting, machines in any industry, irrigation systems, drones, traffic signals, automated transportation, and so forth. Device services may service one or a number of devices, including sensors, actuators, and so forth, at one time. A \"device\" that a DS manages, could be something other than a simple single physical device and could be another gateway and all of that gateway's devices, a device manager, or a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The Device Services layer's microservices communicate with the devices, sensors, actuators, and other IoT objects through protocols native to the IoT object. The DS Layer converts the data produced and communicated by the IoT object, into a common EdgeX Foundry data structure, and sends that converted data into the Core Services layer, and to other microservices in other layers of EdgeX Foundry. The EdgeX Foundry Device Services layer at this time, includes the following microservice: APIs--Device Services--Virtual Device Service Device Service Requirements Requirements for the device service are provided below. These requirements are being used to define what functionality needs to be offered via any Device Service SDK to produce the device service scaffolding code. They may also help the reader understand the duties and role of a device service. DS-SDK Requirements Device Service Design Sequence Diagrams that outline the objects and process of each of the requirements. Design for Requirements #1-4","title":"Device Services Microservices"},{"location":"microservices/device/profile/Ch-DeviceProfile/","text":"Device Profile The Device Services Device Profile block diagram shows the Device Profile Object classes.","title":"Device Profile"},{"location":"microservices/device/profile/Ch-DeviceProfile/#device-profile","text":"The Device Services Device Profile block diagram shows the Device Profile Object classes.","title":"Device Profile"},{"location":"microservices/device/sdk/Ch-DeviceSDK/","text":"Device Services SDK Introduction to the SDK The EdgeX Foundry Device Service Software Development Kit (SDK) takes the Developer through the step-by-step process to create an EdgeX Foundry Device Service microservice. Then setup the SDK and execute the code to generate the Device Service scaffolding to get you started using EdgeX. The Device Service SDK supports: Synchronous read and write operations Asynchronous Device data Initialization and deconstruction of Driver Interface Initialization and destruction of Device Connection Framework for automated Provisioning Mechanism Support for multiple classes of Devices with Profiles Support for sets of actions triggered by a command Cached responses to queries Device Service Workflow Key to diagram Colour of Box Description Orange Everything is part of a Base Service. Light Green Initialization. Gets its own configuration and registers itself. Yellow Update Controller. receives, processes, and publishes the update. Dark Blue UInitializing and setting up of schedules. Gray Scaffolding code to be receivers into Device Service. Processes commands. Purple Initializes itself. Set up in metadata. Registers its Device Service discovery process and registration, and sets up Device Services. Gets Device Watchers . When a Device Service first comes up it has its initial set of devices. The Device Watcher waits to receive information that a new device has occurred.!Then a Device Watcher sends metadata messages out about the new device. Dark Green Send data to Core Data. How to communicate with the devices, and on what schedule, and to receive information back from the devices. Writing a Device Service Writing a new Device Service in Go \u3009","title":"Device Services SDK"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#device-services-sdk","text":"","title":"Device Services SDK"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#introduction-to-the-sdk","text":"The EdgeX Foundry Device Service Software Development Kit (SDK) takes the Developer through the step-by-step process to create an EdgeX Foundry Device Service microservice. Then setup the SDK and execute the code to generate the Device Service scaffolding to get you started using EdgeX. The Device Service SDK supports: Synchronous read and write operations Asynchronous Device data Initialization and deconstruction of Driver Interface Initialization and destruction of Device Connection Framework for automated Provisioning Mechanism Support for multiple classes of Devices with Profiles Support for sets of actions triggered by a command Cached responses to queries","title":"Introduction to the SDK"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#device-service-workflow","text":"Key to diagram Colour of Box Description Orange Everything is part of a Base Service. Light Green Initialization. Gets its own configuration and registers itself. Yellow Update Controller. receives, processes, and publishes the update. Dark Blue UInitializing and setting up of schedules. Gray Scaffolding code to be receivers into Device Service. Processes commands. Purple Initializes itself. Set up in metadata. Registers its Device Service discovery process and registration, and sets up Device Services. Gets Device Watchers . When a Device Service first comes up it has its initial set of devices. The Device Watcher waits to receive information that a new device has occurred.!Then a Device Watcher sends metadata messages out about the new device. Dark Green Send data to Core Data. How to communicate with the devices, and on what schedule, and to receive information back from the devices.","title":"Device Service Workflow"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#writing-a-device-service","text":"Writing a new Device Service in Go \u3009","title":"Writing a Device Service"},{"location":"microservices/device/virtual/Ch-VirtualDevice/","text":"Virtual Device Introduction The Virtual Device Service simulates different kinds of devices to generate Events and Readings to the Core Data Microservice, and Users send commands and get responses through the Command and Control Microservice. These features of the Virtual Device Services are useful when executing functional or performance tests without having any real devices. Virtual Device Service Overall Flow Virtual Device Service has dependencies on Core Data and Meta Data Microservices, since the initialization process needs to check or register profiles, devices, and value descriptors. Therefore, Core Data and Metadata Microservices have to fully start up before Virtual Device Service initialization. At the beginning of the Virtual Device Service initialization process, the process sends a ping request to Core Data and Metadata Microservices to verify their status until they both fully start up. The fixed time out limit is 600 seconds. After 600 seconds, if Core Data and Metadata Microservices have not fully started up, the initialization process fails. When the Virtual Device Service starts up, the initialization process loads the definitions of ValueDescriptor, DeviceService, and DeviceProfile from YAML files and creates them in the Metadata Microservice. Some default YAML files exist, and Users can create their own. Please see Device Profile Definition section (on this page) for more detail. In addition, the Virtual Device Service provides callback APIs for the Metadata Microservice to manage the Device instances. According to the GET Commands in the DeviceProfile definitions, there is an H2 database (in-memory) storing resource in Virtual Devices, called \"Virtual Resources.\" Virtual Resources Example After starting up, the Virtual Device Service reads the H2 database and periodically sends the current value as an Event to the Core Data Microservice. The default frequency is 15 seconds, and that can be modified from the configuration file. The current value of virtual resources is re-generated at a random value before each collection cycle, so the Reading should be different in each Event. The Virtual Device Service provides APIs for Command and Control Microservice, reads the H2 database, and returns the current value for GET Commands. At this time, the Virtual Device Service supports two special PUT Commands: \"enableRandomization\" and \"collectionFrequency.\" These Commands modify the \"ENABLE_RANDOMIZATION\" and \"COLLECTION_FREQUENCY\" column of a specific virtual resource in the H2 database. For example , sending an HTTP PUT method to http : // localhost : 48082 / device / 56 b1acf1d66f9c9762581ea4 / command / 56 b1acedd66f9c9762581e9d / put / 0 with the sample payload : { \"enableRandomization\" : false , \"collectionFrequency\" : 6 } modifies a virtual resource record whose command ID is \"56b1acedd66f9c9762581e9d \" and device ID is \"56b1acf1d66f9c9762581ea4.\" By modifying the \"ENABLE_RANDOMIZATION\" column to FALSE, the value of the virtual resource will not re-generate a random value anymore. By modifying the \"COLLECTION_FREQUENCY\" column, the collecting frequency will be changed after next collection cycle. They both can be modified manually through H2 Console: http://localhost:49990/console Modify the \u201cJDBC URL\u201d to jdbc:h2:mem:testdb, and click on \u201cConnect.\u201d Special Configuration The virtual device micro service does contain some unique configuration properties and these often change between development (for example when run from Eclipse) and the containerized version (i.e. Docker container) of the same service. Here are the list of unique properties you should investigate before running the virtual device service to better understand how it works in your environment: The path used to locate the Device Profile YAML files used to define the virtual devices managed by the device service (see Device Profile Definition section below). application.device-profile-paths=./bacnet_sample_profiles,./modbus_sample_profiles Indicator to the virtual device service to provisioning devices from the YAML profiles in the above directory, creating one device for each profile automatically when set to true. application.auto-create-device=true When developing, it is often advantageous to have the virtual device service start afresh and with a clean Meta data database each time the service starts. The property below indicates whether the device service should clean out any existing virtual devices in the database when it shuts down so a clean environment is available when the service starts backup. Typically set to true for development and false for runtime/demonstration environments. application.auto-cleanup=true How often, in seconds, the virtual device service's scheduler should collect data from the virtual device. application.collection-frequency=15 Service Name and Host Name In EdgeX device services the service name (which is represented by service.name key in the application.properties file or Consul configuration) is the identity of the Device Service object. The name is used by EdgeX to attribute all the information about the service (in particular schedules, device ownership, etc.) to this name. However, the service.host parameter is used to describe how to interact with the service. Depending on your operating mode, the following guidelines for configuring the service host apply. Deployment Mode (running everything containerized in Docker): The Service host (which is represented by the service.host key in the application.properties file or Consul configuration) is the DNS or IP address networking entry of the entity that the service is bound to (container, machine, etc) and reachable from the other microservices. This allows a full location URL for the service to be determined. In Docker environments, the host name is the name of the Docker container running the microservice (such as edgex-device-virtual). Use service.host=\\${service.name} and the docker-compose file for all services (default). Important Note: be sure to use Docker Compose and docker-compose file (found in the developer-scripts repos) to bring up the containers for all services. Docker Compose establishes the networking and container naming for you, which can otherwise be difficult to do and prone to errors if bringing up containers manually. Developer Mode (running everything natively): When running a service natively, the service names will not resolve to a DNS entry as they will in a Docker environment. Use service.host=localhost for all services (default). Hybrid Mode (running some services natively with the rest deployed with Docker): Use service.host=\\<Host Machine IP Address> for the native services (manual configuration) and the docker-compose file to bring up the containerized services (default). Ensure that Addressable objects for the native services are not accidentally created by bringing them up with the docker-compose file, otherwise conflicts may arise. This issue is being addressed in future versions. System Architecture The Virtual Device Service adopts a normal MVC design pattern, separating logic into different layers. System Architecture Graphic Interface Layer --interacting with other microservices. Controllers provide the RESTful API. The implementation is located in org.edgexfoundry.device.virtual.controller package. Scheduled Collection Tasks is a set of async tasks which is executed periodically, and they are created for each virtual resource (GET Command). See org.edgexfoundry.device.virtual.scheduling package for the detailed implementation. Also, org.edgexfoundry.device.virtual.scheduling.Scheduler reads Schedule and ScheduleEvent from Meta Data Microservice and arranges the collection tasks. Tasks execution logic is located in org.edgexfoundry.device.virtual.service.impl.CollectionTaskExecutorImpl, and the tasks creation behavior is located in org.edgexfoundry.device.virtual.service.impl.VirtualResourceManagerImpl.createDefaultRecords(). Service Layer --processing business logic, such as, executing collection tasks and commands, managing profiles and devices, and so forth. See org.edgexfoundry.device.virtual.service.impl package for more details. DAO Layer --processing protocol access. For Virtual Device Services, a Spring Data JPA interface in org.edgexfoundry.device.virtual.dao package. Spring framework will process the communication effort to access H2 DB. Data Layer --an H2 DB to simulate device resources. Device Profile Definition Users can define any virtual device profile in YAML format, if the structure is in accordance with the \"Device and Device Profile Model\" (in the graphic 3 paragraphs below). By assigning the file path to application property \"application.device-profile-paths\" , Virtual Device Service loads all the YAML files under this folder, and this property accepts multiple values separated by comma (,). For instance, the following setting causes Virtual Device Service to load all YAML files under ./bacnet_sample_profiles and ./modbus_sample_profiles folders. In addition to Profile Definition, ValueDescriptors are defined in the \"deviceResources.properties\" part of the profile definition. The structure needs to conform with the ProfileProperty in \"Device Profile\" (in the graphic below the \"Device and Device Profile Model\"), and the ValueDescriptors will be created and send to the Core Data Microservice during the Device creation callback process. By assigning the application property \"application.auto-create-device\" = true (the default value is true), the Virtual Device Service creates Device instances for each profile automatically during starting up, and the Device instances start sending events and readings to Core Data Microservice. Data Model Virtual Device Service Data Model--Device and Device Profile Model Virtual Device Service Data Model--Command Model VirtualResource VirtualResource is the data object generated from Device instances and persisted in H2 database. Data Dictionary Class Name Description ScanList The object containing a protocol discovery method query. Transaction The asynchronous helper object used for gathering sets of device responses.","title":"Virtual Device"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#virtual-device","text":"","title":"Virtual Device"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#introduction","text":"The Virtual Device Service simulates different kinds of devices to generate Events and Readings to the Core Data Microservice, and Users send commands and get responses through the Command and Control Microservice. These features of the Virtual Device Services are useful when executing functional or performance tests without having any real devices. Virtual Device Service Overall Flow Virtual Device Service has dependencies on Core Data and Meta Data Microservices, since the initialization process needs to check or register profiles, devices, and value descriptors. Therefore, Core Data and Metadata Microservices have to fully start up before Virtual Device Service initialization. At the beginning of the Virtual Device Service initialization process, the process sends a ping request to Core Data and Metadata Microservices to verify their status until they both fully start up. The fixed time out limit is 600 seconds. After 600 seconds, if Core Data and Metadata Microservices have not fully started up, the initialization process fails. When the Virtual Device Service starts up, the initialization process loads the definitions of ValueDescriptor, DeviceService, and DeviceProfile from YAML files and creates them in the Metadata Microservice. Some default YAML files exist, and Users can create their own. Please see Device Profile Definition section (on this page) for more detail. In addition, the Virtual Device Service provides callback APIs for the Metadata Microservice to manage the Device instances. According to the GET Commands in the DeviceProfile definitions, there is an H2 database (in-memory) storing resource in Virtual Devices, called \"Virtual Resources.\"","title":"Introduction"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#virtual-resources-example","text":"After starting up, the Virtual Device Service reads the H2 database and periodically sends the current value as an Event to the Core Data Microservice. The default frequency is 15 seconds, and that can be modified from the configuration file. The current value of virtual resources is re-generated at a random value before each collection cycle, so the Reading should be different in each Event. The Virtual Device Service provides APIs for Command and Control Microservice, reads the H2 database, and returns the current value for GET Commands. At this time, the Virtual Device Service supports two special PUT Commands: \"enableRandomization\" and \"collectionFrequency.\" These Commands modify the \"ENABLE_RANDOMIZATION\" and \"COLLECTION_FREQUENCY\" column of a specific virtual resource in the H2 database. For example , sending an HTTP PUT method to http : // localhost : 48082 / device / 56 b1acf1d66f9c9762581ea4 / command / 56 b1acedd66f9c9762581e9d / put / 0 with the sample payload : { \"enableRandomization\" : false , \"collectionFrequency\" : 6 } modifies a virtual resource record whose command ID is \"56b1acedd66f9c9762581e9d \" and device ID is \"56b1acf1d66f9c9762581ea4.\" By modifying the \"ENABLE_RANDOMIZATION\" column to FALSE, the value of the virtual resource will not re-generate a random value anymore. By modifying the \"COLLECTION_FREQUENCY\" column, the collecting frequency will be changed after next collection cycle. They both can be modified manually through H2 Console: http://localhost:49990/console Modify the \u201cJDBC URL\u201d to jdbc:h2:mem:testdb, and click on \u201cConnect.\u201d","title":"Virtual Resources Example"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#special-configuration","text":"The virtual device micro service does contain some unique configuration properties and these often change between development (for example when run from Eclipse) and the containerized version (i.e. Docker container) of the same service. Here are the list of unique properties you should investigate before running the virtual device service to better understand how it works in your environment: The path used to locate the Device Profile YAML files used to define the virtual devices managed by the device service (see Device Profile Definition section below). application.device-profile-paths=./bacnet_sample_profiles,./modbus_sample_profiles Indicator to the virtual device service to provisioning devices from the YAML profiles in the above directory, creating one device for each profile automatically when set to true. application.auto-create-device=true When developing, it is often advantageous to have the virtual device service start afresh and with a clean Meta data database each time the service starts. The property below indicates whether the device service should clean out any existing virtual devices in the database when it shuts down so a clean environment is available when the service starts backup. Typically set to true for development and false for runtime/demonstration environments. application.auto-cleanup=true How often, in seconds, the virtual device service's scheduler should collect data from the virtual device. application.collection-frequency=15 Service Name and Host Name In EdgeX device services the service name (which is represented by service.name key in the application.properties file or Consul configuration) is the identity of the Device Service object. The name is used by EdgeX to attribute all the information about the service (in particular schedules, device ownership, etc.) to this name. However, the service.host parameter is used to describe how to interact with the service. Depending on your operating mode, the following guidelines for configuring the service host apply. Deployment Mode (running everything containerized in Docker): The Service host (which is represented by the service.host key in the application.properties file or Consul configuration) is the DNS or IP address networking entry of the entity that the service is bound to (container, machine, etc) and reachable from the other microservices. This allows a full location URL for the service to be determined. In Docker environments, the host name is the name of the Docker container running the microservice (such as edgex-device-virtual). Use service.host=\\${service.name} and the docker-compose file for all services (default). Important Note: be sure to use Docker Compose and docker-compose file (found in the developer-scripts repos) to bring up the containers for all services. Docker Compose establishes the networking and container naming for you, which can otherwise be difficult to do and prone to errors if bringing up containers manually. Developer Mode (running everything natively): When running a service natively, the service names will not resolve to a DNS entry as they will in a Docker environment. Use service.host=localhost for all services (default). Hybrid Mode (running some services natively with the rest deployed with Docker): Use service.host=\\<Host Machine IP Address> for the native services (manual configuration) and the docker-compose file to bring up the containerized services (default). Ensure that Addressable objects for the native services are not accidentally created by bringing them up with the docker-compose file, otherwise conflicts may arise. This issue is being addressed in future versions.","title":"Special Configuration"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#system-architecture","text":"The Virtual Device Service adopts a normal MVC design pattern, separating logic into different layers. System Architecture Graphic Interface Layer --interacting with other microservices. Controllers provide the RESTful API. The implementation is located in org.edgexfoundry.device.virtual.controller package. Scheduled Collection Tasks is a set of async tasks which is executed periodically, and they are created for each virtual resource (GET Command). See org.edgexfoundry.device.virtual.scheduling package for the detailed implementation. Also, org.edgexfoundry.device.virtual.scheduling.Scheduler reads Schedule and ScheduleEvent from Meta Data Microservice and arranges the collection tasks. Tasks execution logic is located in org.edgexfoundry.device.virtual.service.impl.CollectionTaskExecutorImpl, and the tasks creation behavior is located in org.edgexfoundry.device.virtual.service.impl.VirtualResourceManagerImpl.createDefaultRecords(). Service Layer --processing business logic, such as, executing collection tasks and commands, managing profiles and devices, and so forth. See org.edgexfoundry.device.virtual.service.impl package for more details. DAO Layer --processing protocol access. For Virtual Device Services, a Spring Data JPA interface in org.edgexfoundry.device.virtual.dao package. Spring framework will process the communication effort to access H2 DB. Data Layer --an H2 DB to simulate device resources.","title":"System Architecture"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#device-profile-definition","text":"Users can define any virtual device profile in YAML format, if the structure is in accordance with the \"Device and Device Profile Model\" (in the graphic 3 paragraphs below). By assigning the file path to application property \"application.device-profile-paths\" , Virtual Device Service loads all the YAML files under this folder, and this property accepts multiple values separated by comma (,). For instance, the following setting causes Virtual Device Service to load all YAML files under ./bacnet_sample_profiles and ./modbus_sample_profiles folders. In addition to Profile Definition, ValueDescriptors are defined in the \"deviceResources.properties\" part of the profile definition. The structure needs to conform with the ProfileProperty in \"Device Profile\" (in the graphic below the \"Device and Device Profile Model\"), and the ValueDescriptors will be created and send to the Core Data Microservice during the Device creation callback process. By assigning the application property \"application.auto-create-device\" = true (the default value is true), the Virtual Device Service creates Device instances for each profile automatically during starting up, and the Device instances start sending events and readings to Core Data Microservice.","title":"Device Profile Definition"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#data-model","text":"Virtual Device Service Data Model--Device and Device Profile Model Virtual Device Service Data Model--Command Model VirtualResource VirtualResource is the data object generated from Device instances and persisted in H2 database.","title":"Data Model"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#data-dictionary","text":"Class Name Description ScanList The object containing a protocol discovery method query. Transaction The asynchronous helper object used for gathering sets of device responses.","title":"Data Dictionary"},{"location":"microservices/security/Ch-APIGateway/","text":"API Gateway The security API gateway is the single point of entry for all EdgeX REST traffic. It is the barrier between external clients and the EdgeX microservices preventing unauthorized access to EdgeX REST APIs. The API gateway accepts client requests, verifies the identity of the clients, redirects the requests to correspondent microservice and relays the results back to the client. The API Gateway provides an HTTP REST interface for administration management. The administrative management offers the means to configure API routing, as well as client authentication and access control. This configuration is store in an embedded database. KONG ( https://konghq.com/ ) is the product underlying the API gateway. The EdgeX community has added code to initialize the KONG environment, set up service routes for EdgeX microservices, and add various authentication/authorization mechanisms including JWT authentication, OAuth2 authentication and ACL. Start the API Gateway Start the API gateway with Docker Compose and a Docker Compose manifest file (the Docker Compose file named docker-compose-nexus-{redis,mongo}.yml (or -arm64 variabnts) found at https://github.com/edgexfoundry/developer-scripts/tree/master/releases/geneva/compose-files )). This Compose file starts all of EdgeX including the security services. The command to start EdgeX inclusive of API gateway related services is: : docker-compose up -d For debugging purpose, the API gateway services can be started individually with these commands used in sequence after secret store starts successfully. Lines starts with # are comments to explain the purpose of the command. : docker - compose up - d kong - db # start up backend database for API gateway docker - compose up - d kong - migrations # initialize the backend database for API gateway docker - compose up - d kong # start up KONG the major component of API gateway docker - compose up - d edgex - proxy # initialize KONG , configure proxy routes , apply certificates to routes , and enable various authentication / ACL features . If the last command returns an error message for any reason (such as incorrect configuration file), the API gateway may be in an unstable status. The following command can be used to stop and remove the containers. : docker-compose down # stop and remove the containers After stopping and removing the containers, you can attempt to recreate and start them again. Alternatively you can use the command to reset the API gateway as shown below: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --reset=true After issuing the reset command, attempt to start and reinitialize with the command below. : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --init=true You can learn more about these commands, to include some additional options by running: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013h On successfully starting EdgeX with the API Gateway services, the list of running containers should close follow the listing shown below. Note key security service containers like kong, kong-db, edgex-vault are listed. Configuring API Gateway The API gateway supports two different forms of authentication: JSON Web Token (JWT) or OAuth2 Authentication. Only one authentication method can be enabled at a time. The API Gateway also supports an Access Control List (ACL) which can be enabled with one of the authentication methods mentioned earlier for fine control among the groups. The authentication and ACL need to be specified in the API gateway's configuration file. Setup of authentication and access control occurs automatically as part of API gateway initialization. The configuration file can be found at https://github.com/edgexfoundry/security-api-gateway/blob/master/core/res/configuration-docker.toml Configuration of JWT Authentication for API Gateway When using JWT Authentication, the [kongauth] section needs to be specified in the configuration file as shown below. : [kongauth] name = \"jwt\" Configuration of OAuth2 Authentication for API Gateway When using OAuth2 Authentication, the [kongauth] section needs to specify oauth2 in the configuration file as shown below. Note, today EdgeX only supports \"client credential\" authentication (specified in \"granttype\") currently for OAuth. [kongauth] name = \"oauth2\" scopes = \"email,phone,address\" mandatoryscope = \"true\" enableclientcredentials = \"true\" clientid = \"test\" clientsecret = \"test\" redirecturi = \"http://edgex.com\" granttype = \"client_credentials\" scopegranted = \"email\" resource = \"coredata\" Configuration of ACL for API Gateway Access control is also specified in the configuration file as shown below. Note, users that belong to the whitelist will have access to the resources of EdgeX, and users not belonging to the group listed here will be denied when trying to access resources through the API Gateway. : [kongacl] name = \"acl\" whitelist = \"admin,user\" Using API Gateway Resource Mapping between EdgeX Microservice and API gateway If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Once the API gateway is started and initialized successfully, and all the common ports for EdgeX microservices are blocked by disabling the exposed external ports of the EdgeX microservices through updating the docker compose file, the EdgeX microservice will be behind the gateway. At this time both the microservice host/IP Address (\\<core-data-microservice-ip> in the example) as well as the service port (48080 in the example) are not available to external access. EdgeX uses the gateway as a single entry point for all the REST APIs. With the API gateway in place, the curl command to ping the endpoint of the same Core Data service, as shown above, needs to change to : : curl https://<api-gateway-host-ip>:8443/coredata/api/v1/ping Comparing these two curl commands you may notice several differences. \"Http\" is switched to \"https\" as we enable the SSL/TLS for secure communication. This applies to any client side request. The EdgeX microservice IP address where the request is sent changed to the host/IP address of API gateway service (recall the API gateway becomes the single entry point for all the EdgeX micro services). The API gateway will eventually lateral the request to the Core Data service if the client is authorized. The port of the request is switched from 48080 to 8443, which is the default SSL/TLS port for API gateway (versus the micro service port). This applies to any client side request. The \"/coredata/\" path in the URL is used to identify which EdgeX micro service the request is routed to. As each EdgeX micro service has a dedicated service port open that accepts incoming requests, there is a mapping table kept by the API gateway that maps paths to micro service ports. A partial listing of the map between ports and URL paths is shown in the table below. EdgeX microservice Name Port number Partial URL coredata 48080 coredata metadata 48081 metadata command 48082 command notifications 48060 notifications supportlogging 48061 supportlogging Creating Access Token for API Gateway Authentication If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Again, the request doesn't include client identity information. Once the API gateway is started and initialized successfully, the EdgeX microservice REST APIs will be behind the gateway, an access token must be attached with any client-side HTTP request for identity verification and authentication done by the API gateway. This access token is different from the access token of secret store even though they have the same name. The purpose of the access token for the API gateway is to identity clients that send the requests to consume the REST API of EdgeX. The secret store will then use the token to verify the identity of clients that send the request to access the secrets of EdgeX that are stored in the secret store. To obtain an access token for a client, a user that is associated with the client as well as a group that the user belongs to needs to be added into the API gateway. The command to add a user and the group is: : docker-compose -f docker-compose-nexus-mongo.yml run --rm --entrypoint /edgex/security-proxy-setup edgex-proxy --init=false --useradd=<user> --group=<groupname> The command above will return an access token that can then be used by the client to access the EdgeX REST API resources. Depending on the choice of authentication method, the format of the access token will be something like this if JWT is enabled: : eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiI5M3V3cmZBc0xzS2Qwd1JnckVFdlRzQloxSmtYOTRRciIsImFjY291bnQiOiJhZG1pbmlzdHJhdG9yIn0.em8ffitqrd59_DeYKfQkTZGtUA1T99NikETwtedOgHQ Alternatively, the access token may look like what is shown below if the OAuth2 is enabled: : MNsBh6jDDSxaECzUtimW1nDSvI2v0xsZ If a client needs to be disabled and the client's token invalidated, use the command here to remove/delete the user: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013-userdel=<user> Using API Gateway to Proxy Existing EdgeX Microservices Once the resource mapping and access token to API gateway are in place, a client can use the access token to use the protected EdgeX REST API resources behind the API gateway. Again, without the API Gateway in place, here is the sample request to hit the ping endpoint of the EdgeX Core Data microservice using curl: : curl http://<core-data-microservice-ip>:48080/api/v1/ping With the security service and JWT authentication is enabled, the command changes to: : curl \u2013H \u201chost: edgex\u201d https://<api-gateway-service-ip>:8443/coredata/api/v1/ping? -H \"Authorization: Bearer <access-token>\u201d In summary the difference between the two commands are listed below: --H \"host: edgex\" is used to indicate that the request is for EdgeX domain as the API gateway could be used to take requests for different domains. Use the https versus http protocol identifier for SSL/TLS secure communication. The service port 8443 is the default TLS service port of API gateway Use the URL path \"coredata\" to indicate which EdgeX microservice the request is routed to Use header of -H \"Authorization: Bearer \\<access-token>\" to specify the access token associated with the client that was generated when the client was added. The format for OAuth2 authentication is similar. For OAuth2 use the bearer token from OAuth2 authentication instead of the JWT token. Here is an example of the curl command using OAuth2: : curl \u2013H \"host: edgex\" https://<api-gateway-service-ip>:8443/coredata/api/v1/ping -H \"Authorization:bearer <access-token>\"","title":"API Gateway"},{"location":"microservices/security/Ch-APIGateway/#api-gateway","text":"The security API gateway is the single point of entry for all EdgeX REST traffic. It is the barrier between external clients and the EdgeX microservices preventing unauthorized access to EdgeX REST APIs. The API gateway accepts client requests, verifies the identity of the clients, redirects the requests to correspondent microservice and relays the results back to the client. The API Gateway provides an HTTP REST interface for administration management. The administrative management offers the means to configure API routing, as well as client authentication and access control. This configuration is store in an embedded database. KONG ( https://konghq.com/ ) is the product underlying the API gateway. The EdgeX community has added code to initialize the KONG environment, set up service routes for EdgeX microservices, and add various authentication/authorization mechanisms including JWT authentication, OAuth2 authentication and ACL.","title":"API Gateway"},{"location":"microservices/security/Ch-APIGateway/#start-the-api-gateway","text":"Start the API gateway with Docker Compose and a Docker Compose manifest file (the Docker Compose file named docker-compose-nexus-{redis,mongo}.yml (or -arm64 variabnts) found at https://github.com/edgexfoundry/developer-scripts/tree/master/releases/geneva/compose-files )). This Compose file starts all of EdgeX including the security services. The command to start EdgeX inclusive of API gateway related services is: : docker-compose up -d For debugging purpose, the API gateway services can be started individually with these commands used in sequence after secret store starts successfully. Lines starts with # are comments to explain the purpose of the command. : docker - compose up - d kong - db # start up backend database for API gateway docker - compose up - d kong - migrations # initialize the backend database for API gateway docker - compose up - d kong # start up KONG the major component of API gateway docker - compose up - d edgex - proxy # initialize KONG , configure proxy routes , apply certificates to routes , and enable various authentication / ACL features . If the last command returns an error message for any reason (such as incorrect configuration file), the API gateway may be in an unstable status. The following command can be used to stop and remove the containers. : docker-compose down # stop and remove the containers After stopping and removing the containers, you can attempt to recreate and start them again. Alternatively you can use the command to reset the API gateway as shown below: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --reset=true After issuing the reset command, attempt to start and reinitialize with the command below. : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --init=true You can learn more about these commands, to include some additional options by running: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013h On successfully starting EdgeX with the API Gateway services, the list of running containers should close follow the listing shown below. Note key security service containers like kong, kong-db, edgex-vault are listed.","title":"Start the API Gateway"},{"location":"microservices/security/Ch-APIGateway/#configuring-api-gateway","text":"The API gateway supports two different forms of authentication: JSON Web Token (JWT) or OAuth2 Authentication. Only one authentication method can be enabled at a time. The API Gateway also supports an Access Control List (ACL) which can be enabled with one of the authentication methods mentioned earlier for fine control among the groups. The authentication and ACL need to be specified in the API gateway's configuration file. Setup of authentication and access control occurs automatically as part of API gateway initialization. The configuration file can be found at https://github.com/edgexfoundry/security-api-gateway/blob/master/core/res/configuration-docker.toml Configuration of JWT Authentication for API Gateway When using JWT Authentication, the [kongauth] section needs to be specified in the configuration file as shown below. : [kongauth] name = \"jwt\" Configuration of OAuth2 Authentication for API Gateway When using OAuth2 Authentication, the [kongauth] section needs to specify oauth2 in the configuration file as shown below. Note, today EdgeX only supports \"client credential\" authentication (specified in \"granttype\") currently for OAuth. [kongauth] name = \"oauth2\" scopes = \"email,phone,address\" mandatoryscope = \"true\" enableclientcredentials = \"true\" clientid = \"test\" clientsecret = \"test\" redirecturi = \"http://edgex.com\" granttype = \"client_credentials\" scopegranted = \"email\" resource = \"coredata\" Configuration of ACL for API Gateway Access control is also specified in the configuration file as shown below. Note, users that belong to the whitelist will have access to the resources of EdgeX, and users not belonging to the group listed here will be denied when trying to access resources through the API Gateway. : [kongacl] name = \"acl\" whitelist = \"admin,user\"","title":"Configuring API Gateway"},{"location":"microservices/security/Ch-APIGateway/#using-api-gateway","text":"Resource Mapping between EdgeX Microservice and API gateway If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Once the API gateway is started and initialized successfully, and all the common ports for EdgeX microservices are blocked by disabling the exposed external ports of the EdgeX microservices through updating the docker compose file, the EdgeX microservice will be behind the gateway. At this time both the microservice host/IP Address (\\<core-data-microservice-ip> in the example) as well as the service port (48080 in the example) are not available to external access. EdgeX uses the gateway as a single entry point for all the REST APIs. With the API gateway in place, the curl command to ping the endpoint of the same Core Data service, as shown above, needs to change to : : curl https://<api-gateway-host-ip>:8443/coredata/api/v1/ping Comparing these two curl commands you may notice several differences. \"Http\" is switched to \"https\" as we enable the SSL/TLS for secure communication. This applies to any client side request. The EdgeX microservice IP address where the request is sent changed to the host/IP address of API gateway service (recall the API gateway becomes the single entry point for all the EdgeX micro services). The API gateway will eventually lateral the request to the Core Data service if the client is authorized. The port of the request is switched from 48080 to 8443, which is the default SSL/TLS port for API gateway (versus the micro service port). This applies to any client side request. The \"/coredata/\" path in the URL is used to identify which EdgeX micro service the request is routed to. As each EdgeX micro service has a dedicated service port open that accepts incoming requests, there is a mapping table kept by the API gateway that maps paths to micro service ports. A partial listing of the map between ports and URL paths is shown in the table below. EdgeX microservice Name Port number Partial URL coredata 48080 coredata metadata 48081 metadata command 48082 command notifications 48060 notifications supportlogging 48061 supportlogging Creating Access Token for API Gateway Authentication If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Again, the request doesn't include client identity information. Once the API gateway is started and initialized successfully, the EdgeX microservice REST APIs will be behind the gateway, an access token must be attached with any client-side HTTP request for identity verification and authentication done by the API gateway. This access token is different from the access token of secret store even though they have the same name. The purpose of the access token for the API gateway is to identity clients that send the requests to consume the REST API of EdgeX. The secret store will then use the token to verify the identity of clients that send the request to access the secrets of EdgeX that are stored in the secret store. To obtain an access token for a client, a user that is associated with the client as well as a group that the user belongs to needs to be added into the API gateway. The command to add a user and the group is: : docker-compose -f docker-compose-nexus-mongo.yml run --rm --entrypoint /edgex/security-proxy-setup edgex-proxy --init=false --useradd=<user> --group=<groupname> The command above will return an access token that can then be used by the client to access the EdgeX REST API resources. Depending on the choice of authentication method, the format of the access token will be something like this if JWT is enabled: : eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiI5M3V3cmZBc0xzS2Qwd1JnckVFdlRzQloxSmtYOTRRciIsImFjY291bnQiOiJhZG1pbmlzdHJhdG9yIn0.em8ffitqrd59_DeYKfQkTZGtUA1T99NikETwtedOgHQ Alternatively, the access token may look like what is shown below if the OAuth2 is enabled: : MNsBh6jDDSxaECzUtimW1nDSvI2v0xsZ If a client needs to be disabled and the client's token invalidated, use the command here to remove/delete the user: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013-userdel=<user> Using API Gateway to Proxy Existing EdgeX Microservices Once the resource mapping and access token to API gateway are in place, a client can use the access token to use the protected EdgeX REST API resources behind the API gateway. Again, without the API Gateway in place, here is the sample request to hit the ping endpoint of the EdgeX Core Data microservice using curl: : curl http://<core-data-microservice-ip>:48080/api/v1/ping With the security service and JWT authentication is enabled, the command changes to: : curl \u2013H \u201chost: edgex\u201d https://<api-gateway-service-ip>:8443/coredata/api/v1/ping? -H \"Authorization: Bearer <access-token>\u201d In summary the difference between the two commands are listed below: --H \"host: edgex\" is used to indicate that the request is for EdgeX domain as the API gateway could be used to take requests for different domains. Use the https versus http protocol identifier for SSL/TLS secure communication. The service port 8443 is the default TLS service port of API gateway Use the URL path \"coredata\" to indicate which EdgeX microservice the request is routed to Use header of -H \"Authorization: Bearer \\<access-token>\" to specify the access token associated with the client that was generated when the client was added. The format for OAuth2 authentication is similar. For OAuth2 use the bearer token from OAuth2 authentication instead of the JWT token. Here is an example of the curl command using OAuth2: : curl \u2013H \"host: edgex\" https://<api-gateway-service-ip>:8443/coredata/api/v1/ping -H \"Authorization:bearer <access-token>\"","title":"Using API Gateway"},{"location":"microservices/security/Ch-AccessEdgeXRESTResources/","text":"Access EdgeX REST resources When the EdgeX API Gateway is used, access to the micro service APIs must go through the reverse proxy. Requestors of an EdgeX REST endpoints must therefore change the URL they use to access the services. Example below explain how to map the non-secured micro service URLs with reverse proxy protected URLS. To access the ping endpoint of an EdgeX micro service (using the command service as an example), the URL is http://edgex-command-service:48082/api/v1/ping With API gateway serving as the single access point for the EdgeX services, the ping URL is https://api-gateway-server:8443/command/api/v1/ping?jwt= \\<JWT-Token> Please notice that there are 4 major differences when comparing the URLs above Switch from http to https as the API Gateway server enables https The host address and port are switched from original micro service host address and port to a common api gateway service address and 8443 port as the api gateway server will serve as the single point for all the EdgeX services Use the name of the service (in this case \"command\") within the URL to indicate that the request is to be routed to the appropriate EdgeX service (command in this example) Add a JWT as part of the URL as all the REST resources are protected by either OAuth2 or JWT authentication. The JWT can be obtained when a user account is created with the security API Gateway.","title":"Access EdgeX REST resources"},{"location":"microservices/security/Ch-AccessEdgeXRESTResources/#access-edgex-rest-resources","text":"When the EdgeX API Gateway is used, access to the micro service APIs must go through the reverse proxy. Requestors of an EdgeX REST endpoints must therefore change the URL they use to access the services. Example below explain how to map the non-secured micro service URLs with reverse proxy protected URLS. To access the ping endpoint of an EdgeX micro service (using the command service as an example), the URL is http://edgex-command-service:48082/api/v1/ping With API gateway serving as the single access point for the EdgeX services, the ping URL is https://api-gateway-server:8443/command/api/v1/ping?jwt= \\<JWT-Token> Please notice that there are 4 major differences when comparing the URLs above Switch from http to https as the API Gateway server enables https The host address and port are switched from original micro service host address and port to a common api gateway service address and 8443 port as the api gateway server will serve as the single point for all the EdgeX services Use the name of the service (in this case \"command\") within the URL to indicate that the request is to be routed to the appropriate EdgeX service (command in this example) Add a JWT as part of the URL as all the REST resources are protected by either OAuth2 or JWT authentication. The JWT can be obtained when a user account is created with the security API Gateway.","title":"Access EdgeX REST resources"},{"location":"microservices/security/Ch-SecretStore/","text":"Secret Store There are all kinds of secrets used within EdgeX Foundry micro services, such as tokens, passwords, certificates etc. The secret store serves as the central repository to keep these secrets. The developers of other EdgeX Foundry micro services utilize the secret store to create, store and retrieve secrets relevant to their corresponding micro service. The communications the between secret store and other micro services are secured by TLS. Currently the EdgeX Foundry secret store is implemented with Vault , a HashiCorp open source software product. Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, database credentials, service credentials, or certificates. Vault provides a unified interface to any secret, while providing tight access control and multiple authentication mechanisms (token, LDAP, etc.). Vault adds on key rolling, revocation rules, time-to-live access tokens, secure storage, Shamir Secret Sharing based unlocking mechanism, high availability and detailed auditing. Vault can use several backend systems (filesystem, databases, Consul, Etcd, S3, Azure, etc.) to securely store every sensitive asset. The current EdgeX Foundry implementation of Vault is using Consul , another HashiCorp open source software product. Consul is a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud. Consul uses a consensus protocol to provide Consistency as defined by CAP . The consensus protocol is based on \"Raft: In search of an Understandable Consensus Algorithm\" . For a visual explanation of Raft, see The Secret Lives of Data . The seamless integration of Vault and Consul provides a strong yet simple infrastructure to setup a reliable high availability architecture (Vault failover nodes, Consul Clustering) for the EdgeX Foundry Security services in production. The key features of Vault are: Secure Secret Storage: Arbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Authentication mechanisms (internal and/or external) and authorizations based upon policies provide access management. Dynamic Secrets: Vault can generate secrets on-demand for some systems, automatically revoking them after the lease is up. Data Encryption: Vault can encrypt and decrypt data without storing it. Leasing and Renewal: All secrets in Vault have a lease associated with them (automatic revocation). However, clients can renew leases via built-in renew APIs. Revocation: Vault has built-in support for secret revocation. Single secrets, but also a tree of secrets. Revocation assists in key rolling as well as locking down systems in the case of an intrusion. Start the Secret Store Start the Secret Store with Docker Compose and a Docker Compose manifest file. The Docker Compose file named docker-compose-fuji.yml can be found at these two locations: https://github.com/edgexfoundry/security-secret-store/blob/fuji/docker-compose-fuji.yml https://github.com/edgexfoundry/developer-scripts/blob/master/releases/fuji/compose-files/security/docker-compose-fuji.yml This Compose file starts the entire EdgeX Foundry platform including the security services. The command to start EdgeX Foundry platform including the Secret Store and API gateway related services is: sh> docker-compose up -d For a pristine run, it is strongly recommended to thoroughly clean up any Docker artifacts remaining from the current run. sh> docker-compose down -v The \"down\" operation will remove containers, network and adding the -v option it will also remove persistent volumes: Stopping edgex-vault ... done Stopping edgex-core-consul ... done Stopping edgex-files ... done Removing edgex-vault-worker ... done Removing edgex-vault ... done Removing edgex-config-seed ... done Removing edgex-core-consul ... done Removing edgex-files ... done Removing network security-secret-store_edgex-network Removing volume security-secret-store_db-data Removing volume security-secret-store_log-data Removing volume security-secret-store_consul-config Removing volume security-secret-store_consul-data Removing volume security-secret-store_vault-config Removing volume security-secret-store_vault-file Removing volume security-secret-store_vault-logs Troubleshooting steps For debugging purpose, the Secret Store services can be started individually with these commands. A pre-requisite being to carefully follow the invocation sequence in order to avoid any dependency failure. Lines starting with # (hashtag) are contextual comments to explicit the purpose of the above corresponding command: Clean-up sh> cd <path-to-EdgeX-Foundry-Secret-Store> # <...>/security-secret-store/ sh> docker-compose down -v sh> docker-compose ps # Check no previous container is running sh> docker volume ls # Check and remove any previous persistent and/or unused volumes sh> docker volume prune sh> docker volume rm <volume-name> sh> docker network ls # Check and remove the previous EdgeX Foundry Docker network sh> docker network rm edgex-network Start the first service: volume (platform volume initializations) sh> docker-compose up -d volume Sample output: Creating network \"security-secret-store_edgex-network\" with driver \"bridge\" Creating volume \"security-secret-store_db-data\" with default driver Creating volume \"security-secret-store_log-data\" with default driver Creating volume \"security-secret-store_consul-config\" with default driver Creating volume \"security-secret-store_consul-data\" with default driver Creating volume \"security-secret-store_vault-config\" with default driver Creating volume \"security-secret-store_vault-file\" with default driver Creating volume \"security-secret-store_vault-logs\" with default driver Creating edgex-files ... done Start the second service: consul (Consul is Vault store backend) sh> docker-compose up -d consul Sample output: edgex-files is up-to-date Creating edgex-core-consul ... done Display and inspect consul service logs: important lines are highlighted sh> docker-compose logs consul Sample output: Attaching to edgex - core - consul edgex - core - consul | ==> Starting Consul agent ... edgex - core - consul | ==> Consul agent running ! edgex - core - consul | Version : 'v1.1.0' edgex - core - consul | Node ID : '371cbce6-02a8-65f6-ddea-6df5c40a4c50' edgex - core - consul | Node name : 'edgex-core-consul' edgex - core - consul | Datacenter : 'dc1' ( Segment : '<all>' ) edgex - core - consul | Server : true ( Bootstrap : false ) edgex - core - consul | Client Addr : [ 0.0.0.0 ] ( HTTP : 8500 , HTTPS : - 1 , DNS : 8600 ) edgex - core - consul | Cluster Addr : 127.0.0.1 ( LAN : 8301 , WAN : 8302 ) edgex - core - consul | Encrypt : Gossip : false , TLS - Outgoing : false , TLS - Incoming : false edgex - core - consul | edgex - core - consul | ==> Log data will now stream in as it occurs : edgex - core - consul | edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ DEBUG ] agent : Using random ID \"371cbce6-02a8-65f6-ddea-6df5c40a4c50\" as node ID edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Initial configuration ( index = 1 ) : [ {Suffrage:Voter ID:371cbce6-02a8-65f6-ddea-6df5c40a4c50 Address:127.0.0.1:8300} ] edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Node at 127.0.0.1 : 8300 [ Follower ] entering Follower state ( Leader : \"\" ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] serf : EventMemberJoin : edgex - core - consul . dc1 127.0.0.1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] serf : EventMemberJoin : edgex - core - consul 127.0.0.1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] consul : Adding LAN server edgex - core - consul ( Addr : tcp / 127.0.0.1 : 8300 ) ( DC : dc1 ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] consul : Handled member - join event for server \"edgex-core-consul.dc1\" in area \"wan\" edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : Started DNS server 0.0.0.0 : 8600 ( tcp ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : Started DNS server 0.0.0.0 : 8600 ( udp ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : Started HTTP server on [ :: ] : 8500 ( tcp ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : started state syncer edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ WARN ] raft : Heartbeat timeout from \"\" reached , starting election edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Node at 127.0.0.1 : 8300 [ Candidate ] entering Candidate state in term 2 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ DEBUG ] raft : Votes needed : 1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ DEBUG ] raft : Vote granted from 371 cbce6 - 02 a8 - 65 f6 - ddea - 6 df5c40a4c50 in term 2. Tally : 1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Election won . Tally : 1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Node at 127.0.0.1 : 8300 [ Leader ] entering Leader state edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] consul : cluster leadership acquired Start the third service: config-seed (platform configuration initializations) sh> docker-compose up -d config-seed Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-config-seed ... done Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up Note Line 3: edgex-config-seed service has exited after successful processing (exit code 0) Start the fourth service: vault (Vault tool) Note Vault will be uninitialized and unsealed upon success. The vault-worker service will process the initialization and unsealing tasks. sh> docker-compose up -d vault Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-vault ... done Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: \"enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | Note Line 4 & 7: Vault API endpoint on port 8200 (lines 4 and 7). Line 7: Vault has TLS enabled. Line 10: Vault backend storage is Consul . Start the fifth service: vault-worker (Vault init/unseal process and setups) sh> docker-compose up -d vault-worker Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date edgex-vault is up-to-date Creating edgex-vault-worker ... done Display and inspect \"vault-worker\" service logs: important lines are highlighted sh> docker-compose logs vault-worker Sample output: Attaching to edgex-vault-worker edgex-vault-worker | INFO: 2019/01/13 13:35:42 successful loading the rootCA cert. edgex-vault-worker | INFO: 2019/01/13 13:35:43 {\"keys\":[\"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\"],\"keys_base64\":[\"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\"],\"recovery_keys\":null,\"recovery_keys_base64\":null,\"root_token\":\"01dbbae4-353a-8cdf-8189-4d50e5535a6f\"} edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been initialized successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been unsealed successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault Health Check HTTP Status: 200 OK (StatusCode: 200) edgex-vault-worker | INFO: 2019/01/13 13:35:48 Verifying Admin policy file hash (SHA256). edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault policy file checksum (SHA256): 5ce8d58cf7d931735f6532742f677c109a91a263bcefe9aef73ab2a69f4b43d3 edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Admin policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Admin policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Kong policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Kong policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Admin token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Kong token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful on reading certificate from v1/secret/edgex/pki/tls/edgex-kong. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Cert&key are not in the secret store yet, will need to upload them. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Load cert&key pair from volume successfully, now will upload to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Trying to upload cert&key to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful to add certificate to the secret store. Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | edgex-vault | 2019-01-13T13:35:42.549Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.551Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.554Z [INFO ] core: security barrier initialized: shares=1 threshold=1 edgex-vault | 2019-01-13T13:35:42.575Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:42.584Z [INFO ] core: no mounts; adding default mount table edgex-vault | 2019-01-13T13:35:42.585Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:42.594Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:42.596Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: root token generated edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: pre-seal teardown starting edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: stopping cluster listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: shutting down forwarding rpc listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: forwarding rpc listeners stopped edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: rpc listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: cluster listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] rollback: stopping rollback manager edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] core: pre-seal teardown complete edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: vault is unsealed edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: entering standby mode edgex-vault | 2019-01-13T13:35:43.109Z [INFO ] core: acquired lock, enabling active operation edgex-vault | 2019-01-13T13:35:43.134Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:43.141Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 Note Line 18: Vault initialization successful. Line 35: Vault root token generated. Line 44: Vault unsealing successful. Line 50: Vault key/value store secret successfully mounted . Line 60 & 61: Vault successfully started . Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up edgex-vault docker-entrypoint.sh serve ... Up 0.0.0.0:8200->8200/tcp edgex-vault-worker ./edgex-vault-worker --ini ... Exit 0 Note Line 9: edgex-vault-worker service has exited after successful processing (exit code 0) Display and inspect the created container volumes: important lines are highlighted sh> docker volume ls DRIVER VOLUME NAME local security-secret-store_consul-config local security-secret-store_consul-data local security-secret-store_db-data local security-secret-store_log-data local security-secret-store_vault-config local security-secret-store_vault-file local security-secret-store_vault-logs Display and inspect the container network ( security-secret-store_edgex-network ): important lines are highlighted sh> docker network ls NETWORK ID NAME DRIVER SCOPE 63227826fbc7 bridge bridge local 60763abffde3 host host local 1d236ab1dbbd none null local 0a7f7266d102 security-secret-store_edgex-network bridge local Using Consul Web UI For learning and verification purposes one might use the Consul Web UI interface to gather and double check specific Vault informations. Consul Web UI endpoint port is exposed by the Docker compose file. EdgeX Foundry platform uses the Consul default port number 8500. It is normally not recommended to expose Consul UI port number in production, at least the UI should not be accessible from outside the platform environment. However, because all the Vault secrets are encrypted before being transmitted and stored in the Consul backend, having access to Consul is not sufficient to access any secrets, the vault data encryption/decryption key would be absolutely necessary. Open a Web browser on http://<EdgeX Consul Server>:8500/ui . On the screenshot below, after selecting SERVICES and Vault , the UI will show the various Vault status (heartbeat and init/unseal states), coloring the boxes in green, orange or red depending on the level of importance (info, warning, error). By clicking each of the right side status indicators, more information will be accessible in order to better inspect any situation. As a practical example, we are going to navigate the Consul structure for Vault in order to check if the API Gateway (Kong) TLS certificate and private key were fetched and stored accordingly during the vault-worker process. First select KEY/VALUE menu, and then select vault root structure: We are now going to navigate deeper in the vault tree structure to reach and display the EdgeX Kong TLS assets. Continue by selecting logical/ : {.align-center width=\"348px\" height=\"287px\"} Then select d7809b... an arbitrary UID generated and created by Consul during Vault registration: {.align-center width=\"377px\" height=\"216px\"} Select edgex/ : {.align-center width=\"419px\" height=\"228px\"} Select pki/ : {.align-center width=\"417px\" height=\"222px\"} Select tls/ : {.align-center width=\"418px\" height=\"237px\"} Select edgex-kong/ : {.align-center width=\"423px\" height=\"233px\"} And we are now finally able to display the encrypted Vault secret containing the API Gateway (Kong) TLS server certificate and its corresponding private key. As you can see on the screenshot below the Vault key/value is encrypted and totally opaque to Consul, the Vault data encryption key (DEK) would be necessary to decrypt these secrets. Each Vault secret is encrypted before being transmitted to Consul node(s). Shell Access to Consul Container and Using Consul CLI sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' edgex-core-consul sh root@edgex-core-consul:/ # consul members Node Address Status Type Build Protocol DC Segment edgex-core-consul 127 .0.0.1:8301 alive server 1 .1.0 2 dc1 <all> root@edgex-core-consul:/ # consul catalog nodes Node ID Address DC edgex-core-consul e49af36a 127 .0.0.1 dc1 root@edgex-core-consul:/ # consul catalog services consul edgex-mongo vault Note Line 5: Shows the Consul node status alive (1 node in EdgeX default configuration). Line 9: Shows the Consul nodes (1 node in EdgeX default configuration). Lines 12-14: Show the Consul registered services. Configuring the Secret Store Vault server configuration is essentially concentrated in one JSON file named local.json . This file was prepared during the Vault Docker image build process. In the eventuality of a change, the Vault server container should be accessed to then modify the JSON file. The absolute path being /vault/config/local.json . To reload the new configuration simply send Vault PID a HUP signal to trigger a configuration reload. Sample Vault server configuration file: listener \"tcp\" { address = \"edgex-vault:8200\" tls_disable = \"0\" cluster_address = \"edgex-vault:8201\" tls_min_version = \"tls12\" tls_client_ca_file = \"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls_cert_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls_key_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect_addr = \"https://edgex-vault:8200\" cluster_addr = \"https://edgex-vault:8201\" } default_lease_ttl = \"168h\" max_lease_ttl = \"720h\" The listener clause refers to Vault server process (port, TLS and server name), the backend clause refers to the storage backend (i.e. Consul). To modify this configuration file, execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh root@edgex-vault:/vault # ls -l total 12 drwxr-xr-x 4 vault vault 4096 Jan 13 13 :34 config drwxr-xr-x 2 vault vault 4096 Jun 7 2018 file drwxr-xr-x 2 vault vault 4096 Jun 7 2018 logs Pay attention to the VAULT_CAPATH environment variable passed to the session. This is necessary in order to run succesful Vault CLI command. Every Vault CLI command is a wrapper of the Vault HTTP API. The Vault server is configured with TLS using X.509 PKI materials generated and signed by a local self-signed CA (EdgeXFoundryCA). Therefore, in order for each Vault CLI command (or to that extent cURL commands) to verify the Vault server TLS certificate, the self-signing CA root certificate would have to be known by the CLI command interpreter. This VAULT_CAPATH variable is checked by every Vault CLI commands, alternatively each Vault CLI command can specify an option with the same certificate path if the variable is not set. The self-signed Root CA certificate path can be found in the Vault configuration file (see above local.json), with parameter tls_client_ca_file =\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" . The local.json configuration file can be read and modified within the running container: < root @edgex - vault > : / vault \\# cat config / local . json listener \"tcp\" { address = \"edgex-vault:8200\" tls \\ _disable = \"0\" cluster \\ _address = \"edgex-vault:8201\" tls \\ _min \\ _version = \"tls12\" tls \\ _client \\ _ca \\ _file = \"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls \\ _cert \\ _file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls \\ _key \\ _file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect \\ _addr = \"<https://edgex-vault:8200>\" cluster \\ _addr = \"<https://edgex-vault:8201>\" } default \\ _lease \\ _ttl = \"168h\" max \\ _lease \\ _ttl = \"720h\" A sample Vault CLI command to check Vault status: root@edgex-vault:/vault # vault status Key Value --- ----- Seal Type shamir Sealed false Total Shares 1 Threshold 1 Version 0 .10.2 Cluster Name vault-cluster-57b3c4ed Cluster ID fe6d18bf-fa9c-0d52-3278-bca0390af023 HA Enabled true HA Cluster https://edgex-vault:8201 HA Mode active All the X.509 PKI materials including the self-signing CA are located under /vault/config/pki/EdgeXFoundryCA . root@edgex-vault:/vault # ls -l config/pki/EdgeXFoundryCA/ total 24 -rw-r--r-- 1 vault vault 956 Dec 5 14 :05 EdgeXFoundryCA.pem -r-------- 1 vault vault 306 Dec 5 14 :05 EdgeXFoundryCA.priv.key -rw-r--r-- 1 vault vault 989 Dec 5 14 :05 edgex-kong.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-kong.priv.key -rw-r--r-- 1 vault vault 1001 Dec 5 14 :05 edgex-vault.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-vault.priv.key Note Line 3: self-signing root CA certificate. Line 4: self-signing root CA private key. Line 5: API Gateway (Kong) TLS server certificate. Line 6: API Gateway (Kong) TLS server certificate private key. Line 7: Vault TLS server certificate. Line 8: Vault TLS server certificate private key. The CA name (EdgeXFoundryCA) was defined by the pkisetup tool during the Vault image build process. This tool is also responsible for all the TLS server configuration and creation tasks. If you are willing to change any of the Vault X.509 PKI assets or configuration parameters you will have to modify the pkisetup-vault.json file and rebuild a new Vault Docker image. Similarly to Vault, each EdgeX Foundry service having a TLS server certificate and private key had its X.509 PKI assets generated and signed during the Vault Docker image build process. Therefore, the API Gateway (Kong) configuration file named pkisetup-kong.json would have to be modified accordingly. A new Vault Docker image would have to be built. The Vault Dockerfile contains the pkisetup executions, see below for a corresponding excerpt (highlighted lines): # Create assets folder (needed for unseal key/s, root token and tmp) # Run CA/Vault and Kong PKI/TLS setups and peform housekeeping tasks RUN mkdir /vault/config/assets && \\ chown -R vault:vault /vault && \\ chmod 644 /vault/config/local.json && \\ chmod 744 pkisetup* && \\ ./pkisetup --config pkisetup-vault.json && \\ echo \"\" && \\ ./pkisetup --config pkisetup-kong.json && \\ chown -R vault:vault /vault/config/pki && \\ rm -f /vault/pkisetup /vault/pkisetup-vault.json /vault/pkisetup-kong.json EdgeX Foundry Docker environment implements a basic Vault/Consul architecture that does not provide high availability guaranties. Only one Consul server and one Vault server will be running. In a more sophisticated production environment it would be possible to build a reliable high availability infrastructure regarding Consul and Vault. To facilitate the setup of a minimal failover architecture the security-secret-store repository provides a sample folder named Full-Architecture-Prototype that contains necessary materials (scripts, helpers, configurations, etc.) to achieve that goal. These samples describe an architecture design with two Vault servers in failover mode (active/standby), using each one a Consul client, which subsequently connects to a Consul cluster of 3 nodes (minimal Raft concensus quorum). The Consul clients and servers (nodes) have redundant paths. Using the Secret Store 1st alternative: executing a shell session in the active Vault container to run Vault CLI commands. See paragraph Configuring the Secret Store to have more details on the VAULT_CAPATH environment variable. See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh Locate the assets folder, and the resp-init.json file: root@edgex-vault:/vault # ls -l config/assets/ total 12 -rw-r--r-- 1 root root 366 Jan 13 13 :35 admin-token.json -rw-r--r-- 1 root root 365 Jan 13 13 :35 kong-token.json -rw-r--r-- 1 root root 241 Jan 13 13 :35 resp-init.json Inspect the resp-init.json file to grab the Vault Root Token: root@edgex-vault:/vault # cat config/assets/resp-init.json { \"keys\" : [ \"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\" ] , \"keys_base64\" : [ \"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\" ] , \"recovery_keys\" :null, \"recovery_keys_base64\" :null, \"root_token\" : \"01dbbae4-353a-8cdf-8189-4d50e5535a6f\" } Login to Vault using Vault CLI and the gathered Root Token: root@edgex-vault:/vault # vault login 01dbbae4-353a-8cdf-8189-4d50e5535a6f Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token 01dbbae4-353a-8cdf-8189-4d50e5535a6f token_accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 token_duration \u221e token_renewable false token_policies [ root ] Perform an introspection lookup on the current token login: root@edgex-vault:/vault # vault token lookup Key Value --- ----- accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 creation_time 1547386542 creation_ttl 0 display_name root entity_id n/a expire_time <nil> explicit_max_ttl 0 id 01dbbae4-353a-8cdf-8189-4d50e5535a6f meta <nil> num_uses 0 orphan true path auth/token/root policies [ root ] ttl 0 Note Lines 9 & 10: the Root Token is the only token that has no expiration enforcement rules (Time to Live TTL counter). Perform a check on the current token login to display the corresponding capabilities (policies): root@edgex-vault:/vault # vault token capabilities 01dbbae4-353a-8cdf-8189-4d50e5535a6f root Perform a list request to display the currently mounted secret backends: root@edgex-vault:/vault # vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_ad070930 per-token private secret storage identity/ identity identity_5397dc2f identity store secret/ kv kv_2362c227 key/value secret storage sys/ system system_410e4276 system endpoints used for control, policy and debugging Note Line 5: EdgeX Foundry platform is using the Key/Value secret storage named secret Let's drill down into the secret k/v storage and walk through a predefined hierarchical tree structure (path). Note the pkisetup tool used during the Vault Docker image build process generates all the related X.509 TLS materials. The vault-worker service is storing each service materials into Vault using arbitrary paths, setting up access policies accordingly. For example, the API Gateway (Kong) service X.509 TLS materials: root@edgex-vault:/vault # vault list secret Keys ---- edgex/ root@edgex-vault:/vault # vault list secret/edgex Keys ---- pki/ root@edgex-vault:/vault # vault list secret/edgex/pki Keys ---- tls/ root@edgex-vault:/vault # vault list secret/edgex/pki/tls Keys ---- edgex-kong Displaying the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key ): root@edgex-vault:/vault # vault read secret/edgex/pki/tls/edgex-kong Key Value --- ----- refresh_interval 168h cert -----BEGIN CERTIFICATE----- MIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw CQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x FzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkxt FzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw NTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T YW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n MRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb EboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta gSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN MIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB Af8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt MCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG CCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ tIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz 4HerRLe55EmvF10mF7VCGOXe -----END CERTIFICATE----- key -----BEGIN PRIVATE KEY----- MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b Oib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR PVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk 8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4 = -----END PRIVATE KEY----- Note These two key values are in PEM format. 2nd alternative: using the Vault Web UI. Open a browser session on https://<EdgeX Vault Server>:8200 , accept the self-signed TLS server certificate and sign-in with the Root Token (see above 1st alternative to learn how to fetch this token): {.align-center width=\"606px\" height=\"504px\"} Upper left corner of the current Vault UI session, the sign-out menu displaying the current token name: {.align-center width=\"275px\" height=\"156px\"} Select the Vault secret backend: {.align-center} Navigate the API Gateway (Kong) service X.509 TLS materials path (edgex/pki/tls/edgex-kong): {.align-center} The Vault UI also allows entering Vault CLI commands (see above 1st alternative ) using an embedded console: {.align-center} 3rd alternative: directly using the Vault HTTP API with cURL commands. See paragraph Configuring the Secret Store to have more details on the --cacert option (identical purpose as the VAULT_CAPATH environment variable for Vault CLI). See paragraph Using the Secret Store to have more details on gathering the Vault Root Token (ref: /vault/config/assets/resp-init.json ). See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Displaying (GET) the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/secret/edgex/pki/tls/edgex-kong | jq Note Line 2: the --location option allows following a redirection (necessary when using a Vault cluster) Line 5: the Vault API path prefix /v1/secret and the API Gateway X.509 TLS materials k/v /edgex/pki/tls/edgex-kong . Line 5: the jq tool is a lightweight and flexible command-line JSON processor ( https://stedolan.github.io/jq/ ) allowing JSON pretty printing in the terminal. Sample JSON returned: { \"request_id\" : \"eaa80a1b-0d31-8d11-6ce1-8d9aa3ac6a19\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 604800 , \"data\" : { \"cert\" : \"-----BEGIN CERTIFICATE-----\\nMIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw\\nCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x\\nFzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkx\\nFzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw\\nNTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T\\nYW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n\\nMRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb\\nEboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta\\ngSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN\\nMIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB\\nAf8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt\\nMCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG\\nCCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ\\ntIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz\\n4HerRLe55EmvF10mF7VCGOXe\\n-----END CERTIFICATE-----\\n\" , \"key\" : \"-----BEGIN PRIVATE KEY-----\\nMIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b\\nOib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR\\nPVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk\\n8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4=\\n-----END PRIVATE KEY-----\\n\" }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Note Lines 7 & 8: the two key values (TLS certificate cert & corresponding private key key ) are in PEM format ( https://tools.ietf.org/html/rfc1421 ). Displaying (LIST) the root key path in the Vault secret backend for the EdgeX Foudry platform ( edgex ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request LIST \\ https://edgex-vault:8200/v1/secret | jq Sample JSON returned: { \"request_id\" : \"0e0ea024-176d-21b3-73cb-99f17729b230\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 0 , \"data\" : { \"keys\" : [ \"edgex/\" ] }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Displaying (GET) the Vault seal status ( API path: /v1/sys/seal-status ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/sys/seal-status | jq Sample JSON returned: { \"type\" : \"shamir\" , \"sealed\" : false , \"t\" : 1 , \"n\" : 1 , \"progress\" : 0 , \"nonce\" : \"\" , \"version\" : \"0.10.2\" , \"cluster_name\" : \"vault-cluster-57b3c4ed\" , \"cluster_id\" : \"fe6d18bf-fa9c-0d52-3278-bca0390af023\" } Note Line 3: Vault is unsealed therefore available and ready for requests. Line 4 & 5: Vault Shamir Secret Sharing default configuration for EdgeX Foundry: 1 share with threshold 1 (no sharding). See also Some of the command used in implementing security services have man-style documentation: security-file-token-provider - Generate Vault tokens for EdgeX services security-secrets-setup - Creates an on-device public-key infrastructure (PKI) to secure microservice secret management","title":"Secret Store"},{"location":"microservices/security/Ch-SecretStore/#secret-store","text":"There are all kinds of secrets used within EdgeX Foundry micro services, such as tokens, passwords, certificates etc. The secret store serves as the central repository to keep these secrets. The developers of other EdgeX Foundry micro services utilize the secret store to create, store and retrieve secrets relevant to their corresponding micro service. The communications the between secret store and other micro services are secured by TLS. Currently the EdgeX Foundry secret store is implemented with Vault , a HashiCorp open source software product. Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, database credentials, service credentials, or certificates. Vault provides a unified interface to any secret, while providing tight access control and multiple authentication mechanisms (token, LDAP, etc.). Vault adds on key rolling, revocation rules, time-to-live access tokens, secure storage, Shamir Secret Sharing based unlocking mechanism, high availability and detailed auditing. Vault can use several backend systems (filesystem, databases, Consul, Etcd, S3, Azure, etc.) to securely store every sensitive asset. The current EdgeX Foundry implementation of Vault is using Consul , another HashiCorp open source software product. Consul is a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud. Consul uses a consensus protocol to provide Consistency as defined by CAP . The consensus protocol is based on \"Raft: In search of an Understandable Consensus Algorithm\" . For a visual explanation of Raft, see The Secret Lives of Data . The seamless integration of Vault and Consul provides a strong yet simple infrastructure to setup a reliable high availability architecture (Vault failover nodes, Consul Clustering) for the EdgeX Foundry Security services in production. The key features of Vault are: Secure Secret Storage: Arbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Authentication mechanisms (internal and/or external) and authorizations based upon policies provide access management. Dynamic Secrets: Vault can generate secrets on-demand for some systems, automatically revoking them after the lease is up. Data Encryption: Vault can encrypt and decrypt data without storing it. Leasing and Renewal: All secrets in Vault have a lease associated with them (automatic revocation). However, clients can renew leases via built-in renew APIs. Revocation: Vault has built-in support for secret revocation. Single secrets, but also a tree of secrets. Revocation assists in key rolling as well as locking down systems in the case of an intrusion.","title":"Secret Store"},{"location":"microservices/security/Ch-SecretStore/#start-the-secret-store","text":"Start the Secret Store with Docker Compose and a Docker Compose manifest file. The Docker Compose file named docker-compose-fuji.yml can be found at these two locations: https://github.com/edgexfoundry/security-secret-store/blob/fuji/docker-compose-fuji.yml https://github.com/edgexfoundry/developer-scripts/blob/master/releases/fuji/compose-files/security/docker-compose-fuji.yml This Compose file starts the entire EdgeX Foundry platform including the security services. The command to start EdgeX Foundry platform including the Secret Store and API gateway related services is: sh> docker-compose up -d For a pristine run, it is strongly recommended to thoroughly clean up any Docker artifacts remaining from the current run. sh> docker-compose down -v The \"down\" operation will remove containers, network and adding the -v option it will also remove persistent volumes: Stopping edgex-vault ... done Stopping edgex-core-consul ... done Stopping edgex-files ... done Removing edgex-vault-worker ... done Removing edgex-vault ... done Removing edgex-config-seed ... done Removing edgex-core-consul ... done Removing edgex-files ... done Removing network security-secret-store_edgex-network Removing volume security-secret-store_db-data Removing volume security-secret-store_log-data Removing volume security-secret-store_consul-config Removing volume security-secret-store_consul-data Removing volume security-secret-store_vault-config Removing volume security-secret-store_vault-file Removing volume security-secret-store_vault-logs","title":"Start the Secret Store"},{"location":"microservices/security/Ch-SecretStore/#troubleshooting-steps","text":"For debugging purpose, the Secret Store services can be started individually with these commands. A pre-requisite being to carefully follow the invocation sequence in order to avoid any dependency failure. Lines starting with # (hashtag) are contextual comments to explicit the purpose of the above corresponding command:","title":"Troubleshooting steps"},{"location":"microservices/security/Ch-SecretStore/#clean-up","text":"sh> cd <path-to-EdgeX-Foundry-Secret-Store> # <...>/security-secret-store/ sh> docker-compose down -v sh> docker-compose ps # Check no previous container is running sh> docker volume ls # Check and remove any previous persistent and/or unused volumes sh> docker volume prune sh> docker volume rm <volume-name> sh> docker network ls # Check and remove the previous EdgeX Foundry Docker network sh> docker network rm edgex-network","title":"Clean-up"},{"location":"microservices/security/Ch-SecretStore/#start-the-first-service-volume-platform-volume-initializations","text":"sh> docker-compose up -d volume Sample output: Creating network \"security-secret-store_edgex-network\" with driver \"bridge\" Creating volume \"security-secret-store_db-data\" with default driver Creating volume \"security-secret-store_log-data\" with default driver Creating volume \"security-secret-store_consul-config\" with default driver Creating volume \"security-secret-store_consul-data\" with default driver Creating volume \"security-secret-store_vault-config\" with default driver Creating volume \"security-secret-store_vault-file\" with default driver Creating volume \"security-secret-store_vault-logs\" with default driver Creating edgex-files ... done","title":"Start the first service: volume (platform volume initializations)"},{"location":"microservices/security/Ch-SecretStore/#start-the-second-service-consul-consul-is-vault-store-backend","text":"sh> docker-compose up -d consul Sample output: edgex-files is up-to-date Creating edgex-core-consul ... done Display and inspect consul service logs: important lines are highlighted sh> docker-compose logs consul Sample output: Attaching to edgex - core - consul edgex - core - consul | ==> Starting Consul agent ... edgex - core - consul | ==> Consul agent running ! edgex - core - consul | Version : 'v1.1.0' edgex - core - consul | Node ID : '371cbce6-02a8-65f6-ddea-6df5c40a4c50' edgex - core - consul | Node name : 'edgex-core-consul' edgex - core - consul | Datacenter : 'dc1' ( Segment : '<all>' ) edgex - core - consul | Server : true ( Bootstrap : false ) edgex - core - consul | Client Addr : [ 0.0.0.0 ] ( HTTP : 8500 , HTTPS : - 1 , DNS : 8600 ) edgex - core - consul | Cluster Addr : 127.0.0.1 ( LAN : 8301 , WAN : 8302 ) edgex - core - consul | Encrypt : Gossip : false , TLS - Outgoing : false , TLS - Incoming : false edgex - core - consul | edgex - core - consul | ==> Log data will now stream in as it occurs : edgex - core - consul | edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ DEBUG ] agent : Using random ID \"371cbce6-02a8-65f6-ddea-6df5c40a4c50\" as node ID edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Initial configuration ( index = 1 ) : [ {Suffrage:Voter ID:371cbce6-02a8-65f6-ddea-6df5c40a4c50 Address:127.0.0.1:8300} ] edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Node at 127.0.0.1 : 8300 [ Follower ] entering Follower state ( Leader : \"\" ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] serf : EventMemberJoin : edgex - core - consul . dc1 127.0.0.1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] serf : EventMemberJoin : edgex - core - consul 127.0.0.1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] consul : Adding LAN server edgex - core - consul ( Addr : tcp / 127.0.0.1 : 8300 ) ( DC : dc1 ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] consul : Handled member - join event for server \"edgex-core-consul.dc1\" in area \"wan\" edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : Started DNS server 0.0.0.0 : 8600 ( tcp ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : Started DNS server 0.0.0.0 : 8600 ( udp ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : Started HTTP server on [ :: ] : 8500 ( tcp ) edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] agent : started state syncer edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ WARN ] raft : Heartbeat timeout from \"\" reached , starting election edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Node at 127.0.0.1 : 8300 [ Candidate ] entering Candidate state in term 2 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ DEBUG ] raft : Votes needed : 1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ DEBUG ] raft : Vote granted from 371 cbce6 - 02 a8 - 65 f6 - ddea - 6 df5c40a4c50 in term 2. Tally : 1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Election won . Tally : 1 edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] raft : Node at 127.0.0.1 : 8300 [ Leader ] entering Leader state edgex - core - consul | 2019 / 01 / 13 13 : 25 : 06 [ INFO ] consul : cluster leadership acquired","title":"Start the second service: consul (Consul is Vault store backend)"},{"location":"microservices/security/Ch-SecretStore/#start-the-third-service-config-seed-platform-configuration-initializations","text":"sh> docker-compose up -d config-seed Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-config-seed ... done Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up Note Line 3: edgex-config-seed service has exited after successful processing (exit code 0)","title":"Start the third service: config-seed (platform configuration initializations)"},{"location":"microservices/security/Ch-SecretStore/#start-the-fourth-service-vault-vault-tool","text":"Note Vault will be uninitialized and unsealed upon success. The vault-worker service will process the initialization and unsealing tasks. sh> docker-compose up -d vault Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-vault ... done Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: \"enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | Note Line 4 & 7: Vault API endpoint on port 8200 (lines 4 and 7). Line 7: Vault has TLS enabled. Line 10: Vault backend storage is Consul .","title":"Start the fourth service: vault (Vault tool)"},{"location":"microservices/security/Ch-SecretStore/#start-the-fifth-service-vault-worker-vault-initunseal-process-and-setups","text":"sh> docker-compose up -d vault-worker Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date edgex-vault is up-to-date Creating edgex-vault-worker ... done Display and inspect \"vault-worker\" service logs: important lines are highlighted sh> docker-compose logs vault-worker Sample output: Attaching to edgex-vault-worker edgex-vault-worker | INFO: 2019/01/13 13:35:42 successful loading the rootCA cert. edgex-vault-worker | INFO: 2019/01/13 13:35:43 {\"keys\":[\"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\"],\"keys_base64\":[\"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\"],\"recovery_keys\":null,\"recovery_keys_base64\":null,\"root_token\":\"01dbbae4-353a-8cdf-8189-4d50e5535a6f\"} edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been initialized successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been unsealed successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault Health Check HTTP Status: 200 OK (StatusCode: 200) edgex-vault-worker | INFO: 2019/01/13 13:35:48 Verifying Admin policy file hash (SHA256). edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault policy file checksum (SHA256): 5ce8d58cf7d931735f6532742f677c109a91a263bcefe9aef73ab2a69f4b43d3 edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Admin policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Admin policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Kong policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Kong policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Admin token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Kong token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful on reading certificate from v1/secret/edgex/pki/tls/edgex-kong. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Cert&key are not in the secret store yet, will need to upload them. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Load cert&key pair from volume successfully, now will upload to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Trying to upload cert&key to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful to add certificate to the secret store. Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | edgex-vault | 2019-01-13T13:35:42.549Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.551Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.554Z [INFO ] core: security barrier initialized: shares=1 threshold=1 edgex-vault | 2019-01-13T13:35:42.575Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:42.584Z [INFO ] core: no mounts; adding default mount table edgex-vault | 2019-01-13T13:35:42.585Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:42.594Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:42.596Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: root token generated edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: pre-seal teardown starting edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: stopping cluster listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: shutting down forwarding rpc listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: forwarding rpc listeners stopped edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: rpc listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: cluster listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] rollback: stopping rollback manager edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] core: pre-seal teardown complete edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: vault is unsealed edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: entering standby mode edgex-vault | 2019-01-13T13:35:43.109Z [INFO ] core: acquired lock, enabling active operation edgex-vault | 2019-01-13T13:35:43.134Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:43.141Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 Note Line 18: Vault initialization successful. Line 35: Vault root token generated. Line 44: Vault unsealing successful. Line 50: Vault key/value store secret successfully mounted . Line 60 & 61: Vault successfully started . Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up edgex-vault docker-entrypoint.sh serve ... Up 0.0.0.0:8200->8200/tcp edgex-vault-worker ./edgex-vault-worker --ini ... Exit 0 Note Line 9: edgex-vault-worker service has exited after successful processing (exit code 0) Display and inspect the created container volumes: important lines are highlighted sh> docker volume ls DRIVER VOLUME NAME local security-secret-store_consul-config local security-secret-store_consul-data local security-secret-store_db-data local security-secret-store_log-data local security-secret-store_vault-config local security-secret-store_vault-file local security-secret-store_vault-logs Display and inspect the container network ( security-secret-store_edgex-network ): important lines are highlighted sh> docker network ls NETWORK ID NAME DRIVER SCOPE 63227826fbc7 bridge bridge local 60763abffde3 host host local 1d236ab1dbbd none null local 0a7f7266d102 security-secret-store_edgex-network bridge local","title":"Start the fifth service: vault-worker (Vault init/unseal process and setups)"},{"location":"microservices/security/Ch-SecretStore/#using-consul-web-ui","text":"For learning and verification purposes one might use the Consul Web UI interface to gather and double check specific Vault informations. Consul Web UI endpoint port is exposed by the Docker compose file. EdgeX Foundry platform uses the Consul default port number 8500. It is normally not recommended to expose Consul UI port number in production, at least the UI should not be accessible from outside the platform environment. However, because all the Vault secrets are encrypted before being transmitted and stored in the Consul backend, having access to Consul is not sufficient to access any secrets, the vault data encryption/decryption key would be absolutely necessary. Open a Web browser on http://<EdgeX Consul Server>:8500/ui . On the screenshot below, after selecting SERVICES and Vault , the UI will show the various Vault status (heartbeat and init/unseal states), coloring the boxes in green, orange or red depending on the level of importance (info, warning, error). By clicking each of the right side status indicators, more information will be accessible in order to better inspect any situation. As a practical example, we are going to navigate the Consul structure for Vault in order to check if the API Gateway (Kong) TLS certificate and private key were fetched and stored accordingly during the vault-worker process. First select KEY/VALUE menu, and then select vault root structure: We are now going to navigate deeper in the vault tree structure to reach and display the EdgeX Kong TLS assets. Continue by selecting logical/ : {.align-center width=\"348px\" height=\"287px\"} Then select d7809b... an arbitrary UID generated and created by Consul during Vault registration: {.align-center width=\"377px\" height=\"216px\"} Select edgex/ : {.align-center width=\"419px\" height=\"228px\"} Select pki/ : {.align-center width=\"417px\" height=\"222px\"} Select tls/ : {.align-center width=\"418px\" height=\"237px\"} Select edgex-kong/ : {.align-center width=\"423px\" height=\"233px\"} And we are now finally able to display the encrypted Vault secret containing the API Gateway (Kong) TLS server certificate and its corresponding private key. As you can see on the screenshot below the Vault key/value is encrypted and totally opaque to Consul, the Vault data encryption key (DEK) would be necessary to decrypt these secrets. Each Vault secret is encrypted before being transmitted to Consul node(s).","title":"Using Consul Web UI"},{"location":"microservices/security/Ch-SecretStore/#shell-access-to-consul-container-and-using-consul-cli","text":"sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' edgex-core-consul sh root@edgex-core-consul:/ # consul members Node Address Status Type Build Protocol DC Segment edgex-core-consul 127 .0.0.1:8301 alive server 1 .1.0 2 dc1 <all> root@edgex-core-consul:/ # consul catalog nodes Node ID Address DC edgex-core-consul e49af36a 127 .0.0.1 dc1 root@edgex-core-consul:/ # consul catalog services consul edgex-mongo vault Note Line 5: Shows the Consul node status alive (1 node in EdgeX default configuration). Line 9: Shows the Consul nodes (1 node in EdgeX default configuration). Lines 12-14: Show the Consul registered services.","title":"Shell Access to Consul Container and Using Consul CLI"},{"location":"microservices/security/Ch-SecretStore/#configuring-the-secret-store","text":"Vault server configuration is essentially concentrated in one JSON file named local.json . This file was prepared during the Vault Docker image build process. In the eventuality of a change, the Vault server container should be accessed to then modify the JSON file. The absolute path being /vault/config/local.json . To reload the new configuration simply send Vault PID a HUP signal to trigger a configuration reload. Sample Vault server configuration file: listener \"tcp\" { address = \"edgex-vault:8200\" tls_disable = \"0\" cluster_address = \"edgex-vault:8201\" tls_min_version = \"tls12\" tls_client_ca_file = \"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls_cert_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls_key_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect_addr = \"https://edgex-vault:8200\" cluster_addr = \"https://edgex-vault:8201\" } default_lease_ttl = \"168h\" max_lease_ttl = \"720h\" The listener clause refers to Vault server process (port, TLS and server name), the backend clause refers to the storage backend (i.e. Consul). To modify this configuration file, execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh root@edgex-vault:/vault # ls -l total 12 drwxr-xr-x 4 vault vault 4096 Jan 13 13 :34 config drwxr-xr-x 2 vault vault 4096 Jun 7 2018 file drwxr-xr-x 2 vault vault 4096 Jun 7 2018 logs Pay attention to the VAULT_CAPATH environment variable passed to the session. This is necessary in order to run succesful Vault CLI command. Every Vault CLI command is a wrapper of the Vault HTTP API. The Vault server is configured with TLS using X.509 PKI materials generated and signed by a local self-signed CA (EdgeXFoundryCA). Therefore, in order for each Vault CLI command (or to that extent cURL commands) to verify the Vault server TLS certificate, the self-signing CA root certificate would have to be known by the CLI command interpreter. This VAULT_CAPATH variable is checked by every Vault CLI commands, alternatively each Vault CLI command can specify an option with the same certificate path if the variable is not set. The self-signed Root CA certificate path can be found in the Vault configuration file (see above local.json), with parameter tls_client_ca_file =\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" . The local.json configuration file can be read and modified within the running container: < root @edgex - vault > : / vault \\# cat config / local . json listener \"tcp\" { address = \"edgex-vault:8200\" tls \\ _disable = \"0\" cluster \\ _address = \"edgex-vault:8201\" tls \\ _min \\ _version = \"tls12\" tls \\ _client \\ _ca \\ _file = \"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls \\ _cert \\ _file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls \\ _key \\ _file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect \\ _addr = \"<https://edgex-vault:8200>\" cluster \\ _addr = \"<https://edgex-vault:8201>\" } default \\ _lease \\ _ttl = \"168h\" max \\ _lease \\ _ttl = \"720h\" A sample Vault CLI command to check Vault status: root@edgex-vault:/vault # vault status Key Value --- ----- Seal Type shamir Sealed false Total Shares 1 Threshold 1 Version 0 .10.2 Cluster Name vault-cluster-57b3c4ed Cluster ID fe6d18bf-fa9c-0d52-3278-bca0390af023 HA Enabled true HA Cluster https://edgex-vault:8201 HA Mode active All the X.509 PKI materials including the self-signing CA are located under /vault/config/pki/EdgeXFoundryCA . root@edgex-vault:/vault # ls -l config/pki/EdgeXFoundryCA/ total 24 -rw-r--r-- 1 vault vault 956 Dec 5 14 :05 EdgeXFoundryCA.pem -r-------- 1 vault vault 306 Dec 5 14 :05 EdgeXFoundryCA.priv.key -rw-r--r-- 1 vault vault 989 Dec 5 14 :05 edgex-kong.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-kong.priv.key -rw-r--r-- 1 vault vault 1001 Dec 5 14 :05 edgex-vault.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-vault.priv.key Note Line 3: self-signing root CA certificate. Line 4: self-signing root CA private key. Line 5: API Gateway (Kong) TLS server certificate. Line 6: API Gateway (Kong) TLS server certificate private key. Line 7: Vault TLS server certificate. Line 8: Vault TLS server certificate private key. The CA name (EdgeXFoundryCA) was defined by the pkisetup tool during the Vault image build process. This tool is also responsible for all the TLS server configuration and creation tasks. If you are willing to change any of the Vault X.509 PKI assets or configuration parameters you will have to modify the pkisetup-vault.json file and rebuild a new Vault Docker image. Similarly to Vault, each EdgeX Foundry service having a TLS server certificate and private key had its X.509 PKI assets generated and signed during the Vault Docker image build process. Therefore, the API Gateway (Kong) configuration file named pkisetup-kong.json would have to be modified accordingly. A new Vault Docker image would have to be built. The Vault Dockerfile contains the pkisetup executions, see below for a corresponding excerpt (highlighted lines): # Create assets folder (needed for unseal key/s, root token and tmp) # Run CA/Vault and Kong PKI/TLS setups and peform housekeeping tasks RUN mkdir /vault/config/assets && \\ chown -R vault:vault /vault && \\ chmod 644 /vault/config/local.json && \\ chmod 744 pkisetup* && \\ ./pkisetup --config pkisetup-vault.json && \\ echo \"\" && \\ ./pkisetup --config pkisetup-kong.json && \\ chown -R vault:vault /vault/config/pki && \\ rm -f /vault/pkisetup /vault/pkisetup-vault.json /vault/pkisetup-kong.json EdgeX Foundry Docker environment implements a basic Vault/Consul architecture that does not provide high availability guaranties. Only one Consul server and one Vault server will be running. In a more sophisticated production environment it would be possible to build a reliable high availability infrastructure regarding Consul and Vault. To facilitate the setup of a minimal failover architecture the security-secret-store repository provides a sample folder named Full-Architecture-Prototype that contains necessary materials (scripts, helpers, configurations, etc.) to achieve that goal. These samples describe an architecture design with two Vault servers in failover mode (active/standby), using each one a Consul client, which subsequently connects to a Consul cluster of 3 nodes (minimal Raft concensus quorum). The Consul clients and servers (nodes) have redundant paths.","title":"Configuring the Secret Store"},{"location":"microservices/security/Ch-SecretStore/#using-the-secret-store","text":"","title":"Using the Secret Store"},{"location":"microservices/security/Ch-SecretStore/#1st-alternative-executing-a-shell-session-in-the-active-vault-container-to-run-vault-cli-commands","text":"See paragraph Configuring the Secret Store to have more details on the VAULT_CAPATH environment variable. See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh Locate the assets folder, and the resp-init.json file: root@edgex-vault:/vault # ls -l config/assets/ total 12 -rw-r--r-- 1 root root 366 Jan 13 13 :35 admin-token.json -rw-r--r-- 1 root root 365 Jan 13 13 :35 kong-token.json -rw-r--r-- 1 root root 241 Jan 13 13 :35 resp-init.json Inspect the resp-init.json file to grab the Vault Root Token: root@edgex-vault:/vault # cat config/assets/resp-init.json { \"keys\" : [ \"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\" ] , \"keys_base64\" : [ \"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\" ] , \"recovery_keys\" :null, \"recovery_keys_base64\" :null, \"root_token\" : \"01dbbae4-353a-8cdf-8189-4d50e5535a6f\" } Login to Vault using Vault CLI and the gathered Root Token: root@edgex-vault:/vault # vault login 01dbbae4-353a-8cdf-8189-4d50e5535a6f Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token 01dbbae4-353a-8cdf-8189-4d50e5535a6f token_accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 token_duration \u221e token_renewable false token_policies [ root ] Perform an introspection lookup on the current token login: root@edgex-vault:/vault # vault token lookup Key Value --- ----- accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 creation_time 1547386542 creation_ttl 0 display_name root entity_id n/a expire_time <nil> explicit_max_ttl 0 id 01dbbae4-353a-8cdf-8189-4d50e5535a6f meta <nil> num_uses 0 orphan true path auth/token/root policies [ root ] ttl 0 Note Lines 9 & 10: the Root Token is the only token that has no expiration enforcement rules (Time to Live TTL counter). Perform a check on the current token login to display the corresponding capabilities (policies): root@edgex-vault:/vault # vault token capabilities 01dbbae4-353a-8cdf-8189-4d50e5535a6f root Perform a list request to display the currently mounted secret backends: root@edgex-vault:/vault # vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_ad070930 per-token private secret storage identity/ identity identity_5397dc2f identity store secret/ kv kv_2362c227 key/value secret storage sys/ system system_410e4276 system endpoints used for control, policy and debugging Note Line 5: EdgeX Foundry platform is using the Key/Value secret storage named secret Let's drill down into the secret k/v storage and walk through a predefined hierarchical tree structure (path). Note the pkisetup tool used during the Vault Docker image build process generates all the related X.509 TLS materials. The vault-worker service is storing each service materials into Vault using arbitrary paths, setting up access policies accordingly. For example, the API Gateway (Kong) service X.509 TLS materials: root@edgex-vault:/vault # vault list secret Keys ---- edgex/ root@edgex-vault:/vault # vault list secret/edgex Keys ---- pki/ root@edgex-vault:/vault # vault list secret/edgex/pki Keys ---- tls/ root@edgex-vault:/vault # vault list secret/edgex/pki/tls Keys ---- edgex-kong Displaying the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key ): root@edgex-vault:/vault # vault read secret/edgex/pki/tls/edgex-kong Key Value --- ----- refresh_interval 168h cert -----BEGIN CERTIFICATE----- MIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw CQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x FzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkxt FzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw NTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T YW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n MRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb EboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta gSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN MIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB Af8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt MCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG CCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ tIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz 4HerRLe55EmvF10mF7VCGOXe -----END CERTIFICATE----- key -----BEGIN PRIVATE KEY----- MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b Oib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR PVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk 8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4 = -----END PRIVATE KEY----- Note These two key values are in PEM format.","title":"1st alternative: executing a shell session in the active Vault container to run Vault CLI commands."},{"location":"microservices/security/Ch-SecretStore/#2nd-alternative-using-the-vault-web-ui","text":"Open a browser session on https://<EdgeX Vault Server>:8200 , accept the self-signed TLS server certificate and sign-in with the Root Token (see above 1st alternative to learn how to fetch this token): {.align-center width=\"606px\" height=\"504px\"} Upper left corner of the current Vault UI session, the sign-out menu displaying the current token name: {.align-center width=\"275px\" height=\"156px\"} Select the Vault secret backend: {.align-center} Navigate the API Gateway (Kong) service X.509 TLS materials path (edgex/pki/tls/edgex-kong): {.align-center} The Vault UI also allows entering Vault CLI commands (see above 1st alternative ) using an embedded console: {.align-center}","title":"2nd alternative: using the Vault Web UI."},{"location":"microservices/security/Ch-SecretStore/#3rd-alternative-directly-using-the-vault-http-api-with-curl-commands","text":"See paragraph Configuring the Secret Store to have more details on the --cacert option (identical purpose as the VAULT_CAPATH environment variable for Vault CLI). See paragraph Using the Secret Store to have more details on gathering the Vault Root Token (ref: /vault/config/assets/resp-init.json ). See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Displaying (GET) the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/secret/edgex/pki/tls/edgex-kong | jq Note Line 2: the --location option allows following a redirection (necessary when using a Vault cluster) Line 5: the Vault API path prefix /v1/secret and the API Gateway X.509 TLS materials k/v /edgex/pki/tls/edgex-kong . Line 5: the jq tool is a lightweight and flexible command-line JSON processor ( https://stedolan.github.io/jq/ ) allowing JSON pretty printing in the terminal. Sample JSON returned: { \"request_id\" : \"eaa80a1b-0d31-8d11-6ce1-8d9aa3ac6a19\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 604800 , \"data\" : { \"cert\" : \"-----BEGIN CERTIFICATE-----\\nMIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw\\nCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x\\nFzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkx\\nFzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw\\nNTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T\\nYW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n\\nMRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb\\nEboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta\\ngSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN\\nMIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB\\nAf8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt\\nMCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG\\nCCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ\\ntIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz\\n4HerRLe55EmvF10mF7VCGOXe\\n-----END CERTIFICATE-----\\n\" , \"key\" : \"-----BEGIN PRIVATE KEY-----\\nMIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b\\nOib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR\\nPVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk\\n8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4=\\n-----END PRIVATE KEY-----\\n\" }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Note Lines 7 & 8: the two key values (TLS certificate cert & corresponding private key key ) are in PEM format ( https://tools.ietf.org/html/rfc1421 ). Displaying (LIST) the root key path in the Vault secret backend for the EdgeX Foudry platform ( edgex ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request LIST \\ https://edgex-vault:8200/v1/secret | jq Sample JSON returned: { \"request_id\" : \"0e0ea024-176d-21b3-73cb-99f17729b230\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 0 , \"data\" : { \"keys\" : [ \"edgex/\" ] }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Displaying (GET) the Vault seal status ( API path: /v1/sys/seal-status ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/sys/seal-status | jq Sample JSON returned: { \"type\" : \"shamir\" , \"sealed\" : false , \"t\" : 1 , \"n\" : 1 , \"progress\" : 0 , \"nonce\" : \"\" , \"version\" : \"0.10.2\" , \"cluster_name\" : \"vault-cluster-57b3c4ed\" , \"cluster_id\" : \"fe6d18bf-fa9c-0d52-3278-bca0390af023\" } Note Line 3: Vault is unsealed therefore available and ready for requests. Line 4 & 5: Vault Shamir Secret Sharing default configuration for EdgeX Foundry: 1 share with threshold 1 (no sharding).","title":"3rd alternative: directly using the Vault HTTP API with cURL commands."},{"location":"microservices/security/Ch-SecretStore/#see-also","text":"Some of the command used in implementing security services have man-style documentation: security-file-token-provider - Generate Vault tokens for EdgeX services security-secrets-setup - Creates an on-device public-key infrastructure (PKI) to secure microservice secret management","title":"See also"},{"location":"microservices/security/Ch-Security/","text":"Security Security elements, both inside and outside of EdgeX Foundry, protect the data and control of devices, sensors, and other IoT objects managed by EdgeX Foundry. Based on the fact that EdgeX is a \"vendor-neutral open source software platform at the edge of the network\", the EdgeX security features are also built on a foundation of open interfaces and pluggable, replaceable modules. With security service enabled, the administrator of the EdgeX would be able to initialize the security components, set up running environment for security services, manage user access control, and create JWT( JSON Web Token) for resource access for other EdgeX business services. There are two major EdgeX security components. The first is a security store, which is used to provide a safe place to keep the EdgeX secrets. The second is an API gateway, which is used as a reverse proxy to restrict access to EdgeX REST resources and perform access control related works. In summary, the current features are as below: Secret creation, store and retrieve (password, cert, access key etc.) API gateway for other existing EdgeX microservice REST APIs User account creation with optional either OAuth2 or JWT authentication User account with arbitrary Access Control List groups (ACL)","title":"Security"},{"location":"microservices/security/Ch-Security/#security","text":"Security elements, both inside and outside of EdgeX Foundry, protect the data and control of devices, sensors, and other IoT objects managed by EdgeX Foundry. Based on the fact that EdgeX is a \"vendor-neutral open source software platform at the edge of the network\", the EdgeX security features are also built on a foundation of open interfaces and pluggable, replaceable modules. With security service enabled, the administrator of the EdgeX would be able to initialize the security components, set up running environment for security services, manage user access control, and create JWT( JSON Web Token) for resource access for other EdgeX business services. There are two major EdgeX security components. The first is a security store, which is used to provide a safe place to keep the EdgeX secrets. The second is an API gateway, which is used as a reverse proxy to restrict access to EdgeX REST resources and perform access control related works. In summary, the current features are as below: Secret creation, store and retrieve (password, cert, access key etc.) API gateway for other existing EdgeX microservice REST APIs User account creation with optional either OAuth2 or JWT authentication User account with arbitrary Access Control List groups (ACL)","title":"Security"},{"location":"microservices/security/Ch-SecurityIssues/","text":"Security Issues This page describes how to report EdgeX Foundry security issues and how they are handled. Security Announcements Join the edgexfoundry-announce group at: https://groups.google.com/d/forum/edgexfoundry-announce ) for emails about security and major API announcements. Vulnerability Reporting The EdgeX Foundry Open Source Community is grateful for all security reports made by users and security researchers. All reports are thoroughly investigated by a set of community volunteers. To make a report, please email the private list: security-issues@edgexfoundry.org , providing as much detail as possible. Use the security issue template: security_issue_template . At this time we do not yet offer an encrypted bug reporting option. When to Report a Vulnerability? You think you discovered a potential security vulnerability in EdgeX Foundry You are unsure how a vulnerability affects EdgeX Foundry You think you discovered a vulnerability in another project that EdgeX Foundry depends upon (e.g. docker, MongoDB, Redis,..) When NOT to Report a Vulnerability? You need help tuning EdgeX Foundry components for security You need help applying security related updates Your issue is not security related Security Vulnerability Response Each report is acknowledged and analyzed by Security Issue Review (SIR) team within one week. Any vulnerability information shared with SIR stays private, and is shared with sub-projects as necessary to get the issue fixed. As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated. In the case of 3 rd party dependency (code or library not managed and maintained by the EdgeX community) related security issues, while the issue report triggers the same response workflow, the EdgeX community will defer to owning community for fixes. On receipt of a security issue report, SIR: Discusses the issue privately to understand it Uses the Common Vulnerability Scoring System to grade the issue Determines the sub-projects and developers to involve Develops a fix In conjunction with the product group determines when to release the fix Communicates the fix 7. Uploads a Common Vulnerabilities and Exposures (CVE) style report of the issue and associated threat The issue reporter will be kept in the loop as appropriate. Note that a critical or high severity issue can delay a scheduled release to incorporate a fix or mitigation. Public Disclosure Timing A public disclosure date is negotiated by the EdgeX Product Security Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible AFTER a mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure may be immediate (especially publicly known issues) to a few weeks. The EdgeX Foundry Product Security Committee holds the final say when setting a disclosure date.","title":"Security Issues"},{"location":"microservices/security/Ch-SecurityIssues/#security-issues","text":"This page describes how to report EdgeX Foundry security issues and how they are handled.","title":"Security Issues"},{"location":"microservices/security/Ch-SecurityIssues/#security-announcements","text":"Join the edgexfoundry-announce group at: https://groups.google.com/d/forum/edgexfoundry-announce ) for emails about security and major API announcements.","title":"Security Announcements"},{"location":"microservices/security/Ch-SecurityIssues/#vulnerability-reporting","text":"The EdgeX Foundry Open Source Community is grateful for all security reports made by users and security researchers. All reports are thoroughly investigated by a set of community volunteers. To make a report, please email the private list: security-issues@edgexfoundry.org , providing as much detail as possible. Use the security issue template: security_issue_template . At this time we do not yet offer an encrypted bug reporting option.","title":"Vulnerability Reporting"},{"location":"microservices/security/Ch-SecurityIssues/#when-to-report-a-vulnerability","text":"You think you discovered a potential security vulnerability in EdgeX Foundry You are unsure how a vulnerability affects EdgeX Foundry You think you discovered a vulnerability in another project that EdgeX Foundry depends upon (e.g. docker, MongoDB, Redis,..)","title":"When to Report a Vulnerability?"},{"location":"microservices/security/Ch-SecurityIssues/#when-not-to-report-a-vulnerability","text":"You need help tuning EdgeX Foundry components for security You need help applying security related updates Your issue is not security related","title":"When NOT to Report a Vulnerability?"},{"location":"microservices/security/Ch-SecurityIssues/#security-vulnerability-response","text":"Each report is acknowledged and analyzed by Security Issue Review (SIR) team within one week. Any vulnerability information shared with SIR stays private, and is shared with sub-projects as necessary to get the issue fixed. As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated. In the case of 3 rd party dependency (code or library not managed and maintained by the EdgeX community) related security issues, while the issue report triggers the same response workflow, the EdgeX community will defer to owning community for fixes. On receipt of a security issue report, SIR: Discusses the issue privately to understand it Uses the Common Vulnerability Scoring System to grade the issue Determines the sub-projects and developers to involve Develops a fix In conjunction with the product group determines when to release the fix Communicates the fix 7. Uploads a Common Vulnerabilities and Exposures (CVE) style report of the issue and associated threat The issue reporter will be kept in the loop as appropriate. Note that a critical or high severity issue can delay a scheduled release to incorporate a fix or mitigation.","title":"Security Vulnerability Response"},{"location":"microservices/security/Ch-SecurityIssues/#public-disclosure-timing","text":"A public disclosure date is negotiated by the EdgeX Product Security Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible AFTER a mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure may be immediate (especially publicly known issues) to a few weeks. The EdgeX Foundry Product Security Committee holds the final say when setting a disclosure date.","title":"Public Disclosure Timing"},{"location":"microservices/security/Ch-StartingSecurity/","text":"Starting security services within EdgeX Similar to other EdgeX services, the security service can be started with Docker Compose. The security services can be started automatically with docker-compose up --d with proper docker compose file. An working sample docker compose file can be found from the edgex repo of Github at https://github.com/edgexfoundry/security-api-gateway/ . If the user prefers to start the security service manually, the commands are described below. docker-compose up -d volume docker-compose up -d config-seed docker-compose up -d consul docker-compose up -d vault docker-compose up -d vault-worker docker-compose up -d kong-db docker-compose up -d kong-migrations docker-compose up -d kong docker-compose up -d edgex-proxy","title":"Starting security services within EdgeX"},{"location":"microservices/security/Ch-StartingSecurity/#starting-security-services-within-edgex","text":"Similar to other EdgeX services, the security service can be started with Docker Compose. The security services can be started automatically with docker-compose up --d with proper docker compose file. An working sample docker compose file can be found from the edgex repo of Github at https://github.com/edgexfoundry/security-api-gateway/ . If the user prefers to start the security service manually, the commands are described below. docker-compose up -d volume docker-compose up -d config-seed docker-compose up -d consul docker-compose up -d vault docker-compose up -d vault-worker docker-compose up -d kong-db docker-compose up -d kong-migrations docker-compose up -d kong docker-compose up -d edgex-proxy","title":"Starting security services within EdgeX"},{"location":"microservices/security/security-file-token-provider.1/","text":"NAME security-file-token-provider -- Generate Vault tokens for EdgeX services SYNOPSIS security-file-token-provider [-h--confdir \\<confdir>] [-p|--profile \\<name>] DESCRIPTION security-file-token-provider generates per-service Vault tokens for EdgeX services so that they can make authenticated connections to Vault to retrieve application secrets. security-file-token-provider implements a generic secret seeding mechanism based on pre-created files and is designed for maximum portability. security-file-token-provider takes a configuration file that specifies the services for which tokens shall be generated and the Vault access policy that shall be applied to those tokens. security-file-token-provider assumes that there is some underlying protection mechanism that will be used to prevent EdgeX services from reading each other's tokens. OPTIONS -h, --help : Display help text -c, --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default FILES configuration.toml This file specifies the TCP/IP location of the Vault service and parameters used for Vault token generation. [SecretService] Scheme = \"https\" Server = \"localhost\" Port = 8200 [TokenFileProvider] PrivilegedTokenPath = /run/edgex/secrets/security-file-token-provider/secrets-token.json ConfigFile = token-config.json OutputDir = /run/edgex/secrets/ OutputFilename = secrets-token.json secrets-token.json This file contains a token used to authenticate to Vault. The filename is customizable via OutputFilename . { \"auth\": { \"client_token\": \"s.wOrq9dO9kzOcuvB06CMviJhZ\" } } token-config.json This configuration file tells security-file-token-provider which tokens to generate. In order to avoid a directory full of .hcl files, this configuration file uses the JSON serialization of HCL, documented at https://github.com/hashicorp/hcl/blob/master/README.md . Note that all paths are keys under the \"path\" object. { \"service-name\": { \"edgex_use_defaults\": true, \"custom_policy\": { \"path\": { \"secret/non/standard/location/*\": { \"capabilities\": [ \"list\", \"read\" ] } } }, \"custom_token_parameters\": { } } } When edgex-use-default is true (the default), the following is added to the policy specification for the auto-generated policy. The auto-generated policy is named edgex-secrets-XYZ where XYZ is service-name from the JSON key above. Thus, the final policy created for the token will be the union of the policy below (if using the default policy) plus the custom_policy defined above. { \"path\": { \"secret/edgex/service-name/*\": { \"capabilities\": [ \"create\", \"update\", \"delete\", \"list\", \"read\" ] } } } When edgex-use-default is true (the default), the following is inserted (if not overridden) to the token parameters for the generated token. (See https://www.vaultproject.io/api/auth/token/index.html#create-token .) \"display_name\": token-service-name \"no_parent\": true \"policies\": [ \"edgex-service-service-name\" ] Note that display_name is set by vault to be \"token-\" + the specified display name. This is hard-coded in Vault from versions 0.6 to 1.2.3 and cannot be changed. Additionally, a meta property, edgex-service-name is set to service-name . The edgex-service-name property may be used by clients to infer the location in the secret store where service-specific secrets are held. \"meta\": { \"edgex-service-name\": service-name } {OutputDir}/{service-name}/{OutputFilename} For example: /run/edgex/secrets/edgex-security-proxy-setup/secrets-token.json For each \"service-name\" in {ConfigFile} , a matching directory is created under {OutputDir} and the corresponding Vault token is stored as {OutputFilename} . This file contains the authorization token generated to allow the indicated EdgeX service to retrieve its secrets. PREREQUISITES PrivilegedTokenPath points to a non-expired Vault token that the security-file-token-provider will use to install policies and create per-service tokens. It will create policies with the naming convention \"edgex-service-service-name\" where service-name comes from JSON keys in the configuration file and the Vault policy will be configured to allow creation and modification of policies using this naming convention. This token must have the following policy ( edgex-privileged-token-creator ) configured. path \"auth/token/create\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create-orphan\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create/*\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"sys/policies/acl/edgex-service-*\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } path \"sys/policies/acl\" { capabilities = [ \"list\" ] } AUTHOR EdgeX Foundry \\< info@edgexfoundry.org >","title":"NAME"},{"location":"microservices/security/security-file-token-provider.1/#name","text":"security-file-token-provider -- Generate Vault tokens for EdgeX services","title":"NAME"},{"location":"microservices/security/security-file-token-provider.1/#synopsis","text":"security-file-token-provider [-h--confdir \\<confdir>] [-p|--profile \\<name>]","title":"SYNOPSIS"},{"location":"microservices/security/security-file-token-provider.1/#description","text":"security-file-token-provider generates per-service Vault tokens for EdgeX services so that they can make authenticated connections to Vault to retrieve application secrets. security-file-token-provider implements a generic secret seeding mechanism based on pre-created files and is designed for maximum portability. security-file-token-provider takes a configuration file that specifies the services for which tokens shall be generated and the Vault access policy that shall be applied to those tokens. security-file-token-provider assumes that there is some underlying protection mechanism that will be used to prevent EdgeX services from reading each other's tokens.","title":"DESCRIPTION"},{"location":"microservices/security/security-file-token-provider.1/#options","text":"-h, --help : Display help text -c, --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default","title":"OPTIONS"},{"location":"microservices/security/security-file-token-provider.1/#files","text":"","title":"FILES"},{"location":"microservices/security/security-file-token-provider.1/#configurationtoml","text":"This file specifies the TCP/IP location of the Vault service and parameters used for Vault token generation. [SecretService] Scheme = \"https\" Server = \"localhost\" Port = 8200 [TokenFileProvider] PrivilegedTokenPath = /run/edgex/secrets/security-file-token-provider/secrets-token.json ConfigFile = token-config.json OutputDir = /run/edgex/secrets/ OutputFilename = secrets-token.json","title":"configuration.toml"},{"location":"microservices/security/security-file-token-provider.1/#secrets-tokenjson","text":"This file contains a token used to authenticate to Vault. The filename is customizable via OutputFilename . { \"auth\": { \"client_token\": \"s.wOrq9dO9kzOcuvB06CMviJhZ\" } }","title":"secrets-token.json"},{"location":"microservices/security/security-file-token-provider.1/#token-configjson","text":"This configuration file tells security-file-token-provider which tokens to generate. In order to avoid a directory full of .hcl files, this configuration file uses the JSON serialization of HCL, documented at https://github.com/hashicorp/hcl/blob/master/README.md . Note that all paths are keys under the \"path\" object. { \"service-name\": { \"edgex_use_defaults\": true, \"custom_policy\": { \"path\": { \"secret/non/standard/location/*\": { \"capabilities\": [ \"list\", \"read\" ] } } }, \"custom_token_parameters\": { } } } When edgex-use-default is true (the default), the following is added to the policy specification for the auto-generated policy. The auto-generated policy is named edgex-secrets-XYZ where XYZ is service-name from the JSON key above. Thus, the final policy created for the token will be the union of the policy below (if using the default policy) plus the custom_policy defined above. { \"path\": { \"secret/edgex/service-name/*\": { \"capabilities\": [ \"create\", \"update\", \"delete\", \"list\", \"read\" ] } } } When edgex-use-default is true (the default), the following is inserted (if not overridden) to the token parameters for the generated token. (See https://www.vaultproject.io/api/auth/token/index.html#create-token .) \"display_name\": token-service-name \"no_parent\": true \"policies\": [ \"edgex-service-service-name\" ] Note that display_name is set by vault to be \"token-\" + the specified display name. This is hard-coded in Vault from versions 0.6 to 1.2.3 and cannot be changed. Additionally, a meta property, edgex-service-name is set to service-name . The edgex-service-name property may be used by clients to infer the location in the secret store where service-specific secrets are held. \"meta\": { \"edgex-service-name\": service-name }","title":"token-config.json"},{"location":"microservices/security/security-file-token-provider.1/#outputdirservice-nameoutputfilename","text":"For example: /run/edgex/secrets/edgex-security-proxy-setup/secrets-token.json For each \"service-name\" in {ConfigFile} , a matching directory is created under {OutputDir} and the corresponding Vault token is stored as {OutputFilename} . This file contains the authorization token generated to allow the indicated EdgeX service to retrieve its secrets.","title":"{OutputDir}/{service-name}/{OutputFilename}"},{"location":"microservices/security/security-file-token-provider.1/#prerequisites","text":"PrivilegedTokenPath points to a non-expired Vault token that the security-file-token-provider will use to install policies and create per-service tokens. It will create policies with the naming convention \"edgex-service-service-name\" where service-name comes from JSON keys in the configuration file and the Vault policy will be configured to allow creation and modification of policies using this naming convention. This token must have the following policy ( edgex-privileged-token-creator ) configured. path \"auth/token/create\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create-orphan\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create/*\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"sys/policies/acl/edgex-service-*\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } path \"sys/policies/acl\" { capabilities = [ \"list\" ] }","title":"PREREQUISITES"},{"location":"microservices/security/security-file-token-provider.1/#author","text":"EdgeX Foundry \\< info@edgexfoundry.org >","title":"AUTHOR"},{"location":"microservices/security/security-secrets-setup.1/","text":"NAME security-secrets-setup --- Creates an on-device public-key infrastructure (PKI) to secure microservice secret management SYNOPSIS | security-secrets-setup generate [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup cache [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup import [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup [-h|--help] DESCRIPTION The Vault secret management component of EdgeX Foundry requires TLS encryption of secrets over the wire via a pre-created PKI. security-secrets-setup is responsible for creating a certificate authority and any needed TLS leaf certificates in order to secure the EdgeX security services. security-secrets-setup supports several modes of operation as defined in the OPTIONS section. As the PKI is security-sensitive, this tool takes a number of precautions to safeguard the PKI: The PKI can be deployed to transient storage to address potential attacks to the PKI at-rest. The PKI is deployed such that each service has its own assets folder, which is amenable to security controls imposed by container runtimes such as mandatory access controls or file system namespaces. The private key of the certificate authority (CA) is shredded (securely erased) prior to caching or deployment to block issuance of new CA descendants (this is most relevant in caching mode). Modes of operation generate : Causes a PKI to be generated afresh every time and deployed. Typically, this will be whenever the framework is started. cache : Causes a PKI to be generated exactly once and then copied to a designated cache location for future use. The PKI is then deployed from the cached location. import : This option is similar to cache in that it deploys a PKI from CacheDir to DeployDir , but it forces an error if CacheDir is empty instead of triggering PKI generation. This enables usage models for deploying a pre-populated PKI such as a Kong certificate signed by an external certificate authority or TLS keys signed by an offline enterprise certificate authority. OPTIONS -h, --help : Display help text --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default FILES pkisetup-vault.json, pkisetup-kong.json --------------------------------- Configuration files for certificate parameters. These files conform to the following schema: { \"create_new_rootca\": \"true|false\", \"working_dir\": \"./config\", \"pki_setup_dir\": \"pki\", \"dump_config\": \"true\", \"key_scheme\": { \"dump_keys\": \"false\", \"rsa\": \"false\", \"rsa_key_size\": \"4096\", \"ec\": \"true\", \"ec_curve\": \"384\" }, \"x509_root_ca_parameters\": { \"ca_name\": \"EdgeXFoundryCA\", \"ca_c\": \"US\", \"ca_st\": \"CA\", \"ca_l\": \"San Francisco\", \"ca_o\": \"EdgeXFoundry\" }, \"x509_tls_server_parameters\": { \"tls_host\": \"edgex-vault|edgex-kong\", \"tls_domain\": \"local\", \"tls_c\": \"US\", \"tls_st\": \"CA\", \"tls_l\": \"San Francisco\", \"tls_o\": \"Kong\" } } When generating or caching, the utility hard-codes the names of the configuration files and always processes pkisetup-vault.json first and pkisetup-kong.json second. This will be configurable in the future; at that time the basename of the file would correspond to a directory under DeployDir . configuration.toml Configuration file for configurable directories that the options use. This file conforms to the following schema: [SecretsSetup] WorkDir = \"/path/to/temp/files\" CacheDir = \"/path/to/cached-or-importing/pki\" DeployDir = \"/path/to/deployed/pki\" WorkDir A work area (preferably on a ramdisk) to place working files during certificate generation. If not supplied, temporary files will be generated to a subdirectory ( /edgex/security-secrets-setup ) of $XDG_RUNTIME_DIR . If $XDG_RUNTIME_DIR is undefined, uses /tmp instead. DeployDir Points to the base directory for the final deployment location of the PKI. If not specified, defaults to /run/edgex/secrets/ . For example, if DeployDir was set to /edgex and the service name was edgex-vault then the following files would be placed in /edgex/edgex-vault/ : server.crt for a PEM-encoded end-entity TLS certificate and server.key for the corresponding private key .security-secrets-setup.complete is a sentinel file created after assets are deployed CacheDir Points to a base directory to hold the cached PKI. Identical in structure to that created in DeployDir . Defaults to /etc/edgex/pki if not specified. The PKI is deployed from here when the tool is run in caching or importing. ENVIRONMENT XDG_RUNTIME_DIR Used as default value for WorkDir if not otherwise specified. NOTES As security-secrets-setup is a helper utility to ensure that a PKI is created on first launch, it is intended that security-secrets-setup is always invoked with the same command. Changing from cache to generate will cause the cache to be ignored when deploying a PKI and changing it back will cause a reversion to a stale CA. Changing from cache to import mode of operation is not noticeable by the tool: the PKI that is in the cache will be the one deployed. To force regeneration of the PKI cache after the first launch, the PKI cache must be manually cleaned. The easiest way in Docker would be to delete the Docker volume holding the cached PKI.","title":"NAME"},{"location":"microservices/security/security-secrets-setup.1/#name","text":"security-secrets-setup --- Creates an on-device public-key infrastructure (PKI) to secure microservice secret management","title":"NAME"},{"location":"microservices/security/security-secrets-setup.1/#synopsis","text":"| security-secrets-setup generate [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup cache [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup import [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup [-h|--help]","title":"SYNOPSIS"},{"location":"microservices/security/security-secrets-setup.1/#description","text":"The Vault secret management component of EdgeX Foundry requires TLS encryption of secrets over the wire via a pre-created PKI. security-secrets-setup is responsible for creating a certificate authority and any needed TLS leaf certificates in order to secure the EdgeX security services. security-secrets-setup supports several modes of operation as defined in the OPTIONS section. As the PKI is security-sensitive, this tool takes a number of precautions to safeguard the PKI: The PKI can be deployed to transient storage to address potential attacks to the PKI at-rest. The PKI is deployed such that each service has its own assets folder, which is amenable to security controls imposed by container runtimes such as mandatory access controls or file system namespaces. The private key of the certificate authority (CA) is shredded (securely erased) prior to caching or deployment to block issuance of new CA descendants (this is most relevant in caching mode).","title":"DESCRIPTION"},{"location":"microservices/security/security-secrets-setup.1/#modes-of-operation","text":"generate : Causes a PKI to be generated afresh every time and deployed. Typically, this will be whenever the framework is started. cache : Causes a PKI to be generated exactly once and then copied to a designated cache location for future use. The PKI is then deployed from the cached location. import : This option is similar to cache in that it deploys a PKI from CacheDir to DeployDir , but it forces an error if CacheDir is empty instead of triggering PKI generation. This enables usage models for deploying a pre-populated PKI such as a Kong certificate signed by an external certificate authority or TLS keys signed by an offline enterprise certificate authority.","title":"Modes of operation"},{"location":"microservices/security/security-secrets-setup.1/#options","text":"-h, --help : Display help text --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default","title":"OPTIONS"},{"location":"microservices/security/security-secrets-setup.1/#files","text":"pkisetup-vault.json, pkisetup-kong.json --------------------------------- Configuration files for certificate parameters. These files conform to the following schema: { \"create_new_rootca\": \"true|false\", \"working_dir\": \"./config\", \"pki_setup_dir\": \"pki\", \"dump_config\": \"true\", \"key_scheme\": { \"dump_keys\": \"false\", \"rsa\": \"false\", \"rsa_key_size\": \"4096\", \"ec\": \"true\", \"ec_curve\": \"384\" }, \"x509_root_ca_parameters\": { \"ca_name\": \"EdgeXFoundryCA\", \"ca_c\": \"US\", \"ca_st\": \"CA\", \"ca_l\": \"San Francisco\", \"ca_o\": \"EdgeXFoundry\" }, \"x509_tls_server_parameters\": { \"tls_host\": \"edgex-vault|edgex-kong\", \"tls_domain\": \"local\", \"tls_c\": \"US\", \"tls_st\": \"CA\", \"tls_l\": \"San Francisco\", \"tls_o\": \"Kong\" } } When generating or caching, the utility hard-codes the names of the configuration files and always processes pkisetup-vault.json first and pkisetup-kong.json second. This will be configurable in the future; at that time the basename of the file would correspond to a directory under DeployDir .","title":"FILES"},{"location":"microservices/security/security-secrets-setup.1/#configurationtoml","text":"Configuration file for configurable directories that the options use. This file conforms to the following schema: [SecretsSetup] WorkDir = \"/path/to/temp/files\" CacheDir = \"/path/to/cached-or-importing/pki\" DeployDir = \"/path/to/deployed/pki\"","title":"configuration.toml"},{"location":"microservices/security/security-secrets-setup.1/#workdir","text":"A work area (preferably on a ramdisk) to place working files during certificate generation. If not supplied, temporary files will be generated to a subdirectory ( /edgex/security-secrets-setup ) of $XDG_RUNTIME_DIR . If $XDG_RUNTIME_DIR is undefined, uses /tmp instead.","title":"WorkDir"},{"location":"microservices/security/security-secrets-setup.1/#deploydir","text":"Points to the base directory for the final deployment location of the PKI. If not specified, defaults to /run/edgex/secrets/ . For example, if DeployDir was set to /edgex and the service name was edgex-vault then the following files would be placed in /edgex/edgex-vault/ : server.crt for a PEM-encoded end-entity TLS certificate and server.key for the corresponding private key .security-secrets-setup.complete is a sentinel file created after assets are deployed","title":"DeployDir"},{"location":"microservices/security/security-secrets-setup.1/#cachedir","text":"Points to a base directory to hold the cached PKI. Identical in structure to that created in DeployDir . Defaults to /etc/edgex/pki if not specified. The PKI is deployed from here when the tool is run in caching or importing.","title":"CacheDir"},{"location":"microservices/security/security-secrets-setup.1/#environment","text":"XDG_RUNTIME_DIR Used as default value for WorkDir if not otherwise specified.","title":"ENVIRONMENT"},{"location":"microservices/security/security-secrets-setup.1/#notes","text":"As security-secrets-setup is a helper utility to ensure that a PKI is created on first launch, it is intended that security-secrets-setup is always invoked with the same command. Changing from cache to generate will cause the cache to be ignored when deploying a PKI and changing it back will cause a reversion to a stale CA. Changing from cache to import mode of operation is not noticeable by the tool: the PKI that is in the cache will be the one deployed. To force regeneration of the PKI cache after the first launch, the PKI cache must be manually cleaned. The easiest way in Docker would be to delete the Docker volume holding the cached PKI.","title":"NOTES"},{"location":"microservices/support/Ch-SupportingServices/","text":"Supporting Services Microservices","title":"Supporting Services Microservices"},{"location":"microservices/support/Ch-SupportingServices/#supporting-services-microservices","text":"","title":"Supporting Services Microservices"},{"location":"microservices/support/logging/Ch-Logging/","text":"Logging Introduction Logging is critical for all modern software applications. Proper logging provides the users with the following benefits: Ability to monitor and understand what systems are doing Ability to understand how services interact with each other Problems are detected and fixed quickly Monitoring to foster performance improvements The graphic shows the high-level design architecture of EdgeX Foundry including the Logging Service. Minimum Product Feature Set Provides a RESTful API for other microservices to request log entries with the following characteristics: The RESTful calls should be non-blocking, meaning calling services should fire logging requests without waiting for any response from the log service to achieve minimal impact to the speed and performance to the services. Support multiple logging levels, for example trace, debug, info, warn, error, fatal, and so forth. Each log entry should be associated with its originating service. Provide RESTful APIs to query, clear, or prune log entries based on any combination of following parameters: Timestamp from Timestamp to Log level Originating service Log entries should be persisted in either file or database, and the persistence storage should be managed at configurable levels. Querying via the REST API is only supported for deployments using database storage. Take advantage of an existing logging framework internally and provide the \"wrapper\" for use by EdgeX Foundry Follow applicable standards for logging where possible and not onerous to use on the gateway High Level Design Architecture The above diagram shows the high-level architecture for EdgeX Foundry Logging Service. Other microservices interact with EdgeX Foundry Logging Service through RESTful APIs to submit their logging requests, query historical logging, and remove historical logging. Internally, EdgeX Foundry's Logging Service utilizes the GoKit logger as its internal logging framework. Two configurable persistence options exist supported by EdgeX Foundry Logging Service: file or MongoDB. Configuration Properties Configuration Default Value Dependencies Entries in the Writable section of the configuration can be changed on the fly while the service is running if the service is running with the \u2013registry / -r flag Writable Persistence database * \"file\" to save logging in file; \"database\" to save logging in MongoDB Writable LogLevel INFO * Logs messages set to a level of \"INFO\" or higher The following keys represent the core service-level configuration settings Service MaxResultCount 50000 ** Read data limit per invocation Service BootTimeout 300000 ** Heart beat time in milliseconds Service StartupMsg Logging Service heart beat ** Heart beat message Service Port 48061 ** Micro service port number Service Host localhost ** Micro service host name Service Protocol http ** Micro service host protocol Service ClientMonitor 15000 ** The interval in milliseconds at which any service clients will refresh their endpoint information from the service registry (Consul) Service CheckInterval 10s ** The interval in seconds at which the service registry(Consul) will conduct a health check of this service. Service Timeout 5000 ** Specifies a timeout (in milliseconds) for handling requests Following config only take effect when Writable.Persistence=file Logging File ./logs/edgex-support-logging.log File path to save logging entries Following config only take effect when logging.persistence=database Databases Database Primary Username [empty string] ** DB user name Databases Database Password [empty string] ** DB password Databases Database Host localhost ** DB host name Databases Database Port 27017 ** DB port number Databases Database Database logging ** Database or document store name Databases Database Timeout 5000 ** DB connection timeout Databases Database Type mongodb ** DB type Following config only take effect when connecting to the registry for configuration info Registry Host localhost ** Registry host name Registry Port 8500 ** Registry port number Registry Type consul ** Registry implementation type *means the configuration value can be changed on the fly if using a configuration registry (like Consul). **means the configuration value can be changed but the service must be restarted. ***means the configuration value should NOT be changed. Logging Service Client Library for Go As the reference implementation of EdgeX Foundry microservices is written in Go, we provide a Client Library for Go so that Go-based microservices could directly switch their Loggers to use the EdgeX Foundry Logging Service. The Go LoggingClient is part of the go-mod-core-contracts module . This module can be imported into your project by including a reference in your go.mod. You can either do this manually or by executing \"go get github.com/edgexfoundry/go-mod-core-contracts\" from your project directory will add a reference to the latest tagged version of the module. After that, simply import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" into a given package where your functionality will be implemented. Declare a variable or type member as logger.LoggingClient and it's ready for use. package main import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" func main () { client : = logger . LoggingClient // LoggingClient is now ready for use . A method is exposed for each LogLevel client . Trace ( \"some info\" ) client . Debug ( \"some info\" ) client . Info ( \"some info\" ) client . Warn ( \"some info\" ) client . Error ( \"some info\" ) } Log statements will only be written to the log if they match or exceed the minimum LogLevel set in the configuration (described above). This setting can be changed on the fly without restarting the service to help with real-time troubleshooting. Log statements are currently output in a simple key/value format. For example: level=INFO ts=2019-05-16T22:23:44.424176Z app=edgex-support-notifications source=cleanup.go:32 msg=\"Cleaning up of notifications and transmissions\" Everything up to the \"msg\" key is handled by the logging infrastructure. You get the log level, timestamp, service name and the location in the source code of the logging statement for free with every method invocation on the LoggingClient. The \"msg\" key's value is the first parameter passed to one of the Logging Client methods shown above. So to extend the usage example a bit, the above calls would result in something like: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" You can add as many custom key/value pairs as you like by simply adding them to the method call: client.Info(\"some info\",\"key1\",\"abc\",\"key2\",\"def\") This would result in: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" key1=abc key2=def Quotes are only put around values that contain spaces. EdgeX Logging Keys Within the EdgeX Go reference implementation, log entries are currently written as a set of key/value pairs. We may change this later to be more of a struct type than can be formatted according to the user's requirements (JSON, XML, system, etc). In that case, the targeted struct should contain properties that support the keys utilized by the system and described below. Key Intent level Indicates the log level of the individual log entry (INFO, DEBUG, ERROR, etc) ts The timestamp of the log entry, recorded in UTC app This should contain the service key of the service writing the log entry source The file and line number where the log entry was written msg A field for custom information accompanying the log entry. You do not need to specify this explicitly as it is the first parameter when calling one of the LoggingClient's functions. correlation-id Records the correlation-id header value that is scoped to a given request. It has two sub-ordinate, associated fields (see below). correlation-id path This field records the API route being requested and is utilized when the service begins handling a request. * Example: path=/api/v1/event When beginning the request handling, by convention set \"msg\" to \"Begin request\". correlation-id duration This field records the amount of time taken to handle a given request. When completing the request handling, by convention set \"msg\" to \"Response complete\". Additional keys can be added as need warrants. This document should be kept updated to reflect their inclusion and purpose.","title":"Logging"},{"location":"microservices/support/logging/Ch-Logging/#logging","text":"","title":"Logging"},{"location":"microservices/support/logging/Ch-Logging/#introduction","text":"Logging is critical for all modern software applications. Proper logging provides the users with the following benefits: Ability to monitor and understand what systems are doing Ability to understand how services interact with each other Problems are detected and fixed quickly Monitoring to foster performance improvements The graphic shows the high-level design architecture of EdgeX Foundry including the Logging Service.","title":"Introduction"},{"location":"microservices/support/logging/Ch-Logging/#minimum-product-feature-set","text":"Provides a RESTful API for other microservices to request log entries with the following characteristics: The RESTful calls should be non-blocking, meaning calling services should fire logging requests without waiting for any response from the log service to achieve minimal impact to the speed and performance to the services. Support multiple logging levels, for example trace, debug, info, warn, error, fatal, and so forth. Each log entry should be associated with its originating service. Provide RESTful APIs to query, clear, or prune log entries based on any combination of following parameters: Timestamp from Timestamp to Log level Originating service Log entries should be persisted in either file or database, and the persistence storage should be managed at configurable levels. Querying via the REST API is only supported for deployments using database storage. Take advantage of an existing logging framework internally and provide the \"wrapper\" for use by EdgeX Foundry Follow applicable standards for logging where possible and not onerous to use on the gateway","title":"Minimum Product Feature Set"},{"location":"microservices/support/logging/Ch-Logging/#high-level-design-architecture","text":"The above diagram shows the high-level architecture for EdgeX Foundry Logging Service. Other microservices interact with EdgeX Foundry Logging Service through RESTful APIs to submit their logging requests, query historical logging, and remove historical logging. Internally, EdgeX Foundry's Logging Service utilizes the GoKit logger as its internal logging framework. Two configurable persistence options exist supported by EdgeX Foundry Logging Service: file or MongoDB.","title":"High Level Design Architecture"},{"location":"microservices/support/logging/Ch-Logging/#configuration-properties","text":"Configuration Default Value Dependencies Entries in the Writable section of the configuration can be changed on the fly while the service is running if the service is running with the \u2013registry / -r flag Writable Persistence database * \"file\" to save logging in file; \"database\" to save logging in MongoDB Writable LogLevel INFO * Logs messages set to a level of \"INFO\" or higher The following keys represent the core service-level configuration settings Service MaxResultCount 50000 ** Read data limit per invocation Service BootTimeout 300000 ** Heart beat time in milliseconds Service StartupMsg Logging Service heart beat ** Heart beat message Service Port 48061 ** Micro service port number Service Host localhost ** Micro service host name Service Protocol http ** Micro service host protocol Service ClientMonitor 15000 ** The interval in milliseconds at which any service clients will refresh their endpoint information from the service registry (Consul) Service CheckInterval 10s ** The interval in seconds at which the service registry(Consul) will conduct a health check of this service. Service Timeout 5000 ** Specifies a timeout (in milliseconds) for handling requests Following config only take effect when Writable.Persistence=file Logging File ./logs/edgex-support-logging.log File path to save logging entries Following config only take effect when logging.persistence=database Databases Database Primary Username [empty string] ** DB user name Databases Database Password [empty string] ** DB password Databases Database Host localhost ** DB host name Databases Database Port 27017 ** DB port number Databases Database Database logging ** Database or document store name Databases Database Timeout 5000 ** DB connection timeout Databases Database Type mongodb ** DB type Following config only take effect when connecting to the registry for configuration info Registry Host localhost ** Registry host name Registry Port 8500 ** Registry port number Registry Type consul ** Registry implementation type *means the configuration value can be changed on the fly if using a configuration registry (like Consul). **means the configuration value can be changed but the service must be restarted. ***means the configuration value should NOT be changed.","title":"Configuration Properties"},{"location":"microservices/support/logging/Ch-Logging/#logging-service-client-library-for-go","text":"As the reference implementation of EdgeX Foundry microservices is written in Go, we provide a Client Library for Go so that Go-based microservices could directly switch their Loggers to use the EdgeX Foundry Logging Service. The Go LoggingClient is part of the go-mod-core-contracts module . This module can be imported into your project by including a reference in your go.mod. You can either do this manually or by executing \"go get github.com/edgexfoundry/go-mod-core-contracts\" from your project directory will add a reference to the latest tagged version of the module. After that, simply import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" into a given package where your functionality will be implemented. Declare a variable or type member as logger.LoggingClient and it's ready for use. package main import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" func main () { client : = logger . LoggingClient // LoggingClient is now ready for use . A method is exposed for each LogLevel client . Trace ( \"some info\" ) client . Debug ( \"some info\" ) client . Info ( \"some info\" ) client . Warn ( \"some info\" ) client . Error ( \"some info\" ) } Log statements will only be written to the log if they match or exceed the minimum LogLevel set in the configuration (described above). This setting can be changed on the fly without restarting the service to help with real-time troubleshooting. Log statements are currently output in a simple key/value format. For example: level=INFO ts=2019-05-16T22:23:44.424176Z app=edgex-support-notifications source=cleanup.go:32 msg=\"Cleaning up of notifications and transmissions\" Everything up to the \"msg\" key is handled by the logging infrastructure. You get the log level, timestamp, service name and the location in the source code of the logging statement for free with every method invocation on the LoggingClient. The \"msg\" key's value is the first parameter passed to one of the Logging Client methods shown above. So to extend the usage example a bit, the above calls would result in something like: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" You can add as many custom key/value pairs as you like by simply adding them to the method call: client.Info(\"some info\",\"key1\",\"abc\",\"key2\",\"def\") This would result in: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" key1=abc key2=def Quotes are only put around values that contain spaces.","title":"Logging Service Client Library for Go"},{"location":"microservices/support/logging/Ch-Logging/#edgex-logging-keys","text":"Within the EdgeX Go reference implementation, log entries are currently written as a set of key/value pairs. We may change this later to be more of a struct type than can be formatted according to the user's requirements (JSON, XML, system, etc). In that case, the targeted struct should contain properties that support the keys utilized by the system and described below. Key Intent level Indicates the log level of the individual log entry (INFO, DEBUG, ERROR, etc) ts The timestamp of the log entry, recorded in UTC app This should contain the service key of the service writing the log entry source The file and line number where the log entry was written msg A field for custom information accompanying the log entry. You do not need to specify this explicitly as it is the first parameter when calling one of the LoggingClient's functions. correlation-id Records the correlation-id header value that is scoped to a given request. It has two sub-ordinate, associated fields (see below). correlation-id path This field records the API route being requested and is utilized when the service begins handling a request. * Example: path=/api/v1/event When beginning the request handling, by convention set \"msg\" to \"Begin request\". correlation-id duration This field records the amount of time taken to handle a given request. When completing the request handling, by convention set \"msg\" to \"Response complete\". Additional keys can be added as need warrants. This document should be kept updated to reflect their inclusion and purpose.","title":"EdgeX Logging Keys"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/","text":"Alerts & Notifications Introduction When notification to another system or to a person, needs to occur to notify of something discovered on the node by another microservice, the Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services could need to broadcast, include sensor data detected outside of certain parameters (usually detected by a Rules Engine service) or system or service malfunctions (usually detected by System Management services). Terminology Notifications are informative, whereas Alerts are typically of a more important, critical, or urgent nature, possibly requiring immediate action. The diagram shows the high-level architecture of Alerts and Notifications. On the left side, the APIs are provided for other microservices, on-box applications, and off-box applications to use, and the APIs could be in REST, AMQP, MQTT, or any standard application protocols. Currently in EdgeX Foundry, the RESTful interface is provided. On the right side, the notification receiver could be a person or an application system on Cloud or in a server room. By invoking the Subscription RESTful interface to subscribe the specific types of notifications, the receiver obtains the appropriate notifications through defined receiving channels when events occur. The receiving channels include SMS message, e-mail, REST callback, AMQP, MQTT, and so on. Currently in EdgeX Foundry, e-mail and REST callback channels are provided. When Alerts and Notifications receive notifications from any interface, the notifications are passed to the Notifications Handler internally. The Notifications Handler persists the received notifications first, and passes them to the Distribution Coordinator immediately when a given notification is either critical (severity = \u201cCRITICAL\u201d) or when it is normal (severity = \u201cNORMAL\u201d). When the Distribution Coordinator receives a notification, it first queries the subscription to acquire receivers who need to obtain this notification and their receiving channel information. According to the channel information, the Distribution Coordinator passes this notification to the corresponding channel senders. Then, the channel senders send out the notifications to the subscribed receivers. Data Model MongoDB is selected for the persistence of Alerts and Notifications, so the data model design is without foreign key and based on the paradigm of document structure. Data Dictionary Class Name Description Channel The object used to describe the Notification end point. Notification The object used to describe the message and sender content of a Notification. Transmission The object used for grouping of Notifications. High Level Interaction Diagrams This section shows the sequence diagrams for some of the more critical or complex events regarding Alerts and Notifications. Critical Notifications Sequence When receiving a critical notification (SEVERITY = \"CRITICAL\"), it persists first and triggers the distribution process immediately. After updating the notification status, Alerts and Notifications respond to the client to indicate the notification has been accepted. Normal Notifications Sequence When receiving a normal notification (SEVERITY = \"NORMAL\"), it persists first and responds to the client to indicate the notification has been accepted immediately. After a configurable duration, a scheduler triggers the distribution process in batch. Critical Resend Sequence When encountering any error during sending critical notification, an individual resend task is scheduled, and each transmission record persists. If the resend tasks keeps failing and the resend count exceeds the configurable limit, the escalation process is triggered. The escalated notification is sent to particular receivers of a special subscription (slug = \"ESCALATION\"). Resend Sequence For other non-critical notifications, the resend operation is triggered by a scheduler. Cleanup Sequence Cleanup service removes old notification and transmission records. Configuration Properties Configuration Default Value Dependencies Service MaxResultCount 50000 * Read data limit per invocation Service BootTimeout 300000 * Heart beat time in milliseconds Service StartupMsg This is the Support Notifications Microservice * Heart beat message Service Port 48060 ** Micro service port number Service Host localhost ** Micro service host name Service Protocol http ** Micro service host protocol Service ClientMonitor 15000 ** Service CheckInterval 10s ** Service Timeout 5000 ** ResendLimit 2 * Number of attempts to resend a notification Following config only take effect when logging.persistence=file Logging File /logs/edgex-support-notifications.log File path to save logging entries Logging EnableRemote false Indicate whether to use the logging service (vs local log file) Following config only take effect when logging.persistence=database Databases Database Primary Username [empty string] ** DB user name Databases Database Password [empty string] * DB password Databases Database Host localhost ** DB host name Databases Database Port 27017 ** DB port number Databases Database Database logging * database or document store name Databases Database Timeout 5000 * DB connection timeout Databases Database Type mongodb ** DB type Following config only take effect when connecting to the registry for configuraiton info Registry Host localhost ** Registry host name Registry Port 8500 ** Registry port number Registry Type consul ** Registry implementation type Following config only take effect when connecting to the remote logging service Clients Clients.Logging Host localhost ** Remote logging service host name Clients Clients.Logging Port 48061 ** Remote logging service port number Clients Clients.Logging Protocol http ** Remote logging service host protocl Following config apply to using the SMTP service Smtp Host smtp.gmail.com ** SMTP service host name Smtp Port 25 ** SMTP service port number Smtp Password mypassword ** SMTP service host access password Smtp Sender jdoe@gmail.com ** SMTP service sendor/username Smtp Subject EdgeX Notification ** SMTP alert message subject *means the configuration value can be changed if necessary. **means the configuration value has to be replaced. Configure Mail Server All the properties with prefix \"smtp\" are for mail server configuration. Configure the mail server appropriately to send Alerts and Notifications. The correct values depend on which mail server is used. Gmail Before using Gmail to send Alerts and Notifications, configure the sign-in security settings through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). An App password is a 16-digit passcode that gives an app or device permission to access your Google Account. For more detail about this topic, please refer to this Google official document: https://support.google.com/accounts/answer/185833 . Allow less secure apps: If the 2-Step Verification is not enabled, you may need to allow less secure apps to access the Gmail account. Please see the instruction from this Google official document on this topic: https://support.google.com/accounts/answer/6010255 . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.gmail.com Smtp Sender= ${ Gmail account } Smtp Password= ${ Gmail password or App password } Yahoo Mail Similar to Gmail, configure the sign-in security settings for Yahoo through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). Please see this Yahoo official document for more detail: https://help.yahoo.com/kb/SLN15241.html . Allow apps that use less secure sign in. Please see this Yahoo official document for more detail on this topic: https://help.yahoo.com/kb/SLN27791.html . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.mail.yahoo.com Smtp Sender= ${ Yahoo account } Smtp Password= ${ Yahoo password or App password }","title":"Alerts & Notifications"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#alerts-notifications","text":"","title":"Alerts &amp; Notifications"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#introduction","text":"When notification to another system or to a person, needs to occur to notify of something discovered on the node by another microservice, the Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services could need to broadcast, include sensor data detected outside of certain parameters (usually detected by a Rules Engine service) or system or service malfunctions (usually detected by System Management services). Terminology Notifications are informative, whereas Alerts are typically of a more important, critical, or urgent nature, possibly requiring immediate action. The diagram shows the high-level architecture of Alerts and Notifications. On the left side, the APIs are provided for other microservices, on-box applications, and off-box applications to use, and the APIs could be in REST, AMQP, MQTT, or any standard application protocols. Currently in EdgeX Foundry, the RESTful interface is provided. On the right side, the notification receiver could be a person or an application system on Cloud or in a server room. By invoking the Subscription RESTful interface to subscribe the specific types of notifications, the receiver obtains the appropriate notifications through defined receiving channels when events occur. The receiving channels include SMS message, e-mail, REST callback, AMQP, MQTT, and so on. Currently in EdgeX Foundry, e-mail and REST callback channels are provided. When Alerts and Notifications receive notifications from any interface, the notifications are passed to the Notifications Handler internally. The Notifications Handler persists the received notifications first, and passes them to the Distribution Coordinator immediately when a given notification is either critical (severity = \u201cCRITICAL\u201d) or when it is normal (severity = \u201cNORMAL\u201d). When the Distribution Coordinator receives a notification, it first queries the subscription to acquire receivers who need to obtain this notification and their receiving channel information. According to the channel information, the Distribution Coordinator passes this notification to the corresponding channel senders. Then, the channel senders send out the notifications to the subscribed receivers.","title":"Introduction"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#data-model","text":"MongoDB is selected for the persistence of Alerts and Notifications, so the data model design is without foreign key and based on the paradigm of document structure.","title":"Data Model"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#data-dictionary","text":"Class Name Description Channel The object used to describe the Notification end point. Notification The object used to describe the message and sender content of a Notification. Transmission The object used for grouping of Notifications.","title":"Data Dictionary"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#high-level-interaction-diagrams","text":"This section shows the sequence diagrams for some of the more critical or complex events regarding Alerts and Notifications. Critical Notifications Sequence When receiving a critical notification (SEVERITY = \"CRITICAL\"), it persists first and triggers the distribution process immediately. After updating the notification status, Alerts and Notifications respond to the client to indicate the notification has been accepted. Normal Notifications Sequence When receiving a normal notification (SEVERITY = \"NORMAL\"), it persists first and responds to the client to indicate the notification has been accepted immediately. After a configurable duration, a scheduler triggers the distribution process in batch. Critical Resend Sequence When encountering any error during sending critical notification, an individual resend task is scheduled, and each transmission record persists. If the resend tasks keeps failing and the resend count exceeds the configurable limit, the escalation process is triggered. The escalated notification is sent to particular receivers of a special subscription (slug = \"ESCALATION\"). Resend Sequence For other non-critical notifications, the resend operation is triggered by a scheduler. Cleanup Sequence Cleanup service removes old notification and transmission records.","title":"High Level Interaction Diagrams"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#configuration-properties","text":"Configuration Default Value Dependencies Service MaxResultCount 50000 * Read data limit per invocation Service BootTimeout 300000 * Heart beat time in milliseconds Service StartupMsg This is the Support Notifications Microservice * Heart beat message Service Port 48060 ** Micro service port number Service Host localhost ** Micro service host name Service Protocol http ** Micro service host protocol Service ClientMonitor 15000 ** Service CheckInterval 10s ** Service Timeout 5000 ** ResendLimit 2 * Number of attempts to resend a notification Following config only take effect when logging.persistence=file Logging File /logs/edgex-support-notifications.log File path to save logging entries Logging EnableRemote false Indicate whether to use the logging service (vs local log file) Following config only take effect when logging.persistence=database Databases Database Primary Username [empty string] ** DB user name Databases Database Password [empty string] * DB password Databases Database Host localhost ** DB host name Databases Database Port 27017 ** DB port number Databases Database Database logging * database or document store name Databases Database Timeout 5000 * DB connection timeout Databases Database Type mongodb ** DB type Following config only take effect when connecting to the registry for configuraiton info Registry Host localhost ** Registry host name Registry Port 8500 ** Registry port number Registry Type consul ** Registry implementation type Following config only take effect when connecting to the remote logging service Clients Clients.Logging Host localhost ** Remote logging service host name Clients Clients.Logging Port 48061 ** Remote logging service port number Clients Clients.Logging Protocol http ** Remote logging service host protocl Following config apply to using the SMTP service Smtp Host smtp.gmail.com ** SMTP service host name Smtp Port 25 ** SMTP service port number Smtp Password mypassword ** SMTP service host access password Smtp Sender jdoe@gmail.com ** SMTP service sendor/username Smtp Subject EdgeX Notification ** SMTP alert message subject *means the configuration value can be changed if necessary. **means the configuration value has to be replaced.","title":"Configuration Properties"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#configure-mail-server","text":"All the properties with prefix \"smtp\" are for mail server configuration. Configure the mail server appropriately to send Alerts and Notifications. The correct values depend on which mail server is used.","title":"Configure Mail Server"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#gmail","text":"Before using Gmail to send Alerts and Notifications, configure the sign-in security settings through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). An App password is a 16-digit passcode that gives an app or device permission to access your Google Account. For more detail about this topic, please refer to this Google official document: https://support.google.com/accounts/answer/185833 . Allow less secure apps: If the 2-Step Verification is not enabled, you may need to allow less secure apps to access the Gmail account. Please see the instruction from this Google official document on this topic: https://support.google.com/accounts/answer/6010255 . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.gmail.com Smtp Sender= ${ Gmail account } Smtp Password= ${ Gmail password or App password }","title":"Gmail"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#yahoo-mail","text":"Similar to Gmail, configure the sign-in security settings for Yahoo through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). Please see this Yahoo official document for more detail: https://help.yahoo.com/kb/SLN15241.html . Allow apps that use less secure sign in. Please see this Yahoo official document for more detail on this topic: https://help.yahoo.com/kb/SLN27791.html . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.mail.yahoo.com Smtp Sender= ${ Yahoo account } Smtp Password= ${ Yahoo password or App password }","title":"Yahoo Mail"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/","text":"Rules Engine Reference Implementation The Rules Engine microservice provides a reference implementation, edge-event triggering mechanism. The rules engine service monitors incoming sensor or device data for readings within target ranges and triggers immediate device actuation. Therefore, the rules engine provides \"intelligence\" at, or near, the network edge for faster response times. The implementation uses a Drools ( https://www.drools.org/ ) rules engine at its core. Drools is an open source rules engine provided by the JBoss community. This microservice is able to be replaced or augmented by many other edge-analytic capabilities provided by 3rd parties. Rules Engine as Export Service Client The reference implementation rules engine is an automatic export service client. When the service initiates, it automatically calls on the Export Client Registration microservice to register itself as a client of all device and sensor readings coming out of Core Data. As an Export Service client, the reference implementation rules engine receives all events and readings through the Export Distribution microservice. Based on data, the reference implementation rules engine is instructed to monitor each event and reading received through the Export Distribution microservice, and the rules engine triggers any actuation to a device through the Core Command microservice (which subsequently communicates the request through Device Service to communicate with the actual device). Rules Engine Direct Connect to Core Data In more time sensitive use cases or environment where a lot of data is being generated by the connected \"things\", it may be appropriate to connect the Rules Engine micro service to the data coming directly out of Core Data. That is, to by pass the Export Services for purposes of rule-based command activation. The rules engine has been programmed for this option. By default, the rules engine micro service registers itself as a client of the export service. This automatic registration can be turned off, and the rules engine can be connected directly to the ZeroMQ published data out of Core Data. Note: as the ZeroMQ pipe out of Core Data is a publish-subscribe mechanism, it allows for multiple subscribers. When rules engine is connected as a subscriber, Core Data is actually publishing simultaneously to two clients or subscribers: Export Services and Rules Engine. In order to disconnect the Rules Engine from the Export Services (as a client) and connect it directly to Core Data, the following Rules Engine micro service configuration parameters (found in application.properties) must be changed: export.client=true # this is normally false by default and is the indication to the Rules Engine micro service to register itself with the Export Services export.zeromq.port=5563 # this port is set to 5566 when connecting to the ZeroMQ pipe out of Export Services. Rules Client and High Level Interaction Diagram The Rules Engine microservice comes with a RESTful service that enables new rules to be added and removed. The RESTful API enables new rules, defined in JSON, to be dynamically added to the rules engine (through the REST POST). The JSON data provided is translated into Drools Rules files (.drl files) by the microservice. Each rule must be associated to a unique name that is used to identify the rule and the Drool file that holds it. Rules can also be requested to be removed by name. Note: Due to issues within Drools, a rule that is removed is only emptied of contents. The name of the rule (and the file that represents it) are still in the system. Therefore, rule names cannot be reused until the Rules Engine microservice is stopped and the empty Drool files are physically deleted. {.align-center} Rules (Defined), and Data Model Rules are provided to the Rules Engine microservice directly through the Rules Engine REST API or indirectly using the client UI. A rule is defined in 4 parts: name, log entry, condition, and action. {.align-center} Name The name uniquely identifies the rule. Log Entry The log entry, or simply log, is the text to be sent to the log when the rule condition is met and the action is triggered. Condition The condition specifies which data (from the Event/Reading supplied through the Export Service) to monitor. Specifically, the condition element of a rule specifies the device ID or name of the device to monitor, and the value checks (or simply \"checks\") to perform on sensor values collected on that device. The device ID or name must match the device ID or name specified in the Event object that is sent by Core Data to the Export Service and then relayed to the Rules Engine Service (through 0MQ). A value check specifies a parameter of the sensor to monitor and test to apply to the parameter. The ValueCheck parameter must match one of the the Reading names associated to the Event provided by CoreData (and is also the name of a legal ValueDescriptor). For example, on a thermostat sensor, the Reading may be reporting the current temperature. Therefore, the Reading name would be \"temperature\" and in order for rule to test the data from this sensor reading, a value check for the same device must contain a parameter of \"temperature\" as well. The operand1, operation, and operand2 must specify an equation around the parameter that the Rules Engine uses to determine whether to trigger the action. For example, a ValueCheck that wishes to specify to the Rules Engine to check all temperature reading \"value\" for any temperature above 72 degrees would specify it as follows: parameter : temperature operand1 : value operation : > operand2 : 72 Because the data in an Event's Reading may be reported in string form, the ValueCheck operands can and should be specified using Java syntax which is negotiated before the evaluation to create the appropriate type data for comparison. If temperature readings were represented as strings in Core Data, then the same ValueCheck would be specified as follows: parameter : temperature operand1 : Integer . parseInt ( value ) operation : > operand2 : 72 Lastly, the action specified in a rule specifies which command to trigger on a device or sensor and which data or parameters to send to the device as part of that call. The actual call is made through the Core Command microservice in REST form. Therefore, the action must specify the following items: The device identifier (per Metadata) that is to be called on The Command identifier (per Metadata) to be executed against the device The data supplied as part of the Command call Thus the data to be provided as part of the call is JSON data to be supplied in the body of the Command POST call. An example of the action properties is as follows: device: 56325f7ee4b05eaae5a89ce1 (the identifier of a device or sensor in Meta Data) command: 56325f6de4b05eaae5a89cdc (a command ID associated to the device per Meta Data) body: {\"value\":\"3\"} (the JSON data supplied in the REST message body). When creating a Rule in JSON to be POST submitted through the Rules Engine client, the entire Rule would be represented as shown below: { \"name\" : \"motortoofastsignal\" , \"condition\" : { \"device\" : \"562114e9e4b0385849b96cd8\" , \"checks\" : [ { \"parameter\" : \"RPM\" , \"operand1\" : \"Integer.parseInt(value)\" , \"operation\" : \">\" , \"operand2\" : \"1200\" } ] }, \"action\" : { \"device\" : \"56325f7ee4b05eaae5a89ce1\" , \"command\" : \"56325f6de4b05eaae5a89cdc\" , \"body\" : \"{\\\"value\\\":\\\"3\\\"}\" }, \"log\" : \"Patlite warning triggered for engine speed too high\" } Rules Engine Configuration The Rules Engine microservice has several configuration properties that are specific to rules engine operations. Additional configuration, such as the microservice's server port, are standard among EdgeX microservices and won't be covered here. The critical properties in the rules engine microservice are located in application.properties. Note that the source code contains an application.properties file in the /src/main/resources folder that serves as the default for development environments (typically) versus the application.properties in the docker-files folder of the source that provides the standard default for the Dockerized version of the microservice. The examples shown below are those from the /src/main/resources defaults. Automatic Rules Engine as an Export Distro client export.client=true When the rules engine microservice comes up, in order to receive data (the sensor Events/Readings) from EdgeX, it automatically registers as an export data client through the export client micro service. If you do not want the rules engine to automatically receive that data from the export services (namely export distro), set export.client to false. In particular, as outlined above, you may wish the rules engine microservice to receive data directly from core data versus the export services and thus may wish export.client set to false. Rules Engine Export Distribution Registration export.client.registration.url=http://localhost:48071/api/v1export.client.registration.name=EdgeXRulesEngine #how long to wait to retry registration export.client.registration.retry.time=10000 #how many times to try registration before exiting export.client.registration.retry.attempts=100 If export.client is set to true to have the rules engine microservice be a client of the export services, then additional properties need to be specified to indicate the location of the export client registration microservice (this may vary per environment -- like in a development versus docker environment), and the name to use for the rules engine with the export client when registering the rules engine. Core Data's Zero MQ Connection information export.zeromq.port=5566 export.zeromq.host=tcp://localhost As already indicated above in the Rules Engine Direct Connect to Core Data section, if Rules Engine is to be connected directly to the data feed (ZeroMQ) coming from Core Data, additional properties must be provided to specify the port and address for subscribing to the Core Data feed. Again, these may differ per environment (for instance local development versus a Dockerized environment). Location and Name of the Drools Template #Drools drl resource path rules.default.path=edgex/rules rules.packagename=org.edgexfoundry.rules rules.fileextension=.drl rules.template.path=edgex/templates rules.template.name=rule-template.drl rules.template.encoding=UTF-8 The rules engine is using Drools under the covers. When creating new rules via the rules engine microservice APIs, the rules engine must have access to a base template (a Drool file with a .drl extension by default) for creating new rules. The template carries certain imports and EdgeX device command call structure that is used by the rules engine to monitor the incoming data and actuate devices/sensors via the Command microservice. The location of the template, name of the template file and other properties associated to the template must be specified in the configuration properties. Typically, only the location of the template file changes per environment. Data Dictionary Class Name Description Action The command that is executed when a Condition is met. Condition The object describing the device and its ValueCheck condition that embodies a Rule. Rule The object containing the Condition and Action that define the Rule. ValueCheck The mathematical expression evaluated for a Condition.","title":"Rules Engine"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine","text":"","title":"Rules Engine"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#reference-implementation","text":"The Rules Engine microservice provides a reference implementation, edge-event triggering mechanism. The rules engine service monitors incoming sensor or device data for readings within target ranges and triggers immediate device actuation. Therefore, the rules engine provides \"intelligence\" at, or near, the network edge for faster response times. The implementation uses a Drools ( https://www.drools.org/ ) rules engine at its core. Drools is an open source rules engine provided by the JBoss community. This microservice is able to be replaced or augmented by many other edge-analytic capabilities provided by 3rd parties.","title":"Reference Implementation"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine-as-export-service-client","text":"The reference implementation rules engine is an automatic export service client. When the service initiates, it automatically calls on the Export Client Registration microservice to register itself as a client of all device and sensor readings coming out of Core Data. As an Export Service client, the reference implementation rules engine receives all events and readings through the Export Distribution microservice. Based on data, the reference implementation rules engine is instructed to monitor each event and reading received through the Export Distribution microservice, and the rules engine triggers any actuation to a device through the Core Command microservice (which subsequently communicates the request through Device Service to communicate with the actual device).","title":"Rules Engine as Export Service Client"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine-direct-connect-to-core-data","text":"In more time sensitive use cases or environment where a lot of data is being generated by the connected \"things\", it may be appropriate to connect the Rules Engine micro service to the data coming directly out of Core Data. That is, to by pass the Export Services for purposes of rule-based command activation. The rules engine has been programmed for this option. By default, the rules engine micro service registers itself as a client of the export service. This automatic registration can be turned off, and the rules engine can be connected directly to the ZeroMQ published data out of Core Data. Note: as the ZeroMQ pipe out of Core Data is a publish-subscribe mechanism, it allows for multiple subscribers. When rules engine is connected as a subscriber, Core Data is actually publishing simultaneously to two clients or subscribers: Export Services and Rules Engine. In order to disconnect the Rules Engine from the Export Services (as a client) and connect it directly to Core Data, the following Rules Engine micro service configuration parameters (found in application.properties) must be changed: export.client=true # this is normally false by default and is the indication to the Rules Engine micro service to register itself with the Export Services export.zeromq.port=5563 # this port is set to 5566 when connecting to the ZeroMQ pipe out of Export Services.","title":"Rules Engine Direct Connect to Core Data"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-client-and-high-level-interaction-diagram","text":"The Rules Engine microservice comes with a RESTful service that enables new rules to be added and removed. The RESTful API enables new rules, defined in JSON, to be dynamically added to the rules engine (through the REST POST). The JSON data provided is translated into Drools Rules files (.drl files) by the microservice. Each rule must be associated to a unique name that is used to identify the rule and the Drool file that holds it. Rules can also be requested to be removed by name. Note: Due to issues within Drools, a rule that is removed is only emptied of contents. The name of the rule (and the file that represents it) are still in the system. Therefore, rule names cannot be reused until the Rules Engine microservice is stopped and the empty Drool files are physically deleted. {.align-center}","title":"Rules Client and High Level Interaction Diagram"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-defined-and-data-model","text":"Rules are provided to the Rules Engine microservice directly through the Rules Engine REST API or indirectly using the client UI. A rule is defined in 4 parts: name, log entry, condition, and action. {.align-center} Name The name uniquely identifies the rule. Log Entry The log entry, or simply log, is the text to be sent to the log when the rule condition is met and the action is triggered. Condition The condition specifies which data (from the Event/Reading supplied through the Export Service) to monitor. Specifically, the condition element of a rule specifies the device ID or name of the device to monitor, and the value checks (or simply \"checks\") to perform on sensor values collected on that device. The device ID or name must match the device ID or name specified in the Event object that is sent by Core Data to the Export Service and then relayed to the Rules Engine Service (through 0MQ). A value check specifies a parameter of the sensor to monitor and test to apply to the parameter. The ValueCheck parameter must match one of the the Reading names associated to the Event provided by CoreData (and is also the name of a legal ValueDescriptor). For example, on a thermostat sensor, the Reading may be reporting the current temperature. Therefore, the Reading name would be \"temperature\" and in order for rule to test the data from this sensor reading, a value check for the same device must contain a parameter of \"temperature\" as well. The operand1, operation, and operand2 must specify an equation around the parameter that the Rules Engine uses to determine whether to trigger the action. For example, a ValueCheck that wishes to specify to the Rules Engine to check all temperature reading \"value\" for any temperature above 72 degrees would specify it as follows: parameter : temperature operand1 : value operation : > operand2 : 72 Because the data in an Event's Reading may be reported in string form, the ValueCheck operands can and should be specified using Java syntax which is negotiated before the evaluation to create the appropriate type data for comparison. If temperature readings were represented as strings in Core Data, then the same ValueCheck would be specified as follows: parameter : temperature operand1 : Integer . parseInt ( value ) operation : > operand2 : 72 Lastly, the action specified in a rule specifies which command to trigger on a device or sensor and which data or parameters to send to the device as part of that call. The actual call is made through the Core Command microservice in REST form. Therefore, the action must specify the following items: The device identifier (per Metadata) that is to be called on The Command identifier (per Metadata) to be executed against the device The data supplied as part of the Command call Thus the data to be provided as part of the call is JSON data to be supplied in the body of the Command POST call. An example of the action properties is as follows: device: 56325f7ee4b05eaae5a89ce1 (the identifier of a device or sensor in Meta Data) command: 56325f6de4b05eaae5a89cdc (a command ID associated to the device per Meta Data) body: {\"value\":\"3\"} (the JSON data supplied in the REST message body). When creating a Rule in JSON to be POST submitted through the Rules Engine client, the entire Rule would be represented as shown below: { \"name\" : \"motortoofastsignal\" , \"condition\" : { \"device\" : \"562114e9e4b0385849b96cd8\" , \"checks\" : [ { \"parameter\" : \"RPM\" , \"operand1\" : \"Integer.parseInt(value)\" , \"operation\" : \">\" , \"operand2\" : \"1200\" } ] }, \"action\" : { \"device\" : \"56325f7ee4b05eaae5a89ce1\" , \"command\" : \"56325f6de4b05eaae5a89cdc\" , \"body\" : \"{\\\"value\\\":\\\"3\\\"}\" }, \"log\" : \"Patlite warning triggered for engine speed too high\" }","title":"Rules (Defined), and Data Model"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine-configuration","text":"The Rules Engine microservice has several configuration properties that are specific to rules engine operations. Additional configuration, such as the microservice's server port, are standard among EdgeX microservices and won't be covered here. The critical properties in the rules engine microservice are located in application.properties. Note that the source code contains an application.properties file in the /src/main/resources folder that serves as the default for development environments (typically) versus the application.properties in the docker-files folder of the source that provides the standard default for the Dockerized version of the microservice. The examples shown below are those from the /src/main/resources defaults. Automatic Rules Engine as an Export Distro client export.client=true When the rules engine microservice comes up, in order to receive data (the sensor Events/Readings) from EdgeX, it automatically registers as an export data client through the export client micro service. If you do not want the rules engine to automatically receive that data from the export services (namely export distro), set export.client to false. In particular, as outlined above, you may wish the rules engine microservice to receive data directly from core data versus the export services and thus may wish export.client set to false. Rules Engine Export Distribution Registration export.client.registration.url=http://localhost:48071/api/v1export.client.registration.name=EdgeXRulesEngine #how long to wait to retry registration export.client.registration.retry.time=10000 #how many times to try registration before exiting export.client.registration.retry.attempts=100 If export.client is set to true to have the rules engine microservice be a client of the export services, then additional properties need to be specified to indicate the location of the export client registration microservice (this may vary per environment -- like in a development versus docker environment), and the name to use for the rules engine with the export client when registering the rules engine. Core Data's Zero MQ Connection information export.zeromq.port=5566 export.zeromq.host=tcp://localhost As already indicated above in the Rules Engine Direct Connect to Core Data section, if Rules Engine is to be connected directly to the data feed (ZeroMQ) coming from Core Data, additional properties must be provided to specify the port and address for subscribing to the Core Data feed. Again, these may differ per environment (for instance local development versus a Dockerized environment). Location and Name of the Drools Template #Drools drl resource path rules.default.path=edgex/rules rules.packagename=org.edgexfoundry.rules rules.fileextension=.drl rules.template.path=edgex/templates rules.template.name=rule-template.drl rules.template.encoding=UTF-8 The rules engine is using Drools under the covers. When creating new rules via the rules engine microservice APIs, the rules engine must have access to a base template (a Drool file with a .drl extension by default) for creating new rules. The template carries certain imports and EdgeX device command call structure that is used by the rules engine to monitor the incoming data and actuate devices/sensors via the Command microservice. The location of the template, name of the template file and other properties associated to the template must be specified in the configuration properties. Typically, only the location of the template file changes per environment.","title":"Rules Engine Configuration"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#data-dictionary","text":"Class Name Description Action The command that is executed when a Condition is met. Condition The object describing the device and its ValueCheck condition that embodies a Rule. Rule The object containing the Condition and Action that define the Rule. ValueCheck The mathematical expression evaluated for a Condition.","title":"Data Dictionary"},{"location":"microservices/support/scheduler/Ch-Scheduling/","text":"Scheduling Introduction The Scheduling microservice includes the Scrubber microservice which cleans up the event and reading data (for Core Data) that has already been exported to the gateway. Optionally, the Scrubber microservice can also be configured to remove stale event/reading data that has not been exported. The removing of stale event/reading data enables the gateway to continue to operate with a static amount of storage, while the system continues to collect new data from devices and sensors in cases where the export facility is not operational or not operating fast enough to keep up with data collection. The removal of both exported records and stale records occurs on a configurable schedule. By default, Scrubber cleans up the data every 30 minutes. The Scrubber microservice does not directly remove the data from EdgeX Foundry's persistent storage itself, rather it calls on Core Data to remove the records. Core Data serves as the single point of access to the persistent event/reading data. The Scrubber microservice is an independent service without any clients. That is, there is no API to call Scrubber. Scrubber operates on time triggers. Scheduler uses a data store to persist the Interval(s) and IntervalAction(s). Persistence is accomplished the Scheduler DB located in your current configured database for EdgeX. Data Dictionary Class Name Description Interval An object defining a specific \"period\" in time. IntervalAction The action taken by a Service when the Interval occurs.","title":"Scheduling"},{"location":"microservices/support/scheduler/Ch-Scheduling/#scheduling","text":"","title":"Scheduling"},{"location":"microservices/support/scheduler/Ch-Scheduling/#introduction","text":"The Scheduling microservice includes the Scrubber microservice which cleans up the event and reading data (for Core Data) that has already been exported to the gateway. Optionally, the Scrubber microservice can also be configured to remove stale event/reading data that has not been exported. The removing of stale event/reading data enables the gateway to continue to operate with a static amount of storage, while the system continues to collect new data from devices and sensors in cases where the export facility is not operational or not operating fast enough to keep up with data collection. The removal of both exported records and stale records occurs on a configurable schedule. By default, Scrubber cleans up the data every 30 minutes. The Scrubber microservice does not directly remove the data from EdgeX Foundry's persistent storage itself, rather it calls on Core Data to remove the records. Core Data serves as the single point of access to the persistent event/reading data. The Scrubber microservice is an independent service without any clients. That is, there is no API to call Scrubber. Scrubber operates on time triggers. Scheduler uses a data store to persist the Interval(s) and IntervalAction(s). Persistence is accomplished the Scheduler DB located in your current configured database for EdgeX.","title":"Introduction"},{"location":"microservices/support/scheduler/Ch-Scheduling/#data-dictionary","text":"Class Name Description Interval An object defining a specific \"period\" in time. IntervalAction The action taken by a Service when the Interval occurs.","title":"Data Dictionary"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/","text":"System Management Agent (SMA) Introduction While the SMA serves several purposes, it is to be considered, first and foremost, as the single connection point of management control for an EdgeX instance. As such, the API calls related to system management are defined so as to interface with the SMA. Examples of API Calls To get an appreciation for some SMA API calls in action, it will be instructive to look at what responses the SMA provides to the caller, for the respective calls (Notice, too, the error messages returned by the SMA, should it encounter a problem). Thus, consider the following calls that the SMA handles: Metrics of a service Configuration of a service Start a service Stop a service Restart a service Health check on a service Let's look at the preceding calls (aka requests), one-by-one, in the following sections. Metrics of a service Example request: /api/v1/metrics/edgex-core-command,edgex-core-data Corresponding response, in JSON format: { \"Metrics\" :{ \"edgex-core-command\" :{ \"CpuBusyAvg\" : 2.224995150836366 , \"Memory\" :{ \"Alloc\" : 1403648 , \"Frees\" : 1504 , \"LiveObjects\" : 18280 , \"Mallocs\" : 19784 , \"Sys\" : 71891192 , \"TotalAlloc\" : 1403648 } }, \"edgex-core-data\" :{ \"CpuBusyAvg\" : 2.854720153816541 , \"Memory\" :{ \"Alloc\" : 929080 , \"Frees\" : 1453 , \"LiveObjects\" : 7700 , \"Mallocs\" : 9153 , \"Sys\" : 70451200 , \"TotalAlloc\" : 929080 } } } } Configuration of a service Example request: /api/v1/config/device-simple,edgex-core-data Corresponding response, in JSON format: { \"Configuration\" : { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : { \"Clients\" : { \"Logging\" : { \"Host\" : \"localhost\" , \"Port\" : 48061 , \"Protocol\" : \"http\" }, \"Metadata\" : { \"Host\" : \"localhost\" , \"Port\" : 48081 , \"Protocol\" : \"http\" } }, \"Databases\" : { \"Primary\" : { \"Host\" : \"localhost\" , \"Name\" : \"coredata\" , \"Password\" : \"\" , \"Port\" : 27017 , \"Timeout\" : 5000 , \"Type\" : \"mongodb\" , \"Username\" : \"\" } }, \"Logging\" : { \"EnableRemote\" : false , \"File\" : \"./logs/edgex-core-data.log\" }, \"MessageQueue\" : { \"Host\" : \"*\" , \"Port\" : 5563 , \"Protocol\" : \"tcp\" , \"Type\" : \"zero\" }, \"Registry\" : { \"Host\" : \"localhost\" , \"Port\" : 8500 , \"Type\" : \"consul\" }, \"Service\" : { \"BootTimeout\" : 30000 , \"CheckInterval\" : \"10s\" , \"ClientMonitor\" : 15000 , \"Host\" : \"localhost\" , \"Port\" : 48080 , \"Protocol\" : \"http\" , \"MaxResultCount\" : 50000 , \"StartupMsg\" : \"This is the Core Data Microservice\" , \"Timeout\" : 5000 }, \"Writable\" : { \"DeviceUpdateLastConnected\" : false , \"LogLevel\" : \"INFO\" , \"MetaDataCheck\" : false , \"PersistData\" : true , \"ServiceUpdateLastConnected\" : false , \"ValidateCheck\" : false } } } } Start a service Example request: /api/v1/operation Example (POST) body accompanying the \"start\" request: { \"action\" : \"start\" , \"services\" :[ \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Started the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\" Stop a service Example request: /api/v1/operation Example (POST) body accompanying the \"stop\" request: { \"action\" : \"stop\" , \"services\" :[ \"edgex-support-notifications\" ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Stopped the requested service.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\" Restart a service Example request: /api/v1/operation Example (POST) body accompanying the \"restart\" request: { \"action\" : \"restart\" , \"services\" :[ \"edgex-support-notifications\" , \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Restarted the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\" Health check on a service Example request: /api/v1/health/device-simple,edgex-core-data,support-notifications Corresponding response, in JSON format: { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : true , \"support-notifications\" : true }","title":"System Management Agent (SMA)"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#system-management-agent-sma","text":"","title":"System Management Agent (SMA)"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#introduction","text":"While the SMA serves several purposes, it is to be considered, first and foremost, as the single connection point of management control for an EdgeX instance. As such, the API calls related to system management are defined so as to interface with the SMA.","title":"Introduction"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#examples-of-api-calls","text":"To get an appreciation for some SMA API calls in action, it will be instructive to look at what responses the SMA provides to the caller, for the respective calls (Notice, too, the error messages returned by the SMA, should it encounter a problem). Thus, consider the following calls that the SMA handles: Metrics of a service Configuration of a service Start a service Stop a service Restart a service Health check on a service Let's look at the preceding calls (aka requests), one-by-one, in the following sections.","title":"Examples of API Calls"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#metrics-of-a-service","text":"Example request: /api/v1/metrics/edgex-core-command,edgex-core-data Corresponding response, in JSON format: { \"Metrics\" :{ \"edgex-core-command\" :{ \"CpuBusyAvg\" : 2.224995150836366 , \"Memory\" :{ \"Alloc\" : 1403648 , \"Frees\" : 1504 , \"LiveObjects\" : 18280 , \"Mallocs\" : 19784 , \"Sys\" : 71891192 , \"TotalAlloc\" : 1403648 } }, \"edgex-core-data\" :{ \"CpuBusyAvg\" : 2.854720153816541 , \"Memory\" :{ \"Alloc\" : 929080 , \"Frees\" : 1453 , \"LiveObjects\" : 7700 , \"Mallocs\" : 9153 , \"Sys\" : 70451200 , \"TotalAlloc\" : 929080 } } } }","title":"Metrics of a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#configuration-of-a-service","text":"Example request: /api/v1/config/device-simple,edgex-core-data Corresponding response, in JSON format: { \"Configuration\" : { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : { \"Clients\" : { \"Logging\" : { \"Host\" : \"localhost\" , \"Port\" : 48061 , \"Protocol\" : \"http\" }, \"Metadata\" : { \"Host\" : \"localhost\" , \"Port\" : 48081 , \"Protocol\" : \"http\" } }, \"Databases\" : { \"Primary\" : { \"Host\" : \"localhost\" , \"Name\" : \"coredata\" , \"Password\" : \"\" , \"Port\" : 27017 , \"Timeout\" : 5000 , \"Type\" : \"mongodb\" , \"Username\" : \"\" } }, \"Logging\" : { \"EnableRemote\" : false , \"File\" : \"./logs/edgex-core-data.log\" }, \"MessageQueue\" : { \"Host\" : \"*\" , \"Port\" : 5563 , \"Protocol\" : \"tcp\" , \"Type\" : \"zero\" }, \"Registry\" : { \"Host\" : \"localhost\" , \"Port\" : 8500 , \"Type\" : \"consul\" }, \"Service\" : { \"BootTimeout\" : 30000 , \"CheckInterval\" : \"10s\" , \"ClientMonitor\" : 15000 , \"Host\" : \"localhost\" , \"Port\" : 48080 , \"Protocol\" : \"http\" , \"MaxResultCount\" : 50000 , \"StartupMsg\" : \"This is the Core Data Microservice\" , \"Timeout\" : 5000 }, \"Writable\" : { \"DeviceUpdateLastConnected\" : false , \"LogLevel\" : \"INFO\" , \"MetaDataCheck\" : false , \"PersistData\" : true , \"ServiceUpdateLastConnected\" : false , \"ValidateCheck\" : false } } } }","title":"Configuration of a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#start-a-service","text":"Example request: /api/v1/operation Example (POST) body accompanying the \"start\" request: { \"action\" : \"start\" , \"services\" :[ \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Started the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\"","title":"Start a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#stop-a-service","text":"Example request: /api/v1/operation Example (POST) body accompanying the \"stop\" request: { \"action\" : \"stop\" , \"services\" :[ \"edgex-support-notifications\" ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Stopped the requested service.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\"","title":"Stop a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#restart-a-service","text":"Example request: /api/v1/operation Example (POST) body accompanying the \"restart\" request: { \"action\" : \"restart\" , \"services\" :[ \"edgex-support-notifications\" , \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Restarted the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\"","title":"Restart a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#health-check-on-a-service","text":"Example request: /api/v1/health/device-simple,edgex-core-data,support-notifications Corresponding response, in JSON format: { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : true , \"support-notifications\" : true }","title":"Health check on a service"},{"location":"walk-through/Ch-Walkthrough/","text":"EdgeX Demonstration API Walk Through In order to better appreciate the EdgeX Foundry micro services (what they do and how they work), how they inter-operate with each other, and some of the more important API calls that each micro service has to offer, this demonstration API walk through shows how a device service and device are established in EdgeX, how data is sent flowing through the various services, and how data is then shipped out of EdgeX to the cloud or enterprise system. Through this demonstration, you will play the part of various EdgeX micro services by manually making REST calls in a way that mimics EdgeX system behavior. After exploring this demonstration, and hopefully exercising the APIs yourself, you should have a much better understanding of how EdgeX Foundry works.","title":"EdgeX Demonstration API Walk Through"},{"location":"walk-through/Ch-Walkthrough/#edgex-demonstration-api-walk-through","text":"In order to better appreciate the EdgeX Foundry micro services (what they do and how they work), how they inter-operate with each other, and some of the more important API calls that each micro service has to offer, this demonstration API walk through shows how a device service and device are established in EdgeX, how data is sent flowing through the various services, and how data is then shipped out of EdgeX to the cloud or enterprise system. Through this demonstration, you will play the part of various EdgeX micro services by manually making REST calls in a way that mimics EdgeX system behavior. After exploring this demonstration, and hopefully exercising the APIs yourself, you should have a much better understanding of how EdgeX Foundry works.","title":"EdgeX Demonstration API Walk Through"},{"location":"walk-through/Ch-WalkthroughCommands/","text":"Calling commands Recall that the Device Profile (the camera monitor profile) included a number of Commands to get and put information from any Device of that type. Also recall that the Device (the countcamera1) was associated to the Device Profile (the camera monitor profile) when the Device was provisioned. List the Commands See APIs Core Services Command Now with all the setup complete, you can ask the Core Command micro service for the list of Commands associated to the Device (the countcamera1). GET to http://localhost:48082/api/v1/device/name/countcamera1 Note all of the URLs returned as part of this response! These are the URLs that clients (internal or external to EdgeX) can call to trigger the various get and put offerings on the Device. Check the Value Descriptors See APIs Core Services Core Data See that the Value Descriptors are in Core Data. There should be a total of 5 Value Descriptors in Core Data. Note that Value Descriptors are stored in Core Data, yet referenced in Metadata. This is because as data coming from a Device is sent to Core Data, Core Data may need to validate the incoming values against the associated Value Descriptor parameters (like min, max, etc.) but without having to make a trip to Core Metadata to do that validation. Getting data into Core Data is a key function of EdgeX and must be accomplished as quickly as possible (without having to make additional REST requests). GET to http://localhost:48080/api/v1/valuedescriptor While we're at it, check that no data has yet been shipped to Core Data. Since the Device Service and Device are in this demonstration wholly manually driven by you, no sensor data should yet have been collected. You can test this theory by asking for the count of Events in Core Data. GET to http://localhost:48080/api/v1/event/count Execute a Command While there is no real Device or Device Service in this walk through, EdgeX doesn't know that. Therefore, with all the configuration and setup you have performed, you can ask EdgeX to set the scan depth or set the snapshot duration to the camera, and EdgeX will dutifully try to perform the task. Of course, since no Device Service or Device exists, as expected EdgeX will ultimately responds with an error. However, through the log files, you can see a Command made of the Core Command micro service, attempts to call on the appropriate Command of the fictitious Device Service that manages our fictitious camera. For example sake, let's launch a Command to set the scan depth of countcamera1 (the name of the single human/dog counting camera Device in EdgeX right now). The first task to launch a request to set the scan depth is to get the URL for the Command to \"PUT\" or set a new scan depth on the Device. As seen above request a list of the Commands by the Device name with the following API on Core Command GET to http://localhost:48082/api/v1/device/name/countcamera1 Now locate and copy the URL for the PUT Depth Command. Because of the IDs used, this will be different on each system so a generic API call will not suffice here. Below is a picture containing a slice of the JSON returned by the GET request above and desired PUT Command URL highlighted - yours will vary based on IDs. Copy this URL into your REST client tool of choice and make a PUT to that URL on Core Command with the new depth as the parameter with that request. PUT to http : // localhost : 48082 / api / v1 / device /< system specific device id >/ command /< system specific command id > BODY : { \"depth\" : \"9\" } Again, because no Device Service (or Device) actually exists, Core Command will respond with an HTTP 502 Bad Gateway error. However, checking the logging output will prove that the Core Command micro service did receive the request and attempted to call on the non-existent Device Service to issue the actuating command. docker logs edgex - core - command INFO : 2019 / 02 / 15 19 : 32 : 13 Issuing GET command to : http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth ERROR : 2019 / 02 / 15 19 : 32 : 13 Get http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth : dial tcp 172 . 17 . 0 . 1 : 49977 : getsockopt : connection refused","title":"Calling commands"},{"location":"walk-through/Ch-WalkthroughCommands/#calling-commands","text":"Recall that the Device Profile (the camera monitor profile) included a number of Commands to get and put information from any Device of that type. Also recall that the Device (the countcamera1) was associated to the Device Profile (the camera monitor profile) when the Device was provisioned.","title":"Calling commands"},{"location":"walk-through/Ch-WalkthroughCommands/#list-the-commands","text":"See APIs Core Services Command Now with all the setup complete, you can ask the Core Command micro service for the list of Commands associated to the Device (the countcamera1). GET to http://localhost:48082/api/v1/device/name/countcamera1 Note all of the URLs returned as part of this response! These are the URLs that clients (internal or external to EdgeX) can call to trigger the various get and put offerings on the Device.","title":"List the Commands"},{"location":"walk-through/Ch-WalkthroughCommands/#check-the-value-descriptors","text":"See APIs Core Services Core Data See that the Value Descriptors are in Core Data. There should be a total of 5 Value Descriptors in Core Data. Note that Value Descriptors are stored in Core Data, yet referenced in Metadata. This is because as data coming from a Device is sent to Core Data, Core Data may need to validate the incoming values against the associated Value Descriptor parameters (like min, max, etc.) but without having to make a trip to Core Metadata to do that validation. Getting data into Core Data is a key function of EdgeX and must be accomplished as quickly as possible (without having to make additional REST requests). GET to http://localhost:48080/api/v1/valuedescriptor While we're at it, check that no data has yet been shipped to Core Data. Since the Device Service and Device are in this demonstration wholly manually driven by you, no sensor data should yet have been collected. You can test this theory by asking for the count of Events in Core Data. GET to http://localhost:48080/api/v1/event/count","title":"Check the Value Descriptors"},{"location":"walk-through/Ch-WalkthroughCommands/#execute-a-command","text":"While there is no real Device or Device Service in this walk through, EdgeX doesn't know that. Therefore, with all the configuration and setup you have performed, you can ask EdgeX to set the scan depth or set the snapshot duration to the camera, and EdgeX will dutifully try to perform the task. Of course, since no Device Service or Device exists, as expected EdgeX will ultimately responds with an error. However, through the log files, you can see a Command made of the Core Command micro service, attempts to call on the appropriate Command of the fictitious Device Service that manages our fictitious camera. For example sake, let's launch a Command to set the scan depth of countcamera1 (the name of the single human/dog counting camera Device in EdgeX right now). The first task to launch a request to set the scan depth is to get the URL for the Command to \"PUT\" or set a new scan depth on the Device. As seen above request a list of the Commands by the Device name with the following API on Core Command GET to http://localhost:48082/api/v1/device/name/countcamera1 Now locate and copy the URL for the PUT Depth Command. Because of the IDs used, this will be different on each system so a generic API call will not suffice here. Below is a picture containing a slice of the JSON returned by the GET request above and desired PUT Command URL highlighted - yours will vary based on IDs. Copy this URL into your REST client tool of choice and make a PUT to that URL on Core Command with the new depth as the parameter with that request. PUT to http : // localhost : 48082 / api / v1 / device /< system specific device id >/ command /< system specific command id > BODY : { \"depth\" : \"9\" } Again, because no Device Service (or Device) actually exists, Core Command will respond with an HTTP 502 Bad Gateway error. However, checking the logging output will prove that the Core Command micro service did receive the request and attempted to call on the non-existent Device Service to issue the actuating command. docker logs edgex - core - command INFO : 2019 / 02 / 15 19 : 32 : 13 Issuing GET command to : http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth ERROR : 2019 / 02 / 15 19 : 32 : 13 Get http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth : dial tcp 172 . 17 . 0 . 1 : 49977 : getsockopt : connection refused","title":"Execute a Command"},{"location":"walk-through/Ch-WalkthroughData/","text":"Defining your data When a new Device Service is first started in EdgeX, there are many many tasks to perform - all in preparation for the Device Service to manage one or more Devices, which are yet unknown to EdgeX. In general, the Device Service tasks when it first starts can be categorized into: Establish the reference information around the Device Service and Device. Make the Device Service itself known to the rest of EdgeX Provision the Devices the Device Service will manage with EdgeX Reference information includes things such as defining the address (called an Addressable ) of the Device and Device Service or establishing the new unit of measure (called a Value Descriptor in EdgeX) used by the Device. The term \"provision\" is the way we talk about establishing the initial connection to the physical Device and have it be known to and communication with EdgeX. After the first run of a Device Service, these steps are not repeated. For example, after its initial startup, a Device Service would not need to re-establish the reference information into EdgeX. Instead,it would simply check that these operations have been accomplished and do not need to be redone. Creating Reference Information in EdgeX There is a lot of background information that EdgeX needs to know about the Device and Device Service before it can start collecting data from the Device or send actuation commands to the Device. Say, for example, the camera Device wanted to report its human and canine counts. If it were to just start sending numbers into EdgeX, EdgeX would have no idea of what those numbers represented or even where they came from. Further, if someone/something wanted to send a command to the camera, it would not know how to reach the camera without some additional information like where the camera is located on the network. This background or reference information is what a Device Service must define in EdgeX when it first comes up. The API calls here give you a glimpse of this communication between the fledgling Device Service and the other EdgeX micro services. By the way, the order in which these calls are shown may not be the exact order that a Device Service does them. As you become more familiar with Device Services and the Device Service SDK, the small nuances and differences will become clear. Addressables See Core Metadata API RAML at APIs Core Services Metadata The Device Service will often establish at least two Addressable objects with the Core Metadata micro service. An Addressable is a flexible EdgeX object that specifies a physical address of something - in this case the physical address of the Device Service and the Device (the camera). While an Addressable could be created for a named MQTT pipe or other protocol endpoint, for this example, we will assume that both the Device Service and Device are able to be reached via HTTP REST calls. So in this case, the Device Service would make two calls to Core Metadata, to create the Addressable for the Device Service: POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera control\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49977 , \"path\" : \"/cameracontrol\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } and the Addressable for the Device (the camera in this case): POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera1 address\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49999 , \"path\" : \"/camera1\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } Note that for an Addressable, a unique name must be provided. Obviously, these address are phony and made up for the purposes of this exercise. This is OK and it will still allow you to see how your Device and Device Services will work going forward. Walk Through Alert! If you are using Postman, be sure that you are POSTing raw data, not form-encoded data. If your API call was successful, you will get a generated ID for your new Addressable that looks like this: 5b773ad19f8fc200012a802d Value Descriptors See Core Data API RAML at APIs Core Services Core Data Next, the Device Service needs to inform EdgeX about the type of data it will be sending on the behalf of the Devices. If you are given the number 5, what does that mean to you? Nothing, without some context and unit of measure. For example, if I was to say 5 feet is the scan depth of the camera right now, you have a much better understanding about what the number 5 represents. In EdgeX, Value Descriptors provide the context and unit of measure for any data (or values) sent to and from a Device. As the name implies, a Value Descriptor describes a value - its unit of measure, its min and max values (if there are any), the way to display the value when showing it on the screen, and more. Any data obtained from a Device (we call this \"get\" from the Device) or any data sent to the Device for actuation (we call this \"set\" or \"put\" to the Device) requires a Value Descriptor to be associated with that data. In this demo, there are four Value Descriptors required: human count, canine count, scan depth, and snapshot duration. The Device Service would make four POST requests to Core Data to establish these Value Descriptors. Walk Through alert! Pay attention to the port numbers. In the previous section you were calling the core-metadata service (port 48081), in these you will be calling core-data (port 48080) POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"humancount\" , \"description\" : \"people count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"humans\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"caninecount\" , \"description\" : \"dog count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"canines\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"depth\" , \"description\" : \"scan distance\" , \"min\" : \"1\" , \"max\" : \"10\" , \"type\" : \"I\" , \"uomLabel\" : \"feet\" , \"defaultValue\" : \"1\" , \"formatting\" : \"%s\" , \"labels\" :[ \"scan\" , \"distance\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"duration\" , \"description\" : \"time between events\" , \"min\" : \"10\" , \"max\" : \"180\" , \"type\" : \"I\" , \"uomLabel\" : \"seconds\" , \"defaultValue\" : \"10\" , \"formatting\" : \"%s\" , \"labels\" :[ \"duration\" , \"time\" ] } An error can occur when communication with the camera. Therefore a fifth Value Descriptor is created for this eventuality. POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"cameraerror\" , \"description\" : \"error response message from a camera\" , \"min\" : \"\" , \"max\" : \"\" , \"type\" : \"S\" , \"uomLabel\" : \"\" , \"defaultValue\" : \"error\" , \"formatting\" : \"%s\" , \"labels\" :[ \"error\" , \"message\" ] } Again, the name of each Value Descriptor must be unique (within all of EdgeX). The type of a Value Descriptor indicates the type of the associated value: I (integer), F (floating point number), S (character or string), B (boolean), or J (JSON object). Formatting is used by UIs and should follow the printf formatting standard for how to represent the associated value. Walk Through alert! If you make a GET call to the http://localhost:48080/api/v1/valuedescriptor URL you will get a listing (in JSON) of all the Value Descriptors currently defined in your instance of EdgeX, including the ones you just added.","title":"Defining your data"},{"location":"walk-through/Ch-WalkthroughData/#defining-your-data","text":"When a new Device Service is first started in EdgeX, there are many many tasks to perform - all in preparation for the Device Service to manage one or more Devices, which are yet unknown to EdgeX. In general, the Device Service tasks when it first starts can be categorized into: Establish the reference information around the Device Service and Device. Make the Device Service itself known to the rest of EdgeX Provision the Devices the Device Service will manage with EdgeX Reference information includes things such as defining the address (called an Addressable ) of the Device and Device Service or establishing the new unit of measure (called a Value Descriptor in EdgeX) used by the Device. The term \"provision\" is the way we talk about establishing the initial connection to the physical Device and have it be known to and communication with EdgeX. After the first run of a Device Service, these steps are not repeated. For example, after its initial startup, a Device Service would not need to re-establish the reference information into EdgeX. Instead,it would simply check that these operations have been accomplished and do not need to be redone.","title":"Defining your data"},{"location":"walk-through/Ch-WalkthroughData/#creating-reference-information-in-edgex","text":"There is a lot of background information that EdgeX needs to know about the Device and Device Service before it can start collecting data from the Device or send actuation commands to the Device. Say, for example, the camera Device wanted to report its human and canine counts. If it were to just start sending numbers into EdgeX, EdgeX would have no idea of what those numbers represented or even where they came from. Further, if someone/something wanted to send a command to the camera, it would not know how to reach the camera without some additional information like where the camera is located on the network. This background or reference information is what a Device Service must define in EdgeX when it first comes up. The API calls here give you a glimpse of this communication between the fledgling Device Service and the other EdgeX micro services. By the way, the order in which these calls are shown may not be the exact order that a Device Service does them. As you become more familiar with Device Services and the Device Service SDK, the small nuances and differences will become clear.","title":"Creating Reference Information in EdgeX"},{"location":"walk-through/Ch-WalkthroughData/#addressables","text":"See Core Metadata API RAML at APIs Core Services Metadata The Device Service will often establish at least two Addressable objects with the Core Metadata micro service. An Addressable is a flexible EdgeX object that specifies a physical address of something - in this case the physical address of the Device Service and the Device (the camera). While an Addressable could be created for a named MQTT pipe or other protocol endpoint, for this example, we will assume that both the Device Service and Device are able to be reached via HTTP REST calls. So in this case, the Device Service would make two calls to Core Metadata, to create the Addressable for the Device Service: POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera control\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49977 , \"path\" : \"/cameracontrol\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } and the Addressable for the Device (the camera in this case): POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera1 address\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49999 , \"path\" : \"/camera1\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } Note that for an Addressable, a unique name must be provided. Obviously, these address are phony and made up for the purposes of this exercise. This is OK and it will still allow you to see how your Device and Device Services will work going forward. Walk Through Alert! If you are using Postman, be sure that you are POSTing raw data, not form-encoded data. If your API call was successful, you will get a generated ID for your new Addressable that looks like this: 5b773ad19f8fc200012a802d","title":"Addressables"},{"location":"walk-through/Ch-WalkthroughData/#value-descriptors","text":"See Core Data API RAML at APIs Core Services Core Data Next, the Device Service needs to inform EdgeX about the type of data it will be sending on the behalf of the Devices. If you are given the number 5, what does that mean to you? Nothing, without some context and unit of measure. For example, if I was to say 5 feet is the scan depth of the camera right now, you have a much better understanding about what the number 5 represents. In EdgeX, Value Descriptors provide the context and unit of measure for any data (or values) sent to and from a Device. As the name implies, a Value Descriptor describes a value - its unit of measure, its min and max values (if there are any), the way to display the value when showing it on the screen, and more. Any data obtained from a Device (we call this \"get\" from the Device) or any data sent to the Device for actuation (we call this \"set\" or \"put\" to the Device) requires a Value Descriptor to be associated with that data. In this demo, there are four Value Descriptors required: human count, canine count, scan depth, and snapshot duration. The Device Service would make four POST requests to Core Data to establish these Value Descriptors. Walk Through alert! Pay attention to the port numbers. In the previous section you were calling the core-metadata service (port 48081), in these you will be calling core-data (port 48080) POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"humancount\" , \"description\" : \"people count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"humans\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"caninecount\" , \"description\" : \"dog count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"canines\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"depth\" , \"description\" : \"scan distance\" , \"min\" : \"1\" , \"max\" : \"10\" , \"type\" : \"I\" , \"uomLabel\" : \"feet\" , \"defaultValue\" : \"1\" , \"formatting\" : \"%s\" , \"labels\" :[ \"scan\" , \"distance\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"duration\" , \"description\" : \"time between events\" , \"min\" : \"10\" , \"max\" : \"180\" , \"type\" : \"I\" , \"uomLabel\" : \"seconds\" , \"defaultValue\" : \"10\" , \"formatting\" : \"%s\" , \"labels\" :[ \"duration\" , \"time\" ] } An error can occur when communication with the camera. Therefore a fifth Value Descriptor is created for this eventuality. POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"cameraerror\" , \"description\" : \"error response message from a camera\" , \"min\" : \"\" , \"max\" : \"\" , \"type\" : \"S\" , \"uomLabel\" : \"\" , \"defaultValue\" : \"error\" , \"formatting\" : \"%s\" , \"labels\" :[ \"error\" , \"message\" ] } Again, the name of each Value Descriptor must be unique (within all of EdgeX). The type of a Value Descriptor indicates the type of the associated value: I (integer), F (floating point number), S (character or string), B (boolean), or J (JSON object). Formatting is used by UIs and should follow the printf formatting standard for how to represent the associated value. Walk Through alert! If you make a GET call to the http://localhost:48080/api/v1/valuedescriptor URL you will get a listing (in JSON) of all the Value Descriptors currently defined in your instance of EdgeX, including the ones you just added.","title":"Value Descriptors"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/","text":"Defining your device A Device Profile can be thought of as a template or as a type or classification of Device. General characteristics about the type of Device, the data theses Devices provide, and how to command them is all provided in a Device Profile. Other pages within these docs provide more details about a Device Profile and its purpose (see Core-Metadata to start). It is typical that as part of the reference information setup sequence, the Device Service provides the Device Profiles for the types of Devices it manages. Adding a Device Profile See Core Metadata API RAML at APIs Core Services Metadata Our fictitious Device Service will manage only the human/dog counting camera, so it only needs to make one POST request to create the monitoring camera Device Profile. Since Device Profiles are often represented in YAML, make a muti-part form-data POST with the Device Profile file below to create the Camera Monitor profile. POST to http://localhost:48081/api/v1/deviceprofile/uploadfile No headers FORM-DATA: key: \"file\" value: EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} Walk Through alert! In this step you will want to use the form-data POST format in Postman, with a key named \"file\" of type \"File\". Download and use the provided EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} for this. Each profile has a unique name along with a description, manufacturer, model and collection of labels to assist in queries for particular profiles. These are relatively straightforward attributes of a profile. Understanding Commands The Device Profile defines how to communicate with any Device that abides by the profile. In particular, it defines the Commands that can be sent to the Device (via the Device Service). Commands are named and have either a get (for retrieving data from the Device) or put (to send data to the Device) or both. Each Command can have a single get and single put. Both get and put are optional, but it would not make sense to have a Command without at least one get or at least one put. The Command name must be unique for that profile (the Command name does not have to be unique across all of EdgeX - for example, many profiles may contain a \"status\" Command). Understanding Command Gets and Puts The get and put each have a path which is used by EdgeX to call on the specific Command get or put at the URL address provided for the service. Hypothetically, if the address to a Device Service was \" http://abc:9999 \" and the get Command had a path of \"foo\", then internally, EdgeX would know to use \" http://abc:9999/foo \" to call on the get Command. Get and puts then have response objects (an array of response objects). A get must have at least one response object. A put is not required to have a response. Responses might be \"good\" or \"error\" responses. Each get should have at least one \"good\" response, but it may have several error responses depending on what problems or issues the Device Service may need to reply with. Each response is made up of a code (which suggests if it is a good or error response), a description (human readable information about what is in the response), and an array of expected values. For practical purposes, the code is usually an HTTP status code like 200 (for good responses), 404 or 503 (examples of bad responses). The expected values in a response are an array of Value Descriptor names. If a call to an get Command is expected to return back the human and dog count data, then the response's expected values would be: [humancount, caninecount]. When the actual call to the Device Service is made, the body of the return response from the service is expected to return a value for each of the expected values in a map where the Value Descriptor names are used as keys. Again, using the human and dog counts as an example, if the expected values were [humancount, caninecount] then the body of a good response from the service would contain a map that looks something like this: { \"humancount\" : 5 , \"caninecount\" : 2 } Here is an example set of responses that might be used for a get Command in the camera example. Note that one response is coded as the \"good\" response (code 200) while the other is for \"error\" response (code 404). The expected values for the good response are the Value Descriptor names for the camera's count data. The expected values for the \"error\" response is the Value Descriptor name for an error message. { \"responses\" :[ { \"code\" : \"200\" , \"description\" : \"ok\" , \"expectedValues\" :[ \"humancount\" , \"caninecount\" ]}, { \"code\" : \"404\" , \"description\" : \"bad request\" , \"expectedValues\" :[ \"cameraerror\" ]} ] } } Understanding Command Parameters Commands are used to send data to Devices (via Device Services) as much as they are used to get data from Devices. Therefore, any Command may have a set of parameters associated with its call. Parameter data is added to the body of the Command request. Parameters are defined via an array of parameterNames on a Command. Here again, this array is just an array of Value Descriptor names. Each Value Descriptor defines the name and type of information to be supplied as a parameter to the Command call. For example, if a Command had a parameterNames array of [depth, duration] , then the receiving command is expecting values that match the depth and duration Value Descriptors. Similar to the way expected values are used to set the keys of the response body, the parameter names are used as keys in a map to pass parameter values in a Command call that has parameters. Here might be what is populated in the body of the Command call when the parameterNames are [depth, duration] . { \"depth\" : 1 , \"duration\" : 10 } If you open the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} file, see that there are Commands to get people and dog counts (and a command called Counts, which provides both values). There are also commands to get/put the snapshot duration and scan depth. Also note the expected values for the Commands. The expected values should match the name of the Value Descriptors from above that give context to the returned values. In real implementations, the Device Profile may contain many more details (like device resource and resource elements) to assist the Device Service in its communications with Devices. Expected Values Alert! Metadata does not currently check that the expected values match an existing Value Descriptor by name. Therefore, make sure you provide the expected values array carefully when creating Device Profiles.","title":"Defining your device"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#defining-your-device","text":"A Device Profile can be thought of as a template or as a type or classification of Device. General characteristics about the type of Device, the data theses Devices provide, and how to command them is all provided in a Device Profile. Other pages within these docs provide more details about a Device Profile and its purpose (see Core-Metadata to start). It is typical that as part of the reference information setup sequence, the Device Service provides the Device Profiles for the types of Devices it manages.","title":"Defining your device"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#adding-a-device-profile","text":"See Core Metadata API RAML at APIs Core Services Metadata Our fictitious Device Service will manage only the human/dog counting camera, so it only needs to make one POST request to create the monitoring camera Device Profile. Since Device Profiles are often represented in YAML, make a muti-part form-data POST with the Device Profile file below to create the Camera Monitor profile. POST to http://localhost:48081/api/v1/deviceprofile/uploadfile No headers FORM-DATA: key: \"file\" value: EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} Walk Through alert! In this step you will want to use the form-data POST format in Postman, with a key named \"file\" of type \"File\". Download and use the provided EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} for this. Each profile has a unique name along with a description, manufacturer, model and collection of labels to assist in queries for particular profiles. These are relatively straightforward attributes of a profile.","title":"Adding a Device Profile"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#understanding-commands","text":"The Device Profile defines how to communicate with any Device that abides by the profile. In particular, it defines the Commands that can be sent to the Device (via the Device Service). Commands are named and have either a get (for retrieving data from the Device) or put (to send data to the Device) or both. Each Command can have a single get and single put. Both get and put are optional, but it would not make sense to have a Command without at least one get or at least one put. The Command name must be unique for that profile (the Command name does not have to be unique across all of EdgeX - for example, many profiles may contain a \"status\" Command).","title":"Understanding Commands"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#understanding-command-gets-and-puts","text":"The get and put each have a path which is used by EdgeX to call on the specific Command get or put at the URL address provided for the service. Hypothetically, if the address to a Device Service was \" http://abc:9999 \" and the get Command had a path of \"foo\", then internally, EdgeX would know to use \" http://abc:9999/foo \" to call on the get Command. Get and puts then have response objects (an array of response objects). A get must have at least one response object. A put is not required to have a response. Responses might be \"good\" or \"error\" responses. Each get should have at least one \"good\" response, but it may have several error responses depending on what problems or issues the Device Service may need to reply with. Each response is made up of a code (which suggests if it is a good or error response), a description (human readable information about what is in the response), and an array of expected values. For practical purposes, the code is usually an HTTP status code like 200 (for good responses), 404 or 503 (examples of bad responses). The expected values in a response are an array of Value Descriptor names. If a call to an get Command is expected to return back the human and dog count data, then the response's expected values would be: [humancount, caninecount]. When the actual call to the Device Service is made, the body of the return response from the service is expected to return a value for each of the expected values in a map where the Value Descriptor names are used as keys. Again, using the human and dog counts as an example, if the expected values were [humancount, caninecount] then the body of a good response from the service would contain a map that looks something like this: { \"humancount\" : 5 , \"caninecount\" : 2 } Here is an example set of responses that might be used for a get Command in the camera example. Note that one response is coded as the \"good\" response (code 200) while the other is for \"error\" response (code 404). The expected values for the good response are the Value Descriptor names for the camera's count data. The expected values for the \"error\" response is the Value Descriptor name for an error message. { \"responses\" :[ { \"code\" : \"200\" , \"description\" : \"ok\" , \"expectedValues\" :[ \"humancount\" , \"caninecount\" ]}, { \"code\" : \"404\" , \"description\" : \"bad request\" , \"expectedValues\" :[ \"cameraerror\" ]} ] } }","title":"Understanding Command Gets and Puts"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#understanding-command-parameters","text":"Commands are used to send data to Devices (via Device Services) as much as they are used to get data from Devices. Therefore, any Command may have a set of parameters associated with its call. Parameter data is added to the body of the Command request. Parameters are defined via an array of parameterNames on a Command. Here again, this array is just an array of Value Descriptor names. Each Value Descriptor defines the name and type of information to be supplied as a parameter to the Command call. For example, if a Command had a parameterNames array of [depth, duration] , then the receiving command is expecting values that match the depth and duration Value Descriptors. Similar to the way expected values are used to set the keys of the response body, the parameter names are used as keys in a map to pass parameter values in a Command call that has parameters. Here might be what is populated in the body of the Command call when the parameterNames are [depth, duration] . { \"depth\" : 1 , \"duration\" : 10 } If you open the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} file, see that there are Commands to get people and dog counts (and a command called Counts, which provides both values). There are also commands to get/put the snapshot duration and scan depth. Also note the expected values for the Commands. The expected values should match the name of the Value Descriptors from above that give context to the returned values. In real implementations, the Device Profile may contain many more details (like device resource and resource elements) to assist the Device Service in its communications with Devices. Expected Values Alert! Metadata does not currently check that the expected values match an existing Value Descriptor by name. Therefore, make sure you provide the expected values array carefully when creating Device Profiles.","title":"Understanding Command Parameters"},{"location":"walk-through/Ch-WalkthroughDeviceService/","text":"Register your Device Service Once the reference information is established by the Device Service in Core Data and Meta Data, the Device Service can register or define itself in EdgeX. That is, it can proclaim to EdgeX that \"I have arrived and am functional.\" Register with Core Configuration and Registration See APIs Core Services Configuration and Registry Part of that registration process of the Device Service, indeed any EdgeX micro service, is to register itself with the Core Configuration & Registration. In this process, the micro service provides its location to the Config/Reg micro service and picks up any new/latest configuration information from this central service. Since there is no real Device Service in this demonstration, this part of the inter-micro service exchange is not explored here. Create the Device Service See APIs Core Services Metadata The Device Service must then create an instance of itself in Core Metadata. It is in this registration, that the Device Service is associated to the Addressable for the Device Service that is already Core Metadata. Make this POST to Core Metadata to create the Device Service (using the Addressable's unique name to establish the association) POST to http : // localhost : 48081 / api / v1 / deviceservice BODY : { \"name\" : \"camera control device service\" , \"description\" : \"Manage human and dog counting cameras\" , \"labels\" :[ \"camera\" , \"counter\" ], \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"camera control\" }} The name of the Device Service must be unique across all of EdgeX. Note the admin and operating states. The administrative state (aka admin state) provides control of the Device Service by man or other systems. It can be set to locked or unlocked. When a Device Service is set to locked, it is not suppose to respond to any Command requests nor send data from the Devices. The operating state (aka op state) provides an indication on the part of EdgeX about the internal operating status of the Device Service. The operating state is not set externally (as by another system or man), it is a signal from within EdgeX (and potentially the Device Service itself) about the condition of the service. The operating state of the Device Service may be either enabled or disabled. When the operating state of the Device Service is disabled, it is either experiencing some difficulty or going through some process (for example an upgrade) which does not allow it to function in its normal capacity.","title":"Register your Device Service"},{"location":"walk-through/Ch-WalkthroughDeviceService/#register-your-device-service","text":"Once the reference information is established by the Device Service in Core Data and Meta Data, the Device Service can register or define itself in EdgeX. That is, it can proclaim to EdgeX that \"I have arrived and am functional.\"","title":"Register your Device Service"},{"location":"walk-through/Ch-WalkthroughDeviceService/#register-with-core-configuration-and-registration","text":"See APIs Core Services Configuration and Registry Part of that registration process of the Device Service, indeed any EdgeX micro service, is to register itself with the Core Configuration & Registration. In this process, the micro service provides its location to the Config/Reg micro service and picks up any new/latest configuration information from this central service. Since there is no real Device Service in this demonstration, this part of the inter-micro service exchange is not explored here.","title":"Register with Core Configuration and Registration"},{"location":"walk-through/Ch-WalkthroughDeviceService/#create-the-device-service","text":"See APIs Core Services Metadata The Device Service must then create an instance of itself in Core Metadata. It is in this registration, that the Device Service is associated to the Addressable for the Device Service that is already Core Metadata. Make this POST to Core Metadata to create the Device Service (using the Addressable's unique name to establish the association) POST to http : // localhost : 48081 / api / v1 / deviceservice BODY : { \"name\" : \"camera control device service\" , \"description\" : \"Manage human and dog counting cameras\" , \"labels\" :[ \"camera\" , \"counter\" ], \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"camera control\" }} The name of the Device Service must be unique across all of EdgeX. Note the admin and operating states. The administrative state (aka admin state) provides control of the Device Service by man or other systems. It can be set to locked or unlocked. When a Device Service is set to locked, it is not suppose to respond to any Command requests nor send data from the Devices. The operating state (aka op state) provides an indication on the part of EdgeX about the internal operating status of the Device Service. The operating state is not set externally (as by another system or man), it is a signal from within EdgeX (and potentially the Device Service itself) about the condition of the service. The operating state of the Device Service may be either enabled or disabled. When the operating state of the Device Service is disabled, it is either experiencing some difficulty or going through some process (for example an upgrade) which does not allow it to function in its normal capacity.","title":"Create the Device Service"},{"location":"walk-through/Ch-WalkthroughExporting/","text":"Exporting your device data Great, so the data sent by the camera Device makes it way to Core Data. How can that data be sent to an enterprise system or the Cloud? How can that data be used by an edge analytics system (like the Rules Engine provided with EdgeX) to actuate on a Device? Anything wishing to receive the sensor/device data as it comes into EdgeX must register as an \"export\" client. Export Clients In fact, by default, the Rules Engine is automatically registered as a client of the export services and automatically receives all the Events/Readings from Core Data that are sent by Devices. To see all the existing export clients, you can request a list from the Export Client micro service. GET to http://localhost:48071/api/v1/registration The response from Export Client is a list of registered client details - in this case just the Rules Engine is registered. Register an Export Client To register a new client to receive EdgeX data, you will first need to setup a client capable of receiving HTTP REST calls, or an MQTT topic capable of receiving messages from EdgeX. For the purposes of this demonstration, let's say there is an cloud based MQTT Topic that has been setup ready to receive EdgeX Event/Reading data. To register this MQTT endpoint to receive all Event/Reading data, in JSON format, but encrypted, you will need to request Export Client to make a new EdgeX client. POST to http : // localhost : 48071 / api / v1 / registration BODY : { \"name\" : \"MyMQTTTopic\" , \"addressable\" : { \"name\" : \"MyMQTTBroker\" , \"protocol\" : \"TCP\" , \"address\" : \"tcp://m10.cloudmqtt.com\" , \"port\" : 15421 , \"publisher\" : \"EdgeXExportPublisher\" , \"user\" : \"hukfgtoh\" , \"password\" : \"mypass\" , \"topic\" : \"EdgeXDataTopic\" } , \"format\" : \"JSON\" , \"encryption\" : { \"encryptionAlgorithm\" : \"AES\" , \"encryptionKey\" : \"123\" , \"initializingVector\" : \"123\" } , \"enable\" : true , \"destination\" : \"MQTT_TOPIC\" } Note that the Addressable for the REST address is built into the request. Now, should a new Event be posted to Core Data, the Export Distro micro service will attempt to sent the encrypted, JSON-formated Event/Reading data to the MQTT client. Unless you have actually setup the MQTT Topic to receive the messages, Export Distro will fail to deliver the contents and an error will result. You can check the Export Distro log to see the attempt was made and that the EdgeX Export services are working correctly, despite the non-existence of the receiving MQTT Topic. MQTTOutboundServiceActivator: message sent to MQTT broker: Addressable [name=MyMQTTBroker, protocol=TCP, address=tcp://m10.cloudmqtt.com, port=15421, path=null, publisher=EdgeXExportPublisher, user=hukfgtoh, password=mypass, topic=EdgeXDataTopic, toString()=BaseObject [id=null, created=0, modified=0, origin=0]] : 596283c7e4b0011866276e9 Building your own solutions Congratulations, you've made it all the way through the Walkthrough tutorial!","title":"Exporting your device data"},{"location":"walk-through/Ch-WalkthroughExporting/#exporting-your-device-data","text":"Great, so the data sent by the camera Device makes it way to Core Data. How can that data be sent to an enterprise system or the Cloud? How can that data be used by an edge analytics system (like the Rules Engine provided with EdgeX) to actuate on a Device? Anything wishing to receive the sensor/device data as it comes into EdgeX must register as an \"export\" client.","title":"Exporting your device data"},{"location":"walk-through/Ch-WalkthroughExporting/#export-clients","text":"In fact, by default, the Rules Engine is automatically registered as a client of the export services and automatically receives all the Events/Readings from Core Data that are sent by Devices. To see all the existing export clients, you can request a list from the Export Client micro service. GET to http://localhost:48071/api/v1/registration The response from Export Client is a list of registered client details - in this case just the Rules Engine is registered.","title":"Export Clients"},{"location":"walk-through/Ch-WalkthroughExporting/#register-an-export-client","text":"To register a new client to receive EdgeX data, you will first need to setup a client capable of receiving HTTP REST calls, or an MQTT topic capable of receiving messages from EdgeX. For the purposes of this demonstration, let's say there is an cloud based MQTT Topic that has been setup ready to receive EdgeX Event/Reading data. To register this MQTT endpoint to receive all Event/Reading data, in JSON format, but encrypted, you will need to request Export Client to make a new EdgeX client. POST to http : // localhost : 48071 / api / v1 / registration BODY : { \"name\" : \"MyMQTTTopic\" , \"addressable\" : { \"name\" : \"MyMQTTBroker\" , \"protocol\" : \"TCP\" , \"address\" : \"tcp://m10.cloudmqtt.com\" , \"port\" : 15421 , \"publisher\" : \"EdgeXExportPublisher\" , \"user\" : \"hukfgtoh\" , \"password\" : \"mypass\" , \"topic\" : \"EdgeXDataTopic\" } , \"format\" : \"JSON\" , \"encryption\" : { \"encryptionAlgorithm\" : \"AES\" , \"encryptionKey\" : \"123\" , \"initializingVector\" : \"123\" } , \"enable\" : true , \"destination\" : \"MQTT_TOPIC\" } Note that the Addressable for the REST address is built into the request. Now, should a new Event be posted to Core Data, the Export Distro micro service will attempt to sent the encrypted, JSON-formated Event/Reading data to the MQTT client. Unless you have actually setup the MQTT Topic to receive the messages, Export Distro will fail to deliver the contents and an error will result. You can check the Export Distro log to see the attempt was made and that the EdgeX Export services are working correctly, despite the non-existence of the receiving MQTT Topic. MQTTOutboundServiceActivator: message sent to MQTT broker: Addressable [name=MyMQTTBroker, protocol=TCP, address=tcp://m10.cloudmqtt.com, port=15421, path=null, publisher=EdgeXExportPublisher, user=hukfgtoh, password=mypass, topic=EdgeXDataTopic, toString()=BaseObject [id=null, created=0, modified=0, origin=0]] : 596283c7e4b0011866276e9","title":"Register an Export Client"},{"location":"walk-through/Ch-WalkthroughExporting/#building-your-own-solutions","text":"Congratulations, you've made it all the way through the Walkthrough tutorial!","title":"Building your own solutions"},{"location":"walk-through/Ch-WalkthroughProvision/","text":"Provision a Device In the last act of setup, a Device Service often discovers and provisions new Devices it finds and is going to manage on the part of EdgeX. Note the word \"often\" in the last sentence. Not all Device Services will discover new Devices or provision them right away. Depending on the type of Device and how the Devices communicate, it is up to the Device Service to determine how/when to provision a Device. In some rare cases, the provisioning may be triggered by a human request of the Device Service once everything is in place and once the human can provide the information the Device Service needs to physically connect to the Device. Adding your device See APIs Core Services Metadata For the sake of this demonstration, the call to Core Metadata below will provision the human/dog counting monitor camera as if the Device Service discovered it (by some unknown means) and provisioned the Device as part of some startup process. To create a Device, it must be associated to a Device Profile (by name or id), a Device Service (by name or id), and contain one or more Protocols defining its address. When calling each of the POST calls above, the ID was returned by the associated micro service and used in the call below. In this example, the names of Device Profile, Device Service, and Protocols are used. POST to http : // localhost : 48081 / api / v1 / device BODY : { \"name\" : \"countcamera1\" , \"description\" : \"human and dog counting camera #1\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"protocols\" : { \"camera protocol\" : { \"camera address\" : \"camera 1\" }} , \"labels\" : [ \"camera\" , \"counter\" ], \"location\" : \"\" , \"service\" : { \"name\" : \"camera control device service\" } , \"profile\" : { \"name\" : \"camera monitor profile\" }} Note that camera monitor profile was created by the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} you uploaded in a previous step. Test the Setup With the Device Service and Device now appropriately setup/provisioned in EdgeX, let's try a few of the micro service APIs out to confirm that things have been configured correctly. Check the Device Service See APIs Core Services Metadata To begin, check out that the Device Service is available via Core Metadata. GET to http://localhost:48081/api/v1/deviceservice Note that the associated Addressable is returned with the Device Service. There are many additional APIs on Core Metadata to retrieve a Device Service. As an example, here is one to find all Device Services by label - in this case using the label that was associated to the camera control device service. GET to http://localhost:48081/api/v1/deviceservice/label/camera Check the Device See APIs Core Services Metadata Ensure the monitor camera is among the devices known to Core Metadata. GET to http://localhost:48081/api/v1/device Note that the associated Device Profile, Device Service and Addressable is returned with the Device. Again, there are many additional APIs on Core Metadata to retrieve a Device. As an example, here is one to find all Devices associated to a given Device Profile - in this case using the camera monitor profile Device Profile name. GET to http://localhost:48081/api/v1/device/profilename/camera+monitor+profile","title":"Provision a Device"},{"location":"walk-through/Ch-WalkthroughProvision/#provision-a-device","text":"In the last act of setup, a Device Service often discovers and provisions new Devices it finds and is going to manage on the part of EdgeX. Note the word \"often\" in the last sentence. Not all Device Services will discover new Devices or provision them right away. Depending on the type of Device and how the Devices communicate, it is up to the Device Service to determine how/when to provision a Device. In some rare cases, the provisioning may be triggered by a human request of the Device Service once everything is in place and once the human can provide the information the Device Service needs to physically connect to the Device.","title":"Provision a Device"},{"location":"walk-through/Ch-WalkthroughProvision/#adding-your-device","text":"See APIs Core Services Metadata For the sake of this demonstration, the call to Core Metadata below will provision the human/dog counting monitor camera as if the Device Service discovered it (by some unknown means) and provisioned the Device as part of some startup process. To create a Device, it must be associated to a Device Profile (by name or id), a Device Service (by name or id), and contain one or more Protocols defining its address. When calling each of the POST calls above, the ID was returned by the associated micro service and used in the call below. In this example, the names of Device Profile, Device Service, and Protocols are used. POST to http : // localhost : 48081 / api / v1 / device BODY : { \"name\" : \"countcamera1\" , \"description\" : \"human and dog counting camera #1\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"protocols\" : { \"camera protocol\" : { \"camera address\" : \"camera 1\" }} , \"labels\" : [ \"camera\" , \"counter\" ], \"location\" : \"\" , \"service\" : { \"name\" : \"camera control device service\" } , \"profile\" : { \"name\" : \"camera monitor profile\" }} Note that camera monitor profile was created by the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} you uploaded in a previous step.","title":"Adding your device"},{"location":"walk-through/Ch-WalkthroughProvision/#test-the-setup","text":"With the Device Service and Device now appropriately setup/provisioned in EdgeX, let's try a few of the micro service APIs out to confirm that things have been configured correctly.","title":"Test the Setup"},{"location":"walk-through/Ch-WalkthroughProvision/#check-the-device-service","text":"See APIs Core Services Metadata To begin, check out that the Device Service is available via Core Metadata. GET to http://localhost:48081/api/v1/deviceservice Note that the associated Addressable is returned with the Device Service. There are many additional APIs on Core Metadata to retrieve a Device Service. As an example, here is one to find all Device Services by label - in this case using the label that was associated to the camera control device service. GET to http://localhost:48081/api/v1/deviceservice/label/camera","title":"Check the Device Service"},{"location":"walk-through/Ch-WalkthroughProvision/#check-the-device","text":"See APIs Core Services Metadata Ensure the monitor camera is among the devices known to Core Metadata. GET to http://localhost:48081/api/v1/device Note that the associated Device Profile, Device Service and Addressable is returned with the Device. Again, there are many additional APIs on Core Metadata to retrieve a Device. As an example, here is one to find all Devices associated to a given Device Profile - in this case using the camera monitor profile Device Profile name. GET to http://localhost:48081/api/v1/device/profilename/camera+monitor+profile","title":"Check the Device"},{"location":"walk-through/Ch-WalkthroughReading/","text":"Sending events and reading data In the real world, the human/dog counting camera would start to take pictures, count beings, and send that data to EdgeX. To simulate this activity. in this section, you will make Core Data API calls as if you were the camera's Device and Device Service. Send an Event/Reading See APIs Core Services Core Data Data is submitted to Core Data as an Event. An Event is a collection of sensor readings from a Device (associated to a Device by its ID or name) at a particular point in time. A Reading in an Event is a particular value sensed by the Device and associated to a Value Descriptor (by name) to provide context to the reading. So, the human/dog counting camera might determine that there are current 5 people and 3 dogs in the space it is monitoring. In the EdgeX vernacular, the Device Service upon receiving these sensed values from the Device would create an Event with two Readings - one Reading would contain the key/value pair of humancount:5 and the other Reading would contain the key/value pair of caninecount:3. The Device Service, on creating the Event and associated Reading objects would transmit this information to Core Data via REST call. POST to http : // localhost : 48080 / api / v1 / event BODY : { \"device\" : \"countcamera1\" , \"readings\" :[ { \"name\" : \"humancount\" , \"value\" : \"5\" } , { \"name\" : \"caninecount\" , \"value\" : \"3\" } ] } If desired, the Device Service can also supply an origin property (see below) to the Event or Reading to suggest the time (in Epoch timestamp/milliseconds format) at which the data was sensed/collected. If an origin is not provided, no origin will be set for the Event or Reading, however every Event and Reading is provided a Created and Modified timestamp in the database to give the data some time context. BODY: {\"device\":\"countcamera1\",\"origin\":1471806386919, \"readings\":[{\"name\":\"humancount\",\"value\":\"1\",\"origin\":1471806386919},{\"name\":\"caninecount\",\"value\":\"0\",\"origin\":1471806386919}]} Origin Timestamp Recommendation! Note: Smart devices will often timestamp sensor data and this timestamp can be used as the origin timestamp. In cases where the sensor/device is unable to provide a timestamp (\"dumb\" or brownfield sensors), it is recommended that the Device Service create a timestamp for the sensor data that is applied as the origin timestamp for the Device. Reading data Now that an Event (or two) and associated Readings have been sent to Core Data, you can use the Core Data API to explore that data that is now stored in MongoDB. Recall from the Test Setup section, you checked that no data was yet stored in Core Data. Make the same call and this time, 2 Event records should be the count returned. GET to http://localhost:48080/api/v1/event/count Retrieve 10 of the Events associated to the countcamera1 Device. GET to http://localhost:48080/api/v1/event/device/countcamera1/10 Retrieve 10 of the human count Readings associated to the countcamera1 Device (i.e. - get Readings by Value Descriptor) GET to http://localhost:48080/api/v1/reading/name/humancount/10","title":"Sending events and reading data"},{"location":"walk-through/Ch-WalkthroughReading/#sending-events-and-reading-data","text":"In the real world, the human/dog counting camera would start to take pictures, count beings, and send that data to EdgeX. To simulate this activity. in this section, you will make Core Data API calls as if you were the camera's Device and Device Service.","title":"Sending events and reading data"},{"location":"walk-through/Ch-WalkthroughReading/#send-an-eventreading","text":"See APIs Core Services Core Data Data is submitted to Core Data as an Event. An Event is a collection of sensor readings from a Device (associated to a Device by its ID or name) at a particular point in time. A Reading in an Event is a particular value sensed by the Device and associated to a Value Descriptor (by name) to provide context to the reading. So, the human/dog counting camera might determine that there are current 5 people and 3 dogs in the space it is monitoring. In the EdgeX vernacular, the Device Service upon receiving these sensed values from the Device would create an Event with two Readings - one Reading would contain the key/value pair of humancount:5 and the other Reading would contain the key/value pair of caninecount:3. The Device Service, on creating the Event and associated Reading objects would transmit this information to Core Data via REST call. POST to http : // localhost : 48080 / api / v1 / event BODY : { \"device\" : \"countcamera1\" , \"readings\" :[ { \"name\" : \"humancount\" , \"value\" : \"5\" } , { \"name\" : \"caninecount\" , \"value\" : \"3\" } ] } If desired, the Device Service can also supply an origin property (see below) to the Event or Reading to suggest the time (in Epoch timestamp/milliseconds format) at which the data was sensed/collected. If an origin is not provided, no origin will be set for the Event or Reading, however every Event and Reading is provided a Created and Modified timestamp in the database to give the data some time context. BODY: {\"device\":\"countcamera1\",\"origin\":1471806386919, \"readings\":[{\"name\":\"humancount\",\"value\":\"1\",\"origin\":1471806386919},{\"name\":\"caninecount\",\"value\":\"0\",\"origin\":1471806386919}]} Origin Timestamp Recommendation! Note: Smart devices will often timestamp sensor data and this timestamp can be used as the origin timestamp. In cases where the sensor/device is unable to provide a timestamp (\"dumb\" or brownfield sensors), it is recommended that the Device Service create a timestamp for the sensor data that is applied as the origin timestamp for the Device.","title":"Send an Event/Reading"},{"location":"walk-through/Ch-WalkthroughReading/#reading-data","text":"Now that an Event (or two) and associated Readings have been sent to Core Data, you can use the Core Data API to explore that data that is now stored in MongoDB. Recall from the Test Setup section, you checked that no data was yet stored in Core Data. Make the same call and this time, 2 Event records should be the count returned. GET to http://localhost:48080/api/v1/event/count Retrieve 10 of the Events associated to the countcamera1 Device. GET to http://localhost:48080/api/v1/event/device/countcamera1/10 Retrieve 10 of the human count Readings associated to the countcamera1 Device (i.e. - get Readings by Value Descriptor) GET to http://localhost:48080/api/v1/reading/name/humancount/10","title":"Reading data"},{"location":"walk-through/Ch-WalkthroughRunning/","text":"Running EdgeX If you have already followed Getting Started Users you will have already downloaded and started these containers, and you can skip this step and go right to the the Walkthrough Use Case Download the docker-compose file After installing Docker and Docker Compose, you need to get a Docker Compose file. EdgeX Foundry has over 12 microservices, each deployed in their own Docker container, and the Docker Compose file will make it easier to download and run them all. A Docker Compose file is a manifest file, which lists: The Docker containers (or more precisely the Docker container images) that should be downloaded, The order in which the containers should be started The parameters under which the containers should be run It is recommended that you use the lastest version of EdgeX Foundry. As of this writing, the latest version can be found here: https://github.com/edgexfoundry/developer-scripts/raw/master/releases/fuji/compose-files/docker-compose-fuji.yml Save this file as docker-compose.yml in your working directory so that the following commands will find it. Download the containers Once you have downloaded the docker-compose.yml file, run the following command to download the containers for each of the EdgeX Foundry microservices. Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully Starting EdgeX For this Walkthrough you will need to run all the services with the exception of any device services, including the device-virtual. The reason is that many of the API calls you make as part of this walk through are actually accomplished by the virtual device service - or any device service for that matter. In this walk through, your manual call of the EdgeX APIs often simulate the work that a device service would do to get a new device setup and to send data to/through EdgeX. Run the following commands to start the core, supporting and export micro services of EdgeX. Docker Command Description Suggested Wait Time After Completing docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds. In the time it takes to type the next command it should be ready. docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from A couple of seconds docker-compose up -d config-seed Populate the configuration/registry microservice A couple of seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries A couple of seconds docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices A couple of seconds docker-compose up -d metadata Start the Core Metadata microservice A couple of seconds docker-compose up -d data Start the Core Data microservice A couple of seconds docker-compose up -d command Start the Core Command microservice A couple of seconds docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices A couple of seconds docker-compose up -d export-client Start the Export Client registration microservice A couple of seconds docker-compose up -d export-distro Start the Export Distribution microservice A couple of seconds docker-compose up -d rulesengine Start the Rules Engine microservice Note: this service is still implemented in Java and takes more time to start 1 minute Run \"docker-compose ps\" to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.)","title":"Running EdgeX"},{"location":"walk-through/Ch-WalkthroughRunning/#running-edgex","text":"If you have already followed Getting Started Users you will have already downloaded and started these containers, and you can skip this step and go right to the the Walkthrough Use Case","title":"Running EdgeX"},{"location":"walk-through/Ch-WalkthroughRunning/#download-the-docker-compose-file","text":"After installing Docker and Docker Compose, you need to get a Docker Compose file. EdgeX Foundry has over 12 microservices, each deployed in their own Docker container, and the Docker Compose file will make it easier to download and run them all. A Docker Compose file is a manifest file, which lists: The Docker containers (or more precisely the Docker container images) that should be downloaded, The order in which the containers should be started The parameters under which the containers should be run It is recommended that you use the lastest version of EdgeX Foundry. As of this writing, the latest version can be found here: https://github.com/edgexfoundry/developer-scripts/raw/master/releases/fuji/compose-files/docker-compose-fuji.yml Save this file as docker-compose.yml in your working directory so that the following commands will find it.","title":"Download the docker-compose file"},{"location":"walk-through/Ch-WalkthroughRunning/#download-the-containers","text":"Once you have downloaded the docker-compose.yml file, run the following command to download the containers for each of the EdgeX Foundry microservices. Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully","title":"Download the containers"},{"location":"walk-through/Ch-WalkthroughRunning/#starting-edgex","text":"For this Walkthrough you will need to run all the services with the exception of any device services, including the device-virtual. The reason is that many of the API calls you make as part of this walk through are actually accomplished by the virtual device service - or any device service for that matter. In this walk through, your manual call of the EdgeX APIs often simulate the work that a device service would do to get a new device setup and to send data to/through EdgeX. Run the following commands to start the core, supporting and export micro services of EdgeX. Docker Command Description Suggested Wait Time After Completing docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds. In the time it takes to type the next command it should be ready. docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from A couple of seconds docker-compose up -d config-seed Populate the configuration/registry microservice A couple of seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries A couple of seconds docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices A couple of seconds docker-compose up -d metadata Start the Core Metadata microservice A couple of seconds docker-compose up -d data Start the Core Data microservice A couple of seconds docker-compose up -d command Start the Core Command microservice A couple of seconds docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices A couple of seconds docker-compose up -d export-client Start the Export Client registration microservice A couple of seconds docker-compose up -d export-distro Start the Export Distribution microservice A couple of seconds docker-compose up -d rulesengine Start the Rules Engine microservice Note: this service is still implemented in Java and takes more time to start 1 minute Run \"docker-compose ps\" to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.)","title":"Starting EdgeX"},{"location":"walk-through/Ch-WalkthroughSetup/","text":"Setup up your environment Install Docker & Docker Compose To run Dockerized EdgeX Foundry, you need to install Docker. See https://docs.docker.com/install/ to learn how to obtain and install Docker. If you are new to using Docker, the same web site provides you additional information. The following short video has is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Docker Compose is used to orchestrate the fetch (or pull), installation, and the start and stop of the EdgeX Foundry microservice containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. Docker Compose is automatically installed with Docker for Mac and Windows users. See: https://docs.docker.com/compose/install/ to determine if your Docker installation already contains Docker Compose, and how to install Compose if it does not. You do not need to be an expert with Docker to run EdgeX Foundry. The instructions in this guide provide you with the steps to get EdgeX Foundry running in your environment. Some basic knowledge of these two technologies of Docker and Docker Compose, are nice to have, but not required. Install Postman You can follow this walkthrough making HTTP calls from the command-line with a tool like curl , but it's easier if you use a tool designed for testing REST APIs. For that we like to use Postman . You can download the native Postman app for your operating system. Walk Through Alert! It is assumed that for the purposes of this walk through demonstration all API micro services are running on \"localhost\". If this is not the case, substitute your hostname for localhost. any POST call has the associated CONTENT-TYPE=application/JSON header associated to it unless explicitly stated otherwise.","title":"Setup up your environment"},{"location":"walk-through/Ch-WalkthroughSetup/#setup-up-your-environment","text":"","title":"Setup up your environment"},{"location":"walk-through/Ch-WalkthroughSetup/#install-docker-docker-compose","text":"To run Dockerized EdgeX Foundry, you need to install Docker. See https://docs.docker.com/install/ to learn how to obtain and install Docker. If you are new to using Docker, the same web site provides you additional information. The following short video has is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Docker Compose is used to orchestrate the fetch (or pull), installation, and the start and stop of the EdgeX Foundry microservice containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. Docker Compose is automatically installed with Docker for Mac and Windows users. See: https://docs.docker.com/compose/install/ to determine if your Docker installation already contains Docker Compose, and how to install Compose if it does not. You do not need to be an expert with Docker to run EdgeX Foundry. The instructions in this guide provide you with the steps to get EdgeX Foundry running in your environment. Some basic knowledge of these two technologies of Docker and Docker Compose, are nice to have, but not required.","title":"Install Docker &amp; Docker Compose"},{"location":"walk-through/Ch-WalkthroughSetup/#install-postman","text":"You can follow this walkthrough making HTTP calls from the command-line with a tool like curl , but it's easier if you use a tool designed for testing REST APIs. For that we like to use Postman . You can download the native Postman app for your operating system. Walk Through Alert! It is assumed that for the purposes of this walk through demonstration all API micro services are running on \"localhost\". If this is not the case, substitute your hostname for localhost. any POST call has the associated CONTENT-TYPE=application/JSON header associated to it unless explicitly stated otherwise.","title":"Install Postman"},{"location":"walk-through/Ch-WalkthroughUseCase/","text":"Example Use Case Suppose you had a new device that you wanted to connect to EdgeX. The device was a camera that took a picture and then had an on-board chip that analyzed the picture and reported the number of humans and canines (dogs) it saw. How often the camera takes a picture and reports its findings can be configured. In fact, the camera device could be sent two actuation commands - that is sent two requests for which it must respond and do something. You could send a request to set its time, in seconds, between picture snapshots (and then calculating the number of humans and dogs it finds in that resulting image). You could also request it to set the scan depth, in feet, of the camera - that is set how far out the camera looks. The farther out it looks, the less accurate the count of humans and dogs becomes, so this is something the manufacturer wants to allow the user to set based on use case needs. In EdgeX, the camera must be represented by a Device. Each Device is managed by a Device Service micro service. The Device Service communicates with the underlying hardware - in this case the camera - in the protocol of choice for that Device. The Device Service collects the data from the Devices it manages and passes that data into EdgeX (into Core Data). In this case, the Device Service would be collecting the count of humans and dogs that the camera sees. The Device Service also serves to translate the request for actuation from EdgeX and the rest of the world into protocol requests that the physical Device would understand. So in this example, the Device Service would take requests to set the duration between snapshots and to set the scan depth and translate those requests into protocol commands that the camera understood. Exactly how this camera physically connects to the host machine running EdgeX and how the Device Service works under the covers to communicate with the camera Device is immaterial for the point of this demonstration.","title":"Example Use Case"},{"location":"walk-through/Ch-WalkthroughUseCase/#example-use-case","text":"Suppose you had a new device that you wanted to connect to EdgeX. The device was a camera that took a picture and then had an on-board chip that analyzed the picture and reported the number of humans and canines (dogs) it saw. How often the camera takes a picture and reports its findings can be configured. In fact, the camera device could be sent two actuation commands - that is sent two requests for which it must respond and do something. You could send a request to set its time, in seconds, between picture snapshots (and then calculating the number of humans and dogs it finds in that resulting image). You could also request it to set the scan depth, in feet, of the camera - that is set how far out the camera looks. The farther out it looks, the less accurate the count of humans and dogs becomes, so this is something the manufacturer wants to allow the user to set based on use case needs. In EdgeX, the camera must be represented by a Device. Each Device is managed by a Device Service micro service. The Device Service communicates with the underlying hardware - in this case the camera - in the protocol of choice for that Device. The Device Service collects the data from the Devices it manages and passes that data into EdgeX (into Core Data). In this case, the Device Service would be collecting the count of humans and dogs that the camera sees. The Device Service also serves to translate the request for actuation from EdgeX and the rest of the world into protocol requests that the physical Device would understand. So in this example, the Device Service would take requests to set the duration between snapshots and to set the scan depth and translate those requests into protocol commands that the camera understood. Exactly how this camera physically connects to the host machine running EdgeX and how the Device Service works under the covers to communicate with the camera Device is immaterial for the point of this demonstration.","title":"Example Use Case"}]}